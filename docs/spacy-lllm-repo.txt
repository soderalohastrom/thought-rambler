Directory structure:
â””â”€â”€ explosion-spacy-llm/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ MANIFEST.in
    â”œâ”€â”€ migration_guide.md
    â”œâ”€â”€ push-tags.sh
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ requirements-dev.txt
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ setup.cfg
    â”œâ”€â”€ setup.py
    â”œâ”€â”€ .pre-commit-config.yaml
    â”œâ”€â”€ spacy_llm/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ cache.py
    â”‚   â”œâ”€â”€ compat.py
    â”‚   â”œâ”€â”€ ty.py
    â”‚   â”œâ”€â”€ util.py
    â”‚   â”œâ”€â”€ models/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ hf/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ base.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ dolly.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ falcon.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ llama2.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ mistral.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ openllama.py
    â”‚   â”‚   â”‚   â””â”€â”€ stablelm.py
    â”‚   â”‚   â”œâ”€â”€ langchain/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ model.py
    â”‚   â”‚   â””â”€â”€ rest/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ base.py
    â”‚   â”‚       â”œâ”€â”€ anthropic/
    â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”‚   â”œâ”€â”€ model.py
    â”‚   â”‚       â”‚   â””â”€â”€ registry.py
    â”‚   â”‚       â”œâ”€â”€ azure/
    â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”‚   â”œâ”€â”€ model.py
    â”‚   â”‚       â”‚   â””â”€â”€ registry.py
    â”‚   â”‚       â”œâ”€â”€ cohere/
    â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”‚   â”œâ”€â”€ model.py
    â”‚   â”‚       â”‚   â””â”€â”€ registry.py
    â”‚   â”‚       â”œâ”€â”€ noop/
    â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”‚   â”œâ”€â”€ model.py
    â”‚   â”‚       â”‚   â””â”€â”€ registry.py
    â”‚   â”‚       â”œâ”€â”€ openai/
    â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”‚   â”œâ”€â”€ model.py
    â”‚   â”‚       â”‚   â””â”€â”€ registry.py
    â”‚   â”‚       â””â”€â”€ palm/
    â”‚   â”‚           â”œâ”€â”€ __init__.py
    â”‚   â”‚           â”œâ”€â”€ model.py
    â”‚   â”‚           â””â”€â”€ registry.py
    â”‚   â”œâ”€â”€ pipeline/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â””â”€â”€ llm.py
    â”‚   â”œâ”€â”€ registry/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ normalizer.py
    â”‚   â”‚   â”œâ”€â”€ reader.py
    â”‚   â”‚   â””â”€â”€ util.py
    â”‚   â”œâ”€â”€ tasks/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ builtin_task.py
    â”‚   â”‚   â”œâ”€â”€ noop.py
    â”‚   â”‚   â”œâ”€â”€ entity_linker/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ candidate_selector.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ parser.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ registry.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ task.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ ty.py
    â”‚   â”‚   â”‚   â””â”€â”€ util.py
    â”‚   â”‚   â”œâ”€â”€ lemma/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ parser.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ registry.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ task.py
    â”‚   â”‚   â”‚   â””â”€â”€ util.py
    â”‚   â”‚   â”œâ”€â”€ ner/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ registry.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ task.py
    â”‚   â”‚   â”‚   â””â”€â”€ util.py
    â”‚   â”‚   â”œâ”€â”€ raw/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ parser.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ registry.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ task.py
    â”‚   â”‚   â”‚   â””â”€â”€ util.py
    â”‚   â”‚   â”œâ”€â”€ rel/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ items.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ parser.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ registry.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ task.py
    â”‚   â”‚   â”‚   â””â”€â”€ util.py
    â”‚   â”‚   â”œâ”€â”€ sentiment/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ parser.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ registry.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ task.py
    â”‚   â”‚   â”‚   â””â”€â”€ util.py
    â”‚   â”‚   â”œâ”€â”€ span/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ examples.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ parser.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ registry.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ task.py
    â”‚   â”‚   â”‚   â””â”€â”€ util.py
    â”‚   â”‚   â”œâ”€â”€ spancat/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ registry.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ task.py
    â”‚   â”‚   â”‚   â””â”€â”€ util.py
    â”‚   â”‚   â”œâ”€â”€ summarization/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ parser.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ registry.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ task.py
    â”‚   â”‚   â”‚   â””â”€â”€ util.py
    â”‚   â”‚   â”œâ”€â”€ templates/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ entity_linker.v1.jinja
    â”‚   â”‚   â”‚   â”œâ”€â”€ lemma.v1.jinja
    â”‚   â”‚   â”‚   â”œâ”€â”€ ner.v1.jinja
    â”‚   â”‚   â”‚   â”œâ”€â”€ ner.v2.jinja
    â”‚   â”‚   â”‚   â”œâ”€â”€ ner.v3.jinja
    â”‚   â”‚   â”‚   â”œâ”€â”€ raw.v1.jinja
    â”‚   â”‚   â”‚   â”œâ”€â”€ rel.v1.jinja
    â”‚   â”‚   â”‚   â”œâ”€â”€ sentiment.v1.jinja
    â”‚   â”‚   â”‚   â”œâ”€â”€ spancat.v1.jinja
    â”‚   â”‚   â”‚   â”œâ”€â”€ spancat.v2.jinja
    â”‚   â”‚   â”‚   â”œâ”€â”€ spancat.v3.jinja
    â”‚   â”‚   â”‚   â”œâ”€â”€ summarization.v1.jinja
    â”‚   â”‚   â”‚   â”œâ”€â”€ textcat.v1.jinja
    â”‚   â”‚   â”‚   â”œâ”€â”€ textcat.v2.jinja
    â”‚   â”‚   â”‚   â”œâ”€â”€ textcat.v3.jinja
    â”‚   â”‚   â”‚   â””â”€â”€ translation.v1.jinja
    â”‚   â”‚   â”œâ”€â”€ textcat/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ parser.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ registry.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ task.py
    â”‚   â”‚   â”‚   â””â”€â”€ util.py
    â”‚   â”‚   â”œâ”€â”€ translation/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ parser.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ registry.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ task.py
    â”‚   â”‚   â”‚   â””â”€â”€ util.py
    â”‚   â”‚   â””â”€â”€ util/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ parsing.py
    â”‚   â”‚       â””â”€â”€ sharding.py
    â”‚   â””â”€â”€ tests/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ compat.py
    â”‚       â”œâ”€â”€ conftest.py
    â”‚       â”œâ”€â”€ test_cache.py
    â”‚       â”œâ”€â”€ test_combinations.py
    â”‚       â”œâ”€â”€ test_registry.py
    â”‚       â”œâ”€â”€ models/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ test_anthropic.py
    â”‚       â”‚   â”œâ”€â”€ test_cohere.py
    â”‚       â”‚   â”œâ”€â”€ test_dolly.py
    â”‚       â”‚   â”œâ”€â”€ test_falcon.py
    â”‚       â”‚   â”œâ”€â”€ test_hf.py
    â”‚       â”‚   â”œâ”€â”€ test_langchain.py
    â”‚       â”‚   â”œâ”€â”€ test_llama2.py
    â”‚       â”‚   â”œâ”€â”€ test_mistral.py
    â”‚       â”‚   â”œâ”€â”€ test_openllama.py
    â”‚       â”‚   â”œâ”€â”€ test_palm.py
    â”‚       â”‚   â”œâ”€â”€ test_rest.py
    â”‚       â”‚   â””â”€â”€ test_stablelm.py
    â”‚       â”œâ”€â”€ pipeline/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â””â”€â”€ test_llm.py
    â”‚       â”œâ”€â”€ sharding/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ test_sharding.py
    â”‚       â”‚   â””â”€â”€ util.py
    â”‚       â””â”€â”€ tasks/
    â”‚           â”œâ”€â”€ __init__.py
    â”‚           â”œâ”€â”€ custom.cfg
    â”‚           â”œâ”€â”€ test_custom.py
    â”‚           â”œâ”€â”€ test_entity_linker.py
    â”‚           â”œâ”€â”€ test_lemma.py
    â”‚           â”œâ”€â”€ test_ner.py
    â”‚           â”œâ”€â”€ test_raw.py
    â”‚           â”œâ”€â”€ test_rel.py
    â”‚           â”œâ”€â”€ test_sentiment.py
    â”‚           â”œâ”€â”€ test_span_utils.py
    â”‚           â”œâ”€â”€ test_spancat.py
    â”‚           â”œâ”€â”€ test_summarization.py
    â”‚           â”œâ”€â”€ test_textcat.py
    â”‚           â”œâ”€â”€ test_translation.py
    â”‚           â”œâ”€â”€ examples/
    â”‚           â”‚   â”œâ”€â”€ entity_linker.json
    â”‚           â”‚   â”œâ”€â”€ entity_linker.jsonl
    â”‚           â”‚   â”œâ”€â”€ entity_linker.yml
    â”‚           â”‚   â”œâ”€â”€ lemma.json
    â”‚           â”‚   â”œâ”€â”€ lemma.jsonl
    â”‚           â”‚   â”œâ”€â”€ lemma.yml
    â”‚           â”‚   â”œâ”€â”€ ner.json
    â”‚           â”‚   â”œâ”€â”€ ner.jsonl
    â”‚           â”‚   â”œâ”€â”€ ner.yml
    â”‚           â”‚   â”œâ”€â”€ ner_inconsistent.yml
    â”‚           â”‚   â”œâ”€â”€ raw.json
    â”‚           â”‚   â”œâ”€â”€ raw.jsonl
    â”‚           â”‚   â”œâ”€â”€ raw.yml
    â”‚           â”‚   â”œâ”€â”€ rel.jsonl
    â”‚           â”‚   â”œâ”€â”€ sentiment.json
    â”‚           â”‚   â”œâ”€â”€ sentiment.jsonl
    â”‚           â”‚   â”œâ”€â”€ sentiment.yml
    â”‚           â”‚   â”œâ”€â”€ spancat.json
    â”‚           â”‚   â”œâ”€â”€ spancat.jsonl
    â”‚           â”‚   â”œâ”€â”€ spancat.yml
    â”‚           â”‚   â”œâ”€â”€ summarization.json
    â”‚           â”‚   â”œâ”€â”€ summarization.jsonl
    â”‚           â”‚   â”œâ”€â”€ summarization.yml
    â”‚           â”‚   â”œâ”€â”€ textcat_binary.json
    â”‚           â”‚   â”œâ”€â”€ textcat_binary.jsonl
    â”‚           â”‚   â”œâ”€â”€ textcat_binary.yml
    â”‚           â”‚   â”œâ”€â”€ textcat_multi_excl.json
    â”‚           â”‚   â”œâ”€â”€ textcat_multi_excl.jsonl
    â”‚           â”‚   â”œâ”€â”€ textcat_multi_excl.yml
    â”‚           â”‚   â”œâ”€â”€ textcat_multi_nonexcl.json
    â”‚           â”‚   â”œâ”€â”€ textcat_multi_nonexcl.jsonl
    â”‚           â”‚   â”œâ”€â”€ textcat_multi_nonexcl.yml
    â”‚           â”‚   â”œâ”€â”€ translation.json
    â”‚           â”‚   â”œâ”€â”€ translation.jsonl
    â”‚           â”‚   â””â”€â”€ translation.yml
    â”‚           â”œâ”€â”€ legacy/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ test_ner.py
    â”‚           â”‚   â”œâ”€â”€ test_spancat.py
    â”‚           â”‚   â”œâ”€â”€ examples/
    â”‚           â”‚   â”‚   â”œâ”€â”€ ner.json
    â”‚           â”‚   â”‚   â”œâ”€â”€ ner.jsonl
    â”‚           â”‚   â”‚   â”œâ”€â”€ ner.yml
    â”‚           â”‚   â”‚   â””â”€â”€ ner_inconsistent.yml
    â”‚           â”‚   â””â”€â”€ templates/
    â”‚           â”‚       â””â”€â”€ ner.jinja2
    â”‚           â”œâ”€â”€ misc/
    â”‚           â”‚   â””â”€â”€ el_kb_data.yml
    â”‚           â””â”€â”€ templates/
    â”‚               â”œâ”€â”€ entity_linker.jinja2
    â”‚               â”œâ”€â”€ lemma.jinja2
    â”‚               â”œâ”€â”€ ner.jinja2
    â”‚               â”œâ”€â”€ raw.jinja2
    â”‚               â”œâ”€â”€ sentiment.jinja2
    â”‚               â”œâ”€â”€ spancat.jinja2
    â”‚               â”œâ”€â”€ summarization.jinja2
    â”‚               â”œâ”€â”€ textcat.jinja2
    â”‚               â””â”€â”€ translation.jinja2
    â”œâ”€â”€ usage_examples/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ el_openai/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ el_kb_data.yml
    â”‚   â”‚   â”œâ”€â”€ examples.yml
    â”‚   â”‚   â”œâ”€â”€ fewshot.cfg
    â”‚   â”‚   â”œâ”€â”€ run_pipeline.py
    â”‚   â”‚   â””â”€â”€ zeroshot.cfg
    â”‚   â”œâ”€â”€ multitask_openai/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ examples.yml
    â”‚   â”‚   â”œâ”€â”€ fewshot.cfg
    â”‚   â”‚   â”œâ”€â”€ run_pipeline.py
    â”‚   â”‚   â””â”€â”€ zeroshot.cfg
    â”‚   â”œâ”€â”€ ner_dolly/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ examples.yml
    â”‚   â”‚   â”œâ”€â”€ examples_v2.yml
    â”‚   â”‚   â”œâ”€â”€ fewshot.cfg
    â”‚   â”‚   â”œâ”€â”€ fewshot_v2.cfg
    â”‚   â”‚   â”œâ”€â”€ run_pipeline.py
    â”‚   â”‚   â”œâ”€â”€ zeroshot.cfg
    â”‚   â”‚   â””â”€â”€ zeroshot_v2.cfg
    â”‚   â”œâ”€â”€ ner_langchain_openai/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ ner.cfg
    â”‚   â”‚   â””â”€â”€ run_pipeline.py
    â”‚   â”œâ”€â”€ ner_v3_openai/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ examples.json
    â”‚   â”‚   â”œâ”€â”€ fewshot.cfg
    â”‚   â”‚   â””â”€â”€ run_pipeline.py
    â”‚   â”œâ”€â”€ rel_openai/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ examples.jsonl
    â”‚   â”‚   â”œâ”€â”€ fewshot.cfg
    â”‚   â”‚   â”œâ”€â”€ run_pipeline.py
    â”‚   â”‚   â””â”€â”€ zeroshot.cfg
    â”‚   â”œâ”€â”€ streamlit/
    â”‚   â”‚   â””â”€â”€ streamlit_app.py
    â”‚   â”œâ”€â”€ tests/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ conftest.py
    â”‚   â”‚   â”œâ”€â”€ test_readme_examples.py
    â”‚   â”‚   â””â”€â”€ test_usage_examples.py
    â”‚   â”œâ”€â”€ textcat_dolly/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ examples.jsonl
    â”‚   â”‚   â”œâ”€â”€ fewshot.cfg
    â”‚   â”‚   â”œâ”€â”€ run_pipeline.py
    â”‚   â”‚   â””â”€â”€ zeroshot.cfg
    â”‚   â””â”€â”€ textcat_openai/
    â”‚       â”œâ”€â”€ README.md
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ examples.jsonl
    â”‚       â”œâ”€â”€ fewshot.cfg
    â”‚       â”œâ”€â”€ run_pipeline.py
    â”‚       â””â”€â”€ zeroshot.cfg
    â””â”€â”€ .github/
        â”œâ”€â”€ FUNDING.yml
        â”œâ”€â”€ PULL_REQUEST_TEMPLATE.md
        â””â”€â”€ workflows/
            â”œâ”€â”€ cibuildwheel.yml
            â”œâ”€â”€ explosionbot.yml
            â”œâ”€â”€ publish_pypi.yml
            â”œâ”€â”€ test.yml
            â”œâ”€â”€ test_external.yml
            â”œâ”€â”€ test_gpu.yml
            â””â”€â”€ validate.yml

================================================
FILE: README.md
================================================
<a href="https://explosion.ai"><img src="https://explosion.ai/assets/img/logo.svg" width="125" height="125" align="right" /></a>
<a href="https://explosion.ai"><img src="assets/logo.png" width="125" height="125" align="left" style="margin-right:30px" /></a>

<h1 align="center">
<span style="font: bold 38pt'Courier New';">spacy-llm</span>
<br>Structured NLP with LLMs
</h1>
<br><br>

[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/explosion/spacy-llm/test.yml?branch=main)](https://github.com/explosion/spacy-llm/actions/workflows/test.yml)
[![pypi Version](https://img.shields.io/pypi/v/spacy-llm.svg?style=flat-square&logo=pypi&logoColor=white)](https://pypi.org/project/spacy-llm/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/ambv/black)

This package integrates Large Language Models (LLMs) into [spaCy](https://spacy.io), featuring a modular system for **fast prototyping** and **prompting**, and turning unstructured responses into **robust outputs** for various NLP tasks, **no training data** required.

## Feature Highlight

- Serializable `llm` **component** to integrate prompts into your spaCy pipeline
- **Modular functions** to define the [**task**](https://spacy.io/api/large-language-models#tasks) (prompting and parsing) and [**model**](https://spacy.io/api/large-language-models#models)
- Interfaces with the APIs of
  - **[OpenAI](https://platform.openai.com/docs/api-reference/)**
  - **[Cohere](https://docs.cohere.com/reference/generate)**
  - **[Anthropic](https://docs.anthropic.com/claude/reference/)**
  - **[Google PaLM](https://ai.google/discover/palm2/)**
  - **[Microsoft Azure AI](https://azure.microsoft.com/en-us/solutions/ai)**
- Supports open-source LLMs hosted on Hugging Face ğŸ¤—:
  - **[Falcon](https://huggingface.co/tiiuae)**
  - **[Dolly](https://huggingface.co/databricks)**
  - **[Llama 2](https://huggingface.co/meta-llama)**
  - **[OpenLLaMA](https://huggingface.co/openlm-research)**
  - **[StableLM](https://huggingface.co/stabilityai)**
  - **[Mistral](https://huggingface.co/mistralai)**
- Integration with [LangChain](https://github.com/hwchase17/langchain) ğŸ¦œï¸ğŸ”— - all `langchain` models and features can be used in `spacy-llm`
- Tasks available out of the box:
  - [Named Entity Recognition](https://spacy.io/api/large-language-models#ner)
  - [Text classification](https://spacy.io/api/large-language-models#textcat)
  - [Lemmatization](https://spacy.io/api/large-language-models#lemma)
  - [Relationship extraction](https://spacy.io/api/large-language-models#rel)
  - [Sentiment analysis](https://spacy.io/api/large-language-models#sentiment)
  - [Span categorization](https://spacy.io/api/large-language-models#spancat)
  - [Summarization](https://spacy.io/api/large-language-models#summarization)
  - [Entity linking](https://spacy.io/api/large-language-models#nel)
  - [Translation](https://spacy.io/api/large-language-models#translation)
  - [Raw prompt execution for maximum flexibility](https://spacy.io/api/large-language-models#raw)
  - Soon:
    - Semantic role labeling
- Easy implementation of **your own functions** via [spaCy's registry](https://spacy.io/api/top-level#registry) for custom prompting, parsing and model integrations. For an example, see [here](https://spacy.io/usage/large-language-models#example-4).
- [Map-reduce approach](https://spacy.io/api/large-language-models#task-sharding) for splitting prompts too long for LLM's context window and fusing the results back together

## ğŸ§  Motivation

Large Language Models (LLMs) feature powerful natural language understanding capabilities. With only a few (and sometimes no) examples, an LLM can be prompted to perform custom NLP tasks such as text categorization, named entity recognition, coreference resolution, information extraction and more.

[spaCy](https://spacy.io) is a well-established library for building systems that need to work with language in various ways. spaCy's built-in components are generally powered by supervised learning or rule-based approaches.

Supervised learning is much worse than LLM prompting for prototyping, but for many tasks it's much better for production. A transformer model that runs comfortably on a single GPU is extremely powerful, and it's likely to be a better choice for any task for which you have a well-defined output. You train the model with anything from a few hundred to a few thousand labelled examples, and it will learn to do exactly that. Efficiency, reliability and control are all better with supervised learning, and accuracy will generally be higher than LLM prompting as well.

`spacy-llm` lets you have **the best of both worlds**. You can quickly initialize a pipeline with components powered by LLM prompts, and freely mix in components powered by other approaches. As your project progresses, you can look at replacing some or all of the LLM-powered components as you require.

Of course, there can be components in your system for which the power of an LLM is fully justified. If you want a system that can synthesize information from multiple documents in subtle ways and generate a nuanced summary for you, bigger is better. However, even if your production system needs an LLM for some of the task, that doesn't mean you need an LLM for all of it. Maybe you want to use a cheap text classification model to help you find the texts to summarize, or maybe you want to add a rule-based system to sanity check the output of the summary. These before-and-after tasks are much easier with a mature and well-thought-out library, which is exactly what spaCy provides.

## â³ Install

`spacy-llm` will be installed automatically in future spaCy versions. For now, you can run the following in the same virtual environment where you already have `spacy` [installed](https://spacy.io/usage).

```bash
python -m pip install spacy-llm
```

> âš ï¸ This package is still experimental and it is possible that changes made to the interface will be breaking in minor version updates.

## ğŸ Quickstart

Let's run some text classification using a GPT model from OpenAI. 

Create a new API key from openai.com or fetch an existing one, and ensure the
keys are set as environmental variables. For more background information, see
the documentation around setting [API keys](https://spacy.io/api/large-language-models#api-keys).

### In Python code

To do some quick experiments, from 0.5.0 onwards you can run:

```python
import spacy

nlp = spacy.blank("en")
llm = nlp.add_pipe("llm_textcat")
llm.add_label("INSULT")
llm.add_label("COMPLIMENT")
doc = nlp("You look gorgeous!")
print(doc.cats)
# {"COMPLIMENT": 1.0, "INSULT": 0.0}
```

By using the `llm_textcat` factory, the latest version of the built-in textcat task is used, 
as well as the default GPT-3-5 model from OpenAI.

### Using a config file

To control the various parameters of the `llm` pipeline, we can use 
[spaCy's config system](https://spacy.io/api/data-formats#config).
To start, create a config file `config.cfg` containing at least the following (or see the
full example
[here](https://github.com/explosion/spacy-llm/tree/main/usage_examples/textcat_openai)):

```ini
[nlp]
lang = "en"
pipeline = ["llm"]

[components]

[components.llm]
factory = "llm"

[components.llm.task]
@llm_tasks = "spacy.TextCat.v3"
labels = ["COMPLIMENT", "INSULT"]

[components.llm.model]
@llm_models = "spacy.GPT-4.v2"
```

Now run:

```python
from spacy_llm.util import assemble

nlp = assemble("config.cfg")
doc = nlp("You look gorgeous!")
print(doc.cats)
# {"COMPLIMENT": 1.0, "INSULT": 0.0}
```

That's it! There's a lot of other features - prompt templating, more tasks, logging etc. For more information on how to
use those, check out https://spacy.io/api/large-language-models.


## ğŸš€ Ongoing work

In the near future, we will

- Add more example tasks
- Support a broader range of models
- Provide more example use-cases and tutorials

PRs are always welcome!

## ğŸ“ï¸ Reporting issues

If you have questions regarding the usage of `spacy-llm`, or want to give us feedback after giving it a spin, please use
the [discussion board](https://github.com/explosion/spacy-llm/discussions).
Bug reports can be filed on the [spaCy issue tracker](https://github.com/explosion/spacy-llm/issues). Thank you!

## Migration guides

Please refer to our [migration guide](migration_guide.md).



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2023 Explosion

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: MANIFEST.in
================================================
recursive-include spacy_llm *.py *.txt *.cfg *.jinja2 *.jinja *.toml *.yml *.json *.jsonl
include LICENSE
include README.md
include pyproject.toml



================================================
FILE: migration_guide.md
================================================
# Migration guides

<details open>
  <summary>0.4.x to 0.5.x</summary>

## `0.4.x` to `0.5.x`

`0.5.x` includes internal refactoring that should have minimal to zero impact to the user experience.
Mostly, code and config files from `0.4.x` should just work on `0.5.x`.

### New Chain-of-Thought NER prompting

We've implemented Chain-of-Thought (CoT) prompting for SpanCat and NER tasks,
based on the
[PromptNER paper](https://arxiv.org/pdf/2305.15444.pdf) by Ashok and Lipton
(2023). This implementation is available as `spacy.SpanCat.v3` and `spacy.NER.v3`.
On an internal use-case, we've found this implementation to be much more accurate
than the `v1` and `v2` versions - with an increase of F-score of up to 15
percentage points.

For `v3`, zero-shot prompting should remain pretty much the same, though behind the scenes,
a dummy prompt example will be used by the CoT implementation.
For few-shot learning, the provided examples need to be provided in a slightly
[different format](https://spacy.io/api/large-language-models#ner) than the `v1` and `v2` versions.

First, you can provide an explicit `description` of what entities should look like.

In `0.4.x`:

```ini
[components.llm.task]
@llm_tasks = "spacy.NER.v2"
labels = ["DISH", "INGREDIENT", "EQUIPMENT"]
```

In `0.5.x`:

```ini
[components.llm.task]
@llm_tasks = "spacy.NER.v3"
labels = ["DISH", "INGREDIENT", "EQUIPMENT"]
description = Entities are the names food dishes,
    ingredients, and any kind of cooking equipment.
    Adjectives, verbs, adverbs are not entities.
    Pronouns are not entities.
```

Further, the examples for few-shot learning look different, and you can include both positive as well as negative examples
using the new fields `is_entity` and `reason`.

In `0.4.x`:

```json
[
  {
    "text": "You can't get a great chocolate flavor with carob.",
    "entities": {
      "INGREDIENT": ["carob"]
    }
  },
    ...
]
```

In `0.5.x`:

```json
[
    {
        "text": "You can't get a great chocolate flavor with carob.",
        "spans": [
            {
                "text": "chocolate",
                "is_entity": false,
                "label": "==NONE==",
                "reason": "is a flavor in this context, not an ingredient"
            },
            {
                "text": "carob",
                "is_entity": true,
                "label": "INGREDIENT",
                "reason": "is an ingredient to add chocolate flavor"
            }
        ]
    },
    ...
]
```

For a full example using 0.5.0 with Chain-of-Thought prompting for NER, see
[this usage example](https://github.com/explosion/spacy-llm/tree/main/usage_examples/ner_v3_openai).

</details>

<details>
  <summary>0.3.x to 0.4.x</summary>

## `0.3.x` to `0.4.x`

`0.4.x` significantly refactors the code to make it more robust and the config more intuitive. 0.4.0 changes the config
paradigm from `backend`- to `model`-centric. This is reflected in the external API in a different config structure.

Remember that there are three different types of models: the first uses the native REST implementation to communicate
with hosted LLMs, the second builds on HuggingFace's `transformers` model to run models locally and the third leverages
`langchain` to operate on hosted or local models. While the config for all three is rather similar (especially in
0.4.x), there are differences in how these models have to be configured. We show how to migrate your config from 0.3.x
to 0.4.x for each of these model types.

### All model types

- The registry name has changed - instead of `@llm_backends`, use `@llm_models`.
- The `api` attribute has been removed.

### Models using REST

This is the default method to communicate with hosted models. Whenever you don't explicitly use LangChain models
(see section at the bottom) or run models locally, you are using this kind of model.

In `0.3.x`:

```ini
[components.llm.backend]
@llm_backends = "spacy.REST.v1"
api = "OpenAI"
config = {"model": "gpt-3.5-turbo", "temperature": 0.3}
```

In `0.4.x`:

```ini
[components.llm.model]
@llm_models = "spacy.GPT-3-5.v1"
name = "gpt-3.5-turbo"
config = {"temperature": 0.3}
```

Note that the factory function (marked with `@`) refers to the name of the model. Variants of the same model can be
specified with the `name` attribute - for `gpt-3.5` this could be `"gpt-3.5-turbo"` or `"gpt-3.5-turbo-16k"`.

### Models using HuggingFace

On top of the changes described in the section above, HF models like `spacy.Dolly.v1` now accept `config_init` and
`config_run` to reflect that differerent arguments can be passed at init or run time.

In `0.3.x`:

```ini
[components.llm.backend]
@llm_backends = "spacy.Dolly_HF.v1"
model = "databricks/dolly-v2-3b"
config = {}
```

In `0.4.x`:

```ini
[components.llm.model]
@llm_models = "spacy.Dolly.v1"
name = "dolly-v2-3b"  # or databricks/dolly-v2-3b - the prefix is optional
config_init = {}  # Arguments passed to HF model at initialization time
config_run = {}  # Arguments passed to HF model at inference time
```

### Models using LangChain

LangChain models are now accessible via `langchain.[API].[version]`, e. g. `langchain.OpenAI.v1`. Other than that the
changes from 0.3.x to 0.4.x are identical with REST-based models.

In `0.3.x`:

```ini
[components.llm.backend]
@llm_backends = "spacy.LangChain.v1"
api = "OpenAI"
config = {"temperature": 0.3}
```

In `0.4.x`:

```ini
[components.llm.model]
@llm_models = "langchain.OpenAI.v1"
name = "gpt-3.5-turbo"
config = {"temperature": 0.3}
```

</details>



================================================
FILE: push-tags.sh
================================================
#!/usr/bin/env bash

set -e

# Insist repository is clean
git diff-index --quiet HEAD

git checkout $1
git pull origin $1
git push origin $1

version=$(grep "version = " setup.cfg)
version=${version/version = }
version=${version/\'/}
version=${version/\'/}
version=${version/\"/}
version=${version/\"/}
git tag "v$version"
git push origin "v$version"



================================================
FILE: pyproject.toml
================================================
[tool.ruff]
ignore = [
    "E501",
]
select = [
    "E",    # pycodestyle errors
    "W",    # pycodestyle warnings
    "F",    # Pyflakes
    "Q",    # flake8-quotes
    "T201"  # flake8-print
]
typing-modules = ['spacy_llm.compat']

[tool.pytest.ini_options]
testpaths = ["tests"]
filterwarnings = [
    "error",
    "ignore:^.*pkg_resources.*:DeprecationWarning",
    "ignore:.*function is now available as sqlalchemy.orm.declarative_base().*:",
    "ignore:^.*You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use.*:UserWarning",
    "ignore:^.*Xformers is not installed correctly.*:",
    "ignore:^.*The 'warn' method is deprecated, use 'warning' instead.*:DeprecationWarning",
    "ignore:^.*Support for class-based `config` is deprecated.*:",
    "ignore:^.*The `dict` method is deprecated; use `model_dump` instead.*",
    "ignore:^.*The `parse_obj` method is deprecated; use `model_validate` instead.*",
    "ignore:^.*`__get_validators__` is deprecated.*",
    "ignore:^.*The `construct` method is deprecated.*",
    "ignore:^.*Skipping device Apple Paravirtual device that does not support Metal 2.0.*",
    "ignore:^.*Pydantic V1 style `@validator` validators are deprecated.*",
    "ignore:^.*was deprecated in langchain-community.*"
]
markers = [
    "external: interacts with a (potentially cost-incurring) third-party API",
    "gpu: requires a GPU to run"
]

[tool.isort]
multi_line_output = 9
profile = "black"



================================================
FILE: requirements-dev.txt
================================================
pre-commit>=2.13.0
cython>=0.25,<3.0
pytest>=5.2.0,!=7.1.0
pytest-timeout>=1.3.0,<2.0.0
mock>=2.0.0,<3.0.0
mypy>=0.990,<1.1.0; platform_machine != "aarch64" and python_version >= "3.7"
black==22.3.0
types-requests==2.28.11.16
# Prompting libraries needed for testing
langchain>=0.1,<0.2; python_version>="3.9"
# Workaround for LangChain bug: pin OpenAI version. To be removed after LangChain has been fixed - see
# https://github.com/langchain-ai/langchain/issues/12967.
openai>=0.27,<=0.28.1; python_version>="3.9"

# Necessary for running all local models on GPU.
transformers[sentencepiece]>=4.0.0
torch
einops>=0.4

# Necessary for pytest checks and ignores.
sqlalchemy; python_version<"3.9"
pydantic


================================================
FILE: requirements.txt
================================================
spacy>=3.5.0,<4.0.0
confection>=0.1.3,<1.0.0
jinja2
pytest>=5.2.0,!=7.1.0
pytest-timeout>=1.3.0,<2.0.0
mock>=2.0.0,<3.0.0


================================================
FILE: setup.cfg
================================================
[metadata]
version = 0.7.3
description = Integrating LLMs into structured NLP pipelines
author = Explosion
author_email = contact@explosion.ai
license = MIT
long_description = file: README.md
long_description_content_type = text/markdown
classifiers =
    Development Status :: 4 - Beta
    Environment :: Console
    Intended Audience :: Developers
    Intended Audience :: Science/Research
    License :: OSI Approved :: MIT License
    Operating System :: POSIX :: Linux
    Operating System :: MacOS :: MacOS X
    Operating System :: Microsoft :: Windows
    Programming Language :: Python :: 3
    Programming Language :: Python :: 3.9
    Programming Language :: Python :: 3.10
    Programming Language :: Python :: 3.11
    Programming Language :: Python :: 3.12
    Topic :: Scientific/Engineering
project_urls =
    Release notes = https://github.com/explosion/spacy-llm/releases
    Source = https://github.com/explosion/spacy-llm

[options]
zip_safe = false
include_package_data = true
python_requires = >=3.9
install_requires =
    spacy>=3.5,<4.0
    jinja2
    confection>=0.1.3,<1.0.0

[options.entry_points]
spacy_factories =
    llm = spacy_llm.pipeline.llm:make_llm
spacy_misc =
    spacy.FewShotReader.v1 = spacy_llm.registry:fewshot_reader
    spacy.FileReader.v1 = spacy_llm.registry:file_reader

[options.extras_require]
langchain =
    langchain>=0.1,<0.2
transformers =
    torch>=1.13.1,<2.0
    transformers>=4.28.1,<5.0
    einops>=0.4
    xformers

[bdist_wheel]
universal = true

[sdist]
formats = gztar

[mypy]
ignore_missing_imports = true
no_implicit_optional = true
allow_redefinition = true

[tool:pytest]
markers = 
    external: interacts with a (potentially cost-incurring) third-party API
    gpu: requires a GPU to run
filterwarnings =
    ignore:pkg_resources:DeprecationWarning



================================================
FILE: setup.py
================================================
#!/usr/bin/env python

if __name__ == "__main__":
    from setuptools import setup, find_packages

    setup(name="spacy-llm", packages=find_packages())



================================================
FILE: .pre-commit-config.yaml
================================================
repos:
-   repo: https://github.com/charliermarsh/ruff-pre-commit
    rev: 'v0.0.272'
    hooks:
    - id: ruff

- repo: https://github.com/pycqa/isort
  rev: 5.12.0
  hooks:
    - id: isort
      name: isort (python)
    - id: isort
      name: isort (cython)
      types: [cython]
    - id: isort
      name: isort (pyi)
      types: [pyi]

-   repo: https://github.com/ambv/black
    rev: 22.3.0
    hooks:
    - id: black
      entry: black spacy_llm
      additional_dependencies: ['click==8.0.4']

-   repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.1.1
    hooks:
    - id: mypy
      name: mypy
      entry: mypy
      exclude: ^(.github/)
      language: python
      types: [python]
      require_serial: true
      args: [ "--ignore-missing-imports", "--no-implicit-optional", "--allow-redefinition" ]



================================================
FILE: spacy_llm/__init__.py
================================================
from . import cache  # noqa: F401
from . import models  # noqa: F401
from . import registry  # noqa: F401
from . import tasks  # noqa: F401
from .pipeline import llm
from .pipeline.llm import logger  # noqa: F401

__all__ = ["llm"]



================================================
FILE: spacy_llm/cache.py
================================================
import warnings
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Union

import numpy
import srsly  # type: ignore[import]
from spacy.tokens import Doc, DocBin
from spacy.vocab import Vocab

from .registry import registry
from .ty import PromptTemplateProvider, ShardingLLMTask


@registry.llm_misc("spacy.BatchCache.v1")
def make_cache(
    path: Optional[Union[str, Path]],
    batch_size: int,
    max_batches_in_mem: int,
):
    return BatchCache(
        path=path, batch_size=batch_size, max_batches_in_mem=max_batches_in_mem
    )


class BatchCache:
    """Utility class handling caching functionality for the `llm` component."""

    _INDEX_NAME: str = "index.jsonl"

    def __init__(
        self,
        path: Optional[Union[str, Path]],
        batch_size: int,
        max_batches_in_mem: int,
    ):
        """Initialize Cache instance. Acts as NoOp if path is None.
        path (Optional[Union[str,Path]]): Cache directory.
        batch_size (int): Number of docs in one batch (file).
        max_batches_in_mem (int): Max. number of batches to hold in memory.
        """
        self._path = Path(path) if path else None

        # Number of Docs in one batch.
        self._batch_size = batch_size
        # Max. number of batches to keep in memory.
        self.max_batches_in_mem = max_batches_in_mem
        self._vocab: Optional[Vocab] = None
        self._prompt_template: Optional[str] = None
        self._prompt_template_checked: bool = False

        # Stores doc hash -> batch hash to allow efficient lookup of available Docs.
        self._doc2batch: Dict[int, int] = {}
        # Hashes of batches loaded into memory.
        self._batch_hashes: List[int] = []
        # Container for currently loaded batch of Docs (batch hash -> doc hash -> Doc).
        self._loaded_docs: Dict[int, Dict[int, Doc]] = {}
        # Queue for processed, not yet persisted docs.
        self._cache_queue: List[Doc] = []
        # Statistics.
        self._stats: Dict[str, int] = {
            "hit": 0,
            "hit_contains": 0,
            "missed": 0,
            "missed_contains": 0,
            "added": 0,
            "persisted": 0,
        }

        self._init_cache_dir()

    def initialize(self, vocab: Vocab, task: ShardingLLMTask) -> None:
        """
        Initialize cache with data not available at construction time.
        vocab (Vocab): Vocab object.
        task (ShardingLLMTask): Task.
        """
        self._vocab = vocab
        if isinstance(task, PromptTemplateProvider):
            self.prompt_template = task.prompt_template
        else:
            self.prompt_template = ""
            if self._path:
                warnings.warn(
                    "The specified task does not provide its prompt template via `prompt_template()`. This means that "
                    "the cache cannot verify whether all cached documents were generated using the same prompt "
                    "template."
                )

    @property
    def prompt_template(self) -> Optional[str]:
        """Get prompt template.
        RETURNS (Optional[str]): Prompt template string used for docs to cache/cached docs.
        """
        return self._prompt_template

    @prompt_template.setter
    def prompt_template(self, prompt_template: str) -> None:
        """Set prompt template.
        prompt_template (str): Prompt template string used for docs to cache/cached docs.
        """
        self._prompt_template = prompt_template
        if not self._path:
            return

        # Store prompt template as file.
        prompt_template_path = self._path / "prompt_template.txt"
        # If no file exists: store in plain text for easier debugging.
        if not prompt_template_path.exists():
            with open(prompt_template_path, "w") as file:
                file.write(self._prompt_template)

        else:
            # If file exists and prompt template is not equal to new prompt template: raise, as this indicates we are
            # trying to reuse a cache directory with a changed prompt.
            with open(prompt_template_path, "r") as file:
                existing_prompt_template = "".join(file.readlines())
            if hash(existing_prompt_template) != hash(self._prompt_template):
                raise ValueError(
                    f"Prompt template in cache directory ({prompt_template_path}) is not equal with "
                    f"current prompt template. Reset your cache if you are using a new prompt "
                    f"template."
                )

    def _init_cache_dir(self) -> None:
        """Init cache directory with index and prompt template file."""
        if self._path is None:
            return

        if self._path.exists() and not self._path.is_dir():
            raise ValueError("Cache directory exists and is not a directory.")
        self._path.mkdir(parents=True, exist_ok=True)

        # Read from index file, if it exists.
        index_path = self._index_path
        if index_path.exists():
            for rec in srsly.read_jsonl(index_path):
                self._doc2batch = {
                    **self._doc2batch,
                    **{int(k): int(v) for k, v in rec.items()},
                }

    @property
    def _index_path(self) -> Path:
        """Returns full path to index file.
        RETURNS (Path): Full path to index file.
        """
        assert self._path is not None
        return self._path / BatchCache._INDEX_NAME

    def _batch_path(self, batch_id: int) -> Path:
        """Returns full path to batch file.
        batch_id (int): Batch id/hash
        RETURNS (Path): Full path to batch file.
        """
        assert self._path is not None
        return self._path / f"{batch_id}.spacy"

    @staticmethod
    def _doc_id(doc: Doc) -> int:
        """Generate a unique ID for one doc.
        doc (Doc): Doc to generate a unique ID for.
        RETURN (int): Unique ID for this doc.
        """
        return numpy.sum(doc.to_array(["ORTH"]), dtype=numpy.uint64).item()

    @staticmethod
    def _batch_id(doc_ids: Iterable[int]) -> int:
        """Generate a unique ID for a batch, given a set of doc ids
        doc_ids (Iterable[int]): doc ids
        RETURN (int): Unique ID for this batch.
        """
        return numpy.sum(
            numpy.asarray(doc_ids, dtype=numpy.uint64), dtype=numpy.uint64
        ).item()

    def add(self, doc: Doc) -> None:
        """Adds processed doc. Note: Adding a doc does _not_ mean that this doc is immediately persisted to disk. This
        happens only after the specified batch size has been reached or _persist() has been called explicitly.
        doc (Doc): Doc to add to persistence queue.
        """
        if self._path is None:
            return
        if not self._prompt_template_checked and self._prompt_template is None:
            warnings.warn(
                "No prompt template set for Cache object, entailing that consistency of prompt template used "
                "to generate docs cannot be checked. Be mindful to reset your cache whenever you change your "
                "prompt template."
            )
            self._prompt_template_checked = True

        self._cache_queue.append(doc)
        self._stats["added"] += 1
        if len(self._cache_queue) == self._batch_size:
            self._persist()

    def _persist(self) -> None:
        """Persists all processed docs in the queue to disk as one file."""
        assert self._path

        doc_ids = [self._doc_id(doc) for doc in self._cache_queue]
        batch_id = self._batch_id(doc_ids)

        for doc_id in doc_ids:
            self._doc2batch[doc_id] = batch_id

        batch_path = self._batch_path(batch_id)
        DocBin(docs=self._cache_queue, store_user_data=True).to_disk(batch_path)
        srsly.write_jsonl(
            self._index_path,
            lines=[{str(doc_id): str(batch_id)} for doc_id in doc_ids],
            append=True,
            append_new_line=False,
        )
        self._stats["persisted"] += len(self._cache_queue)
        self._cache_queue = []

    def __contains__(self, doc: Doc) -> bool:
        """Checks whether doc has been processed and cached.
        doc (Doc): Doc to check for.
        RETURNS (bool): Whether doc has been processed and cached.
        """
        if self._doc_id(doc) not in self._doc2batch:
            self._stats["missed_contains"] += 1
            return False
        self._stats["hit_contains"] += 1
        return True

    def __getitem__(self, doc: Doc) -> Optional[Doc]:
        """Returns processed doc, if available in cache. Note that if doc is not in the set of currently loaded
        documents, its batch will be loaded (and an older batch potentially discarded from memory).
        If doc is not in cache, None is returned.
        doc (Doc): Unprocessed doc whose processed equivalent should be returned.
        RETURNS (Optional[Doc]): Cached and processed version of doc, if available. Otherwise None.
        """
        doc_id = self._doc_id(doc)
        batch_id = self._doc2batch.get(doc_id, None)

        # Doc is not in cache.
        if not batch_id:
            self._stats["missed"] += 1
            return None
        self._stats["hit"] += 1

        # Doc's batch is currently not loaded.
        if batch_id not in self._loaded_docs:
            if self._path is None:
                raise ValueError(
                    "Cache directory path was not configured. Documents can't be read from cache."
                )
            if self._vocab is None:
                raise ValueError(
                    "Vocab must be set in order to Cache.__get_item__() to work."
                )

            # Discard batch, if maximal number of batches would be exceeded otherwise.
            if len(self._loaded_docs) == self.max_batches_in_mem:
                self._loaded_docs.pop(self._batch_hashes[0])
                self._batch_hashes = self._batch_hashes[1:]

            # Load target batch.
            self._batch_hashes.append(batch_id)
            self._loaded_docs[batch_id] = {
                self._doc_id(proc_doc): proc_doc
                for proc_doc in DocBin()
                .from_disk(self._batch_path(batch_id))
                .get_docs(self._vocab)
            }

        return self._loaded_docs[batch_id][doc_id]



================================================
FILE: spacy_llm/compat.py
================================================
# mypy: ignore-errors
import sys

if sys.version_info[:2] >= (3, 8):  # Python 3.8+
    from typing import Literal, Protocol, runtime_checkable
else:
    from typing_extensions import runtime_checkable  # noqa: F401
    from typing_extensions import Literal, Protocol  # noqa: F401

if sys.version_info[:2] >= (3, 9):  # Python 3.9+
    from typing import TypedDict  # noqa: F401
else:
    from typing_extensions import TypedDict  # noqa: F401

if sys.version_info[:2] >= (3, 11):  # Python 3.11+
    from typing import Self  # noqa: F401
else:
    from typing_extensions import Self  # noqa: F401

try:
    import langchain
    import langchain_community

    has_langchain = True
except (ImportError, AttributeError):
    langchain = None
    langchain_community = None
    has_langchain = False

try:
    import torch

    has_torch = True
except ImportError:
    torch = None
    has_torch = False

try:
    import transformers

    has_transformers = True
except ImportError:
    transformers = None
    has_transformers = False

try:
    import accelerate

    has_accelerate = True
except ImportError:
    accelerate = None
    has_accelerate = False


from pydantic import VERSION

PYDANTIC_V2 = VERSION.startswith("2.")

if PYDANTIC_V2:
    from pydantic.v1 import BaseModel, ExtraError, ValidationError  # noqa: F401
    from pydantic.v1 import validator
    from pydantic.v1.generics import GenericModel  # noqa: F401
else:
    from pydantic import BaseModel, ExtraError, ValidationError, validator  # noqa: F401
    from pydantic.generics import GenericModel  # noqa: F401



================================================
FILE: spacy_llm/ty.py
================================================
import abc
import inspect
import typing
import warnings
from pathlib import Path
from typing import Any, Callable, Dict, Generic, Iterable, List, Optional, Tuple, Type
from typing import TypeVar, Union, cast

from spacy.tokens import Doc
from spacy.training.example import Example
from spacy.vocab import Vocab

from .compat import GenericModel, Protocol, Self, runtime_checkable
from .models import langchain

_PromptType = Any
_ResponseType = Any
_ParsedResponseType = Any

PromptExecutorType = Callable[
    [Iterable[Iterable[_PromptType]]], Iterable[_ResponseType]
]
ExamplesConfigType = Union[
    Iterable[Dict[str, Any]], Callable[[], Iterable[Dict[str, Any]]], None
]
NTokenEstimator = Callable[[str], int]
ShardMapper = Callable[
    # Requires doc, doc index, context length and callable for rendering template from doc shard text.
    [Doc, int, int, Callable[[Doc, int, int, int], str]],
    # Returns each shard as a doc.
    Iterable[Doc],
]


@runtime_checkable
class Serializable(Protocol):
    def to_bytes(
        self,
        *,
        exclude: Tuple[str] = cast(Tuple[str], tuple()),
    ) -> bytes:
        ...

    def from_bytes(
        self,
        bytes_data: bytes,
        *,
        exclude: Tuple[str] = cast(Tuple[str], tuple()),
    ) -> bytes:
        ...

    def to_disk(
        self,
        path: Path,
        *,
        exclude: Tuple[str] = cast(Tuple[str], tuple()),
    ) -> bytes:
        ...

    def from_disk(
        self,
        path: Path,
        *,
        exclude: Tuple[str] = cast(Tuple[str], tuple()),
    ):
        ...


@runtime_checkable
class ScorableTask(Protocol):
    """Differs from Scorable in that it describes an object with a scorer() function, i. e. a scorable
    as checked for by the LLMWrapper component.
    """

    def scorer(
        self,
        examples: Iterable[Example],
    ) -> Dict[str, Any]:
        """Scores performance on examples."""


@runtime_checkable
class Scorer(Protocol):
    """Differs from ScorableTask in that it describes a Callable with a call signature matching the scorer()
    function + kwargs, i. e. a scorable as passed via the configuration.
    """

    def __call__(self, examples: Iterable[Example], **kwargs) -> Dict[str, Any]:
        """Score performance on examples.
        examples (Iterable[Example]): Examples to score.
        RETURNS (Dict[str, Any]): Dict with metric name -> score.
        """


@runtime_checkable
class ShardingLLMTask(Protocol):
    def generate_prompts(
        self, docs: Iterable[Doc], context_length: Optional[int] = None
    ) -> Iterable[Tuple[Iterable[_PromptType], Iterable[Doc]]]:
        """Generate prompts from docs.
        docs (Iterable[Doc]): Docs to generate prompts from.
        context_length (int): Context length for model this task is executed with. Needed for sharding and fusing docs,
            if the corresponding prompts exceed the context length. If None, context length is assumed to be infinite.
        RETURNS (Iterable[Tuple[Iterable[_PromptType], Iterable[Doc]]]): Iterable with one to n prompts per doc
            (multiple prompts in case of multiple shards) and the corresponding shards. The relationship between shard
            and prompt is 1:1.
        """

    def parse_responses(
        self,
        shards: Iterable[Iterable[Doc]],
        responses: Iterable[Iterable[_ResponseType]],
    ) -> Iterable[Doc]:
        """
        Parses LLM responses.
        docs (Iterable[Iterable[Doc]]): Doc shards to map responses into.
        responses ([Iterable[Iterable[_ResponseType]]]): LLM responses.
        RETURNS (Iterable[Doc]]): Updated (and fused) docs.
        """


@runtime_checkable
class NonshardingLLMTask(Protocol):
    def generate_prompts(self, docs: Iterable[Doc]) -> Iterable[_PromptType]:
        """Generate prompts from docs.
        docs (Iterable[Doc]): Docs to generate prompts from.
        RETURNS (Iterable[_PromptType]): Iterable with one prompt per doc.
        """

    def parse_responses(
        self, docs: Iterable[Doc], responses: Iterable[_ResponseType]
    ) -> Iterable[Doc]:
        """
        Parses LLM responses.
        docs (Iterable[Doc]): Docs to map responses into.
        responses ([Iterable[_ResponseType]]): LLM responses.
        RETURNS (Iterable[Doc]]): Updated docs.
        """


LLMTask = Union[NonshardingLLMTask, ShardingLLMTask]

TaskContraT = TypeVar(
    "TaskContraT", bound=Union[ShardingLLMTask, NonshardingLLMTask], contravariant=True
)
ShardingTaskContraT = TypeVar(
    "ShardingTaskContraT", bound=ShardingLLMTask, contravariant=True
)


@runtime_checkable
class ShardReducer(Protocol[ShardingTaskContraT]):
    """Generic protocol for tasks' shard reducer."""

    def __call__(self, task: ShardingTaskContraT, shards: Iterable[Doc]) -> Doc:
        """Merges shard to single Doc."""
        ...


class FewshotExample(GenericModel, abc.ABC, Generic[TaskContraT]):
    @classmethod
    @abc.abstractmethod
    def generate(cls, example: Example, task: TaskContraT) -> Optional[Self]:
        """Create a fewshot example from a spaCy example.
        example (Example): spaCy example.
        task (TaskContraT): Task for which to generate examples.
        RETURNS (Optional[Self]): Generated example. None, if example couldn't be generated.
        """


class TaskResponseParser(Protocol[TaskContraT]):
    """Generic protocol for parsing functions with specific tasks."""

    def __call__(
        self,
        task: TaskContraT,
        shards: Iterable[Iterable[Doc]],
        responses: Iterable[Iterable[Any]],
    ) -> Iterable[Iterable[Any]]:
        ...


@runtime_checkable
class PromptTemplateProvider(Protocol):
    @property
    def prompt_template(self) -> str:
        ...


@runtime_checkable
class LabeledTask(Protocol):
    @property
    def labels(self) -> Tuple[str, ...]:
        ...

    def add_label(self, label: str, label_definition: Optional[str] = None) -> int:
        ...

    def clear(self) -> None:
        ...


@runtime_checkable
class Cache(Protocol):
    """Defines minimal set of operations a cache implementiation needs to support."""

    def initialize(
        self, vocab: Vocab, task: Union[NonshardingLLMTask, ShardingLLMTask]
    ) -> None:
        """
        Initialize cache with data not available at construction time.
        vocab (Vocab): Vocab object.
        task (Union[LLMTask, ShardingLLMTask]): Task.
        """

    def add(self, doc: Doc) -> None:
        """Adds processed doc to cache (or to a queue that is added to the cache at a later point)
        doc (Doc): Doc to add to persistence queue.
        """

    @property
    def prompt_template(self) -> Optional[str]:
        """Get prompt template.
        RETURNS (Optional[str]): Prompt template string used for docs to cache/cached docs.
        """

    @prompt_template.setter
    def prompt_template(self, prompt_template: str) -> None:
        """Set prompt template.
        prompt_template (str): Prompt template string used for docs to cache/cached docs.
        """

    def __contains__(self, doc: Doc) -> bool:
        """Checks whether doc has been processed and cached.
        doc (Doc): Doc to check for.
        RETURNS (bool): Whether doc has been processed and cached.
        """

    def __getitem__(self, doc: Doc) -> Optional[Doc]:
        """Loads doc from cache. If doc is not in cache, None is returned.
        doc (Doc): Unprocessed doc whose processed equivalent should be returned.
        RETURNS (Optional[Doc]): Cached and processed version of doc, if available. Otherwise None.
        """


@runtime_checkable
class ModelWithContextLength(Protocol):
    @property
    def context_length(self) -> int:
        """Provides context length for the corresponding model.
        RETURNS (int): Context length for the corresponding model.
        """


def _do_args_match(out_arg: Iterable, in_arg: Iterable, nesting_level: int) -> bool:
    """Compares argument type of Iterables for compatibility.
    in_arg (Iterable): Input argument.
    out_arg (Iterable): Output argument.
    nesting_level (int): Expected level of nesting in types. E. g. Iterable[Iterable[Any]] has a level of 2,
        Iterable[Any] of 1. Note that this is assumed for all sub-types in out_arg and in_arg, as this is sufficient for
        the current use case of checking the compatibility of task-to-model and model-to-parser communication flow.
    RETURNS (bool): True if type variables are of the same length and if type variables in out_arg are a subclass
        of (or the same class as) the type variables in in_arg.
    """
    assert hasattr(out_arg, "__args__") and hasattr(in_arg, "__args__")

    out_types, in_types = out_arg, in_arg
    for level in range(nesting_level):
        out_types = (
            out_types.__args__[0] if level < (nesting_level - 1) else out_types.__args__  # type: ignore[attr-defined]
        )
        in_types = (
            in_types.__args__[0] if level < (nesting_level - 1) else in_types.__args__  # type: ignore[attr-defined]
        )
    # Replace Any with object to make issubclass() check work.
    out_types = [arg if arg != Any else object for arg in out_types]
    in_types = [arg if arg != Any else object for arg in in_types]

    if len(out_types) != len(in_types):
        return False

    return all(
        [
            issubclass(out_tv, in_tv) or issubclass(in_tv, out_tv)
            for out_tv, in_tv in zip(out_types, in_types)
        ]
    )


def _extract_model_call_signature(model: PromptExecutorType) -> Dict[str, Any]:
    """Extract call signature from model object.
    model (PromptExecutor): Model object to extract call signature from.
    RETURNS (Dict[str, Any]): Type per argument name.
    """
    if inspect.isfunction(model):
        return typing.get_type_hints(model)

    if not hasattr(model, "__call__"):
        raise ValueError("The object supplied as model must implement `__call__()`.")

    # Assume that __call__() has the necessary type info - except in the case of integration.Model, for which
    # we know this is not the case.
    if not isinstance(model, langchain.LangChain):
        return typing.get_type_hints(model.__call__)

    # If this is an instance of integrations.Model: read type information from .query() instead, only keep
    # information on Iterable args.
    signature = typing.get_type_hints(model.query).items()
    to_ignore: List[str] = []
    for k, v in signature:
        if not (hasattr(v, "__origin__") and issubclass(v.__origin__, Iterable)):
            to_ignore.append(k)

    signature = {
        k: v
        for k, v in typing.get_type_hints(model.query).items()
        # In Python 3.8+ (or 3.6+ if typing_utils is installed) the check for the origin class should be done using
        # typing.get_origin().
        if k not in to_ignore
    }
    assert len(signature) == 2
    assert "return" in signature
    return signature


def supports_sharding(task: Union[NonshardingLLMTask, ShardingLLMTask]) -> bool:
    """Determines task type, as isinstance(instance, Protocol) only checks for method names. This also considers
    argument and return types. Raises an exception if task is neither.
    Note that this is not as thorough as validate_type_consistency() and relies on clues to determine which task type
    a given, type-validated task type is. This doesn't guarantee that a task has valid typing. This method should only
    be in conjunction with validate_type_consistency().
    task (Union[LLMTask, ShardingLLMTask]): Task to check.
    RETURNS (bool): True if task supports sharding, False if not.
    """
    prompt_ret_type = typing.get_type_hints(task.generate_prompts)["return"].__args__[0]
    return (
        hasattr(prompt_ret_type, "_name")
        and prompt_ret_type._name == "Tuple"
        and len(prompt_ret_type.__args__) == 2
    )


def validate_type_consistency(
    task: Union[NonshardingLLMTask, ShardingLLMTask], model: PromptExecutorType
) -> None:
    """Check whether the types of the task and model signatures match.
    task (ShardingLLMTask): Specified task.
    model (PromptExecutor): Specified model.
    """
    # Raises an error or prints a warning if something looks wrong/odd.
    # todo update error messages
    if not isinstance(task, NonshardingLLMTask):
        raise ValueError(
            f"A task needs to adhere to the interface of either 'LLMTask' or 'ShardingLLMTask', but {type(task)} "
            f"doesn't."
        )
    if not hasattr(task, "generate_prompts"):
        raise ValueError(
            "A task needs to have the following method: generate_prompts(self, docs: Iterable[Doc]) -> "
            "Iterable[Tuple[Iterable[Any], Iterable[Doc]]]"
        )
    if not hasattr(task, "parse_responses"):
        raise ValueError(
            "A task needs to have the following method: "
            "parse_responses(self, docs: Iterable[Doc], responses: Iterable[Iterable[Any]]) -> Iterable[Doc]"
        )

    type_hints = {
        "template": typing.get_type_hints(task.generate_prompts),
        "parse": typing.get_type_hints(task.parse_responses),
        "model": _extract_model_call_signature(model),
    }

    parse_in: Optional[Type] = None
    model_in: Optional[Type] = None
    model_out: Optional[Type] = None

    # Validate the 'model' object
    if not (len(type_hints["model"]) == 2 and "return" in type_hints["model"]):
        raise ValueError(
            "The 'model' Callable should have one input argument and one return value."
        )
    for k in type_hints["model"]:
        if k == "return":
            model_out = type_hints["model"][k]
        else:
            model_in = type_hints["model"][k]

    # validate the 'parse' object
    if not (len(type_hints["parse"]) == 3 and "return" in type_hints["parse"]):
        raise ValueError(
            "The 'task.parse_responses()' function should have two input arguments and one return value."
        )
    for k in type_hints["parse"]:
        # find the 'prompt_responses' var without assuming its name
        type_k = type_hints["parse"][k]
        if type_k != typing.Iterable[Doc]:
            parse_in = type_hints["parse"][k]

    template_out = type_hints["template"]["return"]

    # Check that all variables are Iterables.
    for var, msg in (
        (template_out, "`task.generate_prompts()` needs to return an `Iterable`."),
        (
            model_in,
            "The prompts variable in the 'model' needs to be an `Iterable`.",
        ),
        (model_out, "The `model` function needs to return an `Iterable`."),
        (
            parse_in,
            "`responses` in `task.parse_responses()` needs to be an `Iterable`.",
        ),
    ):
        if not (hasattr(var, "_name") and var._name == "Iterable"):
            raise ValueError(msg)

    # Ensure that template/prompt generator output is Iterable of 2-Tuple, the second of which fits doc shards type.
    template_out_type = template_out.__args__[0]
    if (
        hasattr(template_out_type, "_name")
        and template_out_type._name == "Tuple"
        and len(template_out_type.__args__) == 2
    ):
        has_shards = True
        template_out_type = template_out_type.__args__[0]
    else:
        has_shards = False

    # Ensure that the template returns the same type as expected by the model
    assert model_in is not None
    if not _do_args_match(
        template_out_type if has_shards else typing.Iterable[template_out_type],  # type: ignore[valid-type]
        model_in.__args__[0],
        1,
    ):  # type: ignore[arg-type]
        warnings.warn(
            f"First type in value returned from `task.generate_prompts()` (`{template_out_type}`) doesn't match type "
            f"expected by `model` (`{model_in.__args__[0]}`)."
        )

    # Ensure that the parser expects the same type as returned by the model
    if not _do_args_match(model_out, parse_in if has_shards else typing.Iterable[parse_in], 2):  # type: ignore[arg-type,valid-type]
        warnings.warn(
            f"Type returned from `model` (`{model_out}`) doesn't match type expected by "
            f"`task.parse_responses()` (`{parse_in}`)."
        )



================================================
FILE: spacy_llm/util.py
================================================
from pathlib import Path
from typing import Any, Dict, Iterable, List, Union

from confection import Config, SimpleFrozenDict
from spacy.language import Language
from spacy.util import get_sourced_components, load_config, load_model_from_config


def split_labels(labels: Union[str, Iterable[str]]) -> List[str]:
    """Split a comma-separated list of labels.
    If input is a list already, just strip each entry of the list

    labels (Union[str, Iterable[str]]): comma-separated string or list of labels
    RETURNS (List[str]): a split and stripped list of labels
    """
    if not labels:
        return []
    labels = labels.split(",") if isinstance(labels, str) else labels
    return [label.strip() for label in labels]


def assemble_from_config(config: Config) -> Language:
    """Assemble a spaCy pipeline from a confection Config object.

    config (Config): Config to load spaCy pipeline from.
    RETURNS (Language): An initialized spaCy pipeline.
    """
    nlp = load_model_from_config(config, auto_fill=True)
    config = config.interpolate()
    sourced = get_sourced_components(config)
    nlp._link_components()
    with nlp.select_pipes(disable=[*sourced]):
        nlp.initialize()
    return nlp


def assemble(
    config_path: Union[str, Path], *, overrides: Dict[str, Any] = SimpleFrozenDict()
) -> Language:
    """Assemble a spaCy pipeline from a config file.

    config_path (Union[str, Path]): Path to config file.
    overrides (Dict[str, Any], optional): Dictionary of config overrides.
    RETURNS (Language): An initialized spaCy pipeline.
    """
    config_path = Path(config_path)
    config = load_config(config_path, overrides=overrides, interpolate=False)
    return assemble_from_config(config)



================================================
FILE: spacy_llm/models/__init__.py
================================================
from .hf import dolly_hf, openllama_hf, stablelm_hf
from .langchain import query_langchain
from .rest import anthropic, cohere, noop, openai, palm

__all__ = [
    "anthropic",
    "cohere",
    "openai",
    "dolly_hf",
    "noop",
    "stablelm_hf",
    "openllama_hf",
    "palm",
    "query_langchain",
]



================================================
FILE: spacy_llm/models/hf/__init__.py
================================================
from .base import HuggingFace
from .dolly import dolly_hf
from .falcon import falcon_hf
from .llama2 import llama2_hf
from .mistral import mistral_hf
from .openllama import openllama_hf
from .stablelm import stablelm_hf

__all__ = [
    "HuggingFace",
    "dolly_hf",
    "falcon_hf",
    "llama2_hf",
    "mistral_hf",
    "openllama_hf",
    "stablelm_hf",
]



================================================
FILE: spacy_llm/models/hf/base.py
================================================
import abc
import warnings
from typing import Any, Dict, Iterable, Optional, Tuple

from thinc.compat import has_torch_cuda_gpu

from ...compat import Literal, has_accelerate, has_torch, has_transformers, torch


class HuggingFace(abc.ABC):
    """Base class for HuggingFace model classes."""

    MODEL_NAMES = Literal[None]  # noqa: F722

    def __init__(
        self,
        name: str,
        config_init: Optional[Dict[str, Any]],
        config_run: Optional[Dict[str, Any]],
        context_length: Optional[int],
    ):
        """Initializes HF model instance.
        query (Callable[[Any, Iterable[Any]], Iterable[Any]): Callable executing LLM prompts when
            supplied with the `integration` object.
        name (str): Name of HF model to load (without account name).
        config_init (Optional[Dict[str, Any]]): HF config for initializing the model.
        config_run (Optional[Dict[str, Any]]): HF config for running the model.
        context_length (Optional[int]): Context length for this model. Necessary for sharding.
        """
        self._name = name if self.hf_account in name else f"{self.hf_account}/{name}"
        self._context_length = context_length
        default_cfg_init, default_cfg_run = self.compile_default_configs()
        self._config_init, self._config_run = default_cfg_init, default_cfg_run

        if config_init:
            self._config_init = {**self._config_init, **config_init}
        if config_run:
            self._config_run = {**self._config_run, **config_run}

        # `device` and `device_map` are conflicting arguments - ensure they aren't both set.
        if config_init:
            # Case 1: both device and device_map explicitly set by user.
            if "device" in config_init and "device_map" in config_init:
                warnings.warn(
                    "`device` and `device_map` are conflicting arguments - don't set both. Dropping argument "
                    "`device`."
                )
                self._config_init.pop("device")
            # Case 2: we have a CUDA GPU (and hence device="cuda:0" by default), but device_map is set by user.
            elif "device" in default_cfg_init and "device_map" in config_init:
                self._config_init.pop("device")
            # Case 3: we don't have a CUDA GPU (and hence "device_map=auto" by default), but device is set by user.
            elif "device_map" in default_cfg_init and "device" in config_init:
                self._config_init.pop("device_map")

        # Fetch proper torch.dtype, if specified.
        if (
            has_torch
            and "torch_dtype" in self._config_init
            and self._config_init["torch_dtype"] != "auto"
        ):
            try:
                self._config_init["torch_dtype"] = getattr(
                    torch, self._config_init["torch_dtype"]
                )
            except AttributeError as ex:
                raise ValueError(
                    f"Invalid value {self._config_init['torch_dtype']} was specified for `torch_dtype`. "
                    f"Double-check you specified a valid dtype."
                ) from ex

        # Init HF model.
        HuggingFace.check_installation()
        self._check_model()
        self._model = self.init_model()

    @abc.abstractmethod
    def __call__(self, prompts: Iterable[Iterable[Any]]) -> Iterable[Iterable[Any]]:
        """Executes prompts on specified API.
        prompts (Iterable[Iterable[Any]]): Prompts to execute per doc.
        RETURNS (Iterable[Iterable[Any]]): API responses per doc.
        """

    def _check_model(self) -> None:
        """Checks whether model is supported. Raises if it isn't."""
        if self._name.replace(f"{self.hf_account}/", "") not in self.get_model_names():
            raise ValueError(
                f"Model '{self._name}' is not supported - select one of {self.get_model_names()} instead"
            )

    @classmethod
    def get_model_names(cls) -> Tuple[str, ...]:
        """Names of supported models for this HF model implementation.
        RETURNS (Tuple[str]): Names of supported models.
        """
        return tuple(str(arg) for arg in cls.MODEL_NAMES.__args__)  # type: ignore[attr-defined]

    @property
    def context_length(self) -> Optional[int]:
        """Returns context length in number of tokens for this model.
        RETURNS (Optional[int]): Max. number of tokens allowed in prompt for the current model.
        """
        return self._context_length

    @property
    @abc.abstractmethod
    def hf_account(self) -> str:
        """Name of HF account for this model.
        RETURNS (str): Name of HF account.
        """

    @staticmethod
    def check_installation() -> None:
        """Checks whether the required external libraries are installed. Raises an error otherwise."""
        if not has_torch:
            raise ValueError(
                "The HF model requires `torch` to be installed, which it is not. See "
                "https://pytorch.org/ for installation instructions."
            )
        if not has_transformers:
            raise ValueError(
                "The HF model requires `transformers` to be installed, which it is not. See "
                "https://huggingface.co/docs/transformers/installation for installation instructions."
            )

    @staticmethod
    def compile_default_configs() -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """Compiles default init and run configs for HF model.
        RETURNS (Tuple[Dict[str, Any], Dict[str, Any]]): HF model default init config, HF model default run config.
        """
        default_cfg_init: Dict[str, Any] = {}
        default_cfg_run: Dict[str, Any] = {}

        if has_torch:
            default_cfg_init["torch_dtype"] = "bfloat16"
            if has_torch_cuda_gpu:
                # this ensures it fails explicitely when GPU is not enabled or sufficient
                default_cfg_init["device"] = "cuda:0"
            elif has_accelerate:
                # accelerate will distribute the layers depending on availability on GPU/CPU/hard drive
                default_cfg_init["device_map"] = "auto"
                warnings.warn(
                    "Couldn't find a CUDA GPU, so the setting 'device_map:auto' will be used, which may result "
                    "in the LLM being loaded (partly) on the CPU or even the hard disk, which may be slow. "
                    "Install cuda to be able to load and run the LLM on the GPU instead."
                )
            else:
                raise ValueError(
                    "Install CUDA to load and run the LLM on the GPU, or install 'accelerate' to dynamically "
                    "distribute the LLM on the CPU or even the hard disk. The latter may be slow."
                )

        return default_cfg_init, default_cfg_run

    @abc.abstractmethod
    def init_model(self) -> Any:
        """Sets up HF model and needed utilities.
        RETURNS (Any): HF model.
        """



================================================
FILE: spacy_llm/models/hf/dolly.py
================================================
from typing import Any, Callable, Dict, Iterable, Optional, Tuple

from confection import SimpleFrozenDict

from ...compat import Literal, transformers
from ...registry.util import registry
from . import HuggingFace


class Dolly(HuggingFace):
    MODEL_NAMES = Literal["dolly-v2-3b", "dolly-v2-7b", "dolly-v2-12b"]  # noqa: F722

    def init_model(self) -> Any:
        """Sets up HF model and needed utilities.
        RETURNS (Any): HF model.
        """
        return transformers.pipeline(
            model=self._name, return_full_text=False, **self._config_init
        )

    def __call__(self, prompts: Iterable[Iterable[str]]) -> Iterable[Iterable[str]]:  # type: ignore[override]
        """Queries Dolly HF model.
        pipeline (transformers.pipeline): Transformers pipeline to query.
        prompts (Iterable[Iterable[str]]): Prompts per doc to query Dolly model with.
        RETURNS (Iterable[Iterable[str]]): Prompt responses per doc.
        """
        return [
            [
                self._model(pr, **self._config_run)[0]["generated_text"]
                for pr in prompts_for_doc
            ]
            for prompts_for_doc in prompts
        ]

    @property
    def hf_account(self) -> str:
        return "databricks"

    @staticmethod
    def compile_default_configs() -> Tuple[Dict[str, Any], Dict[str, Any]]:
        default_cfg_init, default_cfg_run = HuggingFace.compile_default_configs()
        return (
            {
                **default_cfg_init,
                # Loads a custom pipeline from
                # https://huggingface.co/databricks/dolly-v2-3b/blob/main/instruct_pipeline.py
                # cf also https://huggingface.co/databricks/dolly-v2-12b
                "trust_remote_code": True,
            },
            default_cfg_run,
        )


@registry.llm_models("spacy.Dolly.v1")
def dolly_hf(
    name: Dolly.MODEL_NAMES,
    config_init: Optional[Dict[str, Any]] = SimpleFrozenDict(),
    config_run: Optional[Dict[str, Any]] = SimpleFrozenDict(),
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Generates Dolly instance that can execute a set of prompts and return the raw responses.
    name (Literal): Name of the Dolly model. Has to be one of Dolly.get_model_names().
    config_init (Optional[Dict[str, Any]]): HF config for initializing the model.
    config_run (Optional[Dict[str, Any]]): HF config for running the model.
    RETURNS (Callable[[Iterable[str]], Iterable[str]]): Dolly instance that can execute a set of prompts and return
        the raw responses.
    """
    return Dolly(
        name=name, config_init=config_init, config_run=config_run, context_length=2048
    )



================================================
FILE: spacy_llm/models/hf/falcon.py
================================================
from typing import Any, Callable, Dict, Iterable, Optional, Tuple

from confection import SimpleFrozenDict

from ...compat import Literal, transformers
from ...registry.util import registry
from .base import HuggingFace


class Falcon(HuggingFace):
    MODEL_NAMES = Literal[
        "falcon-rw-1b", "falcon-7b", "falcon-7b-instruct", "falcon-40b-instruct"
    ]  # noqa: F722

    def __init__(
        self,
        name: MODEL_NAMES,
        config_init: Optional[Dict[str, Any]],
        config_run: Optional[Dict[str, Any]],
        context_length: Optional[int],
    ):
        self._tokenizer: Optional["transformers.AutoTokenizer"] = None
        super().__init__(
            name=name,
            config_init=config_init,
            config_run=config_run,
            context_length=context_length,
        )

        assert isinstance(self._tokenizer, transformers.PreTrainedTokenizerBase)
        self._config_run["pad_token_id"] = self._tokenizer.pad_token_id

        # Instantiate GenerationConfig object from config dict.
        self._hf_config_run = transformers.GenerationConfig.from_pretrained(
            self._name, **self._config_run
        )
        # To avoid deprecation warning regarding usage of `max_length`.
        self._hf_config_run.max_new_tokens = self._hf_config_run.max_length

    def init_model(self) -> Any:
        self._tokenizer = transformers.AutoTokenizer.from_pretrained(self._name)
        return transformers.pipeline(
            "text-generation",
            model=self._name,
            tokenizer=self._tokenizer,
            return_full_text=False,
            **self._config_init,
        )

    @property
    def hf_account(self) -> str:
        return "tiiuae"

    def __call__(self, prompts: Iterable[Iterable[str]]) -> Iterable[Iterable[str]]:  # type: ignore[override]
        return [
            [
                self._model(pr, generation_config=self._hf_config_run)[0][
                    "generated_text"
                ]
                for pr in prompts_for_doc
            ]
            for prompts_for_doc in prompts
        ]

    @staticmethod
    def compile_default_configs() -> Tuple[Dict[str, Any], Dict[str, Any]]:
        default_cfg_init, default_cfg_run = HuggingFace.compile_default_configs()
        return (
            {
                **default_cfg_init,
                "trust_remote_code": True,
            },
            default_cfg_run,
        )


@registry.llm_models("spacy.Falcon.v1")
def falcon_hf(
    name: Falcon.MODEL_NAMES,
    config_init: Optional[Dict[str, Any]] = SimpleFrozenDict(),
    config_run: Optional[Dict[str, Any]] = SimpleFrozenDict(),
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Generates Falcon instance that can execute a set of prompts and return the raw responses.
    name (Literal): Name of the Falcon model. Has to be one of Falcon.get_model_names().
    config_init (Optional[Dict[str, Any]]): HF config for initializing the model.
    config_run (Optional[Dict[str, Any]]): HF config for running the model.
    RETURNS (Callable[[Iterable[str]], Iterable[str]]): Falcon instance that can execute a set of prompts and return
        the raw responses.
    """
    return Falcon(
        name=name, config_init=config_init, config_run=config_run, context_length=2048
    )



================================================
FILE: spacy_llm/models/hf/llama2.py
================================================
from typing import Any, Callable, Dict, Iterable, Optional, Tuple

from confection import SimpleFrozenDict

from ...compat import Literal, transformers
from ...registry.util import registry
from .base import HuggingFace


class Llama2(HuggingFace):
    MODEL_NAMES = Literal[
        "Llama-2-7b-hf", "Llama-2-13b-hf", "Llama-2-70b-hf"
    ]  # noqa: F722

    def __init__(
        self,
        name: MODEL_NAMES,
        config_init: Optional[Dict[str, Any]],
        config_run: Optional[Dict[str, Any]],
        context_length: Optional[int],
    ):
        super().__init__(
            name=name,
            config_init=config_init,
            config_run=config_run,
            context_length=context_length,
        )
        # Instantiate GenerationConfig object from config dict.
        self._hf_config_run = transformers.GenerationConfig.from_pretrained(
            self._name,
            **self._config_run,
        )
        # To avoid deprecation warning regarding usage of `max_length`.
        self._hf_config_run.max_new_tokens = self._hf_config_run.max_length

    def init_model(self) -> Any:
        return transformers.pipeline(
            "text-generation",
            model=self._name,
            return_full_text=False,
            **self._config_init,
        )

    @property
    def hf_account(self) -> str:
        return "meta-llama"

    def __call__(self, prompts: Iterable[Iterable[str]]) -> Iterable[Iterable[str]]:  # type: ignore[override]
        return [
            [
                self._model(pr, generation_config=self._hf_config_run)[0][
                    "generated_text"
                ]
                for pr in prompts_for_doc
            ]
            for prompts_for_doc in prompts
        ]

    @staticmethod
    def compile_default_configs() -> Tuple[Dict[str, Any], Dict[str, Any]]:
        return HuggingFace.compile_default_configs()


@registry.llm_models("spacy.Llama2.v1")
def llama2_hf(
    name: Llama2.MODEL_NAMES,
    config_init: Optional[Dict[str, Any]] = SimpleFrozenDict(),
    config_run: Optional[Dict[str, Any]] = SimpleFrozenDict(),
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Generates Llama 2 instance that can execute a set of prompts and return the raw responses.
    name (Literal): Name of the Llama 2 model. Has to be one of Llama2.get_model_names().
    config_init (Optional[Dict[str, Any]]): HF config for initializing the model.
    config_run (Optional[Dict[str, Any]]): HF config for running the model.
    RETURNS (Callable[[Iterable[str]], Iterable[str]]): Llama2 instance that can execute a set of prompts and return
        the raw responses.
    """
    return Llama2(
        name=name, config_init=config_init, config_run=config_run, context_length=4096
    )



================================================
FILE: spacy_llm/models/hf/mistral.py
================================================
from typing import Any, Callable, Dict, Iterable, List, Optional

from confection import SimpleFrozenDict

from ...compat import Literal, transformers
from ...registry.util import registry
from .base import HuggingFace


class Mistral(HuggingFace):
    MODEL_NAMES = Literal["Mistral-7B-v0.1", "Mistral-7B-Instruct-v0.1"]  # noqa: F722

    def __init__(
        self,
        name: MODEL_NAMES,
        config_init: Optional[Dict[str, Any]],
        config_run: Optional[Dict[str, Any]],
        context_length: Optional[int],
    ):
        self._tokenizer: Optional["transformers.AutoTokenizer"] = None
        self._is_instruct = "instruct" in name
        super().__init__(
            name=name,
            config_init=config_init,
            config_run=config_run,
            context_length=context_length,
        )

        assert isinstance(self._tokenizer, transformers.PreTrainedTokenizerBase)

        # Instantiate GenerationConfig object from config dict.
        self._hf_config_run = transformers.GenerationConfig.from_pretrained(
            self._name, **self._config_run
        )
        # To avoid deprecation warning regarding usage of `max_length`.
        self._hf_config_run.max_new_tokens = self._hf_config_run.max_length

    def init_model(self) -> Any:
        self._tokenizer = transformers.AutoTokenizer.from_pretrained(self._name)
        init_cfg = self._config_init
        device: Optional[str] = None
        if "device" in init_cfg:
            device = init_cfg.pop("device")

        model = transformers.AutoModelForCausalLM.from_pretrained(
            self._name, **init_cfg, resume_download=True
        )
        if device:
            model.to(device)

        return model

    @property
    def hf_account(self) -> str:
        return "mistralai"

    def __call__(self, prompts: Iterable[Iterable[str]]) -> Iterable[Iterable[str]]:  # type: ignore[override]
        assert callable(self._tokenizer)
        assert hasattr(self._model, "generate")
        assert hasattr(self._tokenizer, "batch_decode")
        responses: List[List[str]] = []

        for prompts_for_doc in prompts:
            prompts_for_doc = list(prompts_for_doc)

            tokenized_input_ids = [
                self._tokenizer(
                    prompt if not self._is_instruct else f"<s>[INST] {prompt} [/INST]",
                    return_tensors="pt",
                ).input_ids
                for prompt in prompts_for_doc
            ]
            tokenized_input_ids = [
                tp.to(self._model.device) for tp in tokenized_input_ids
            ]

            responses.append(
                [
                    self._tokenizer.decode(
                        self._model.generate(
                            input_ids=tok_ii, generation_config=self._hf_config_run
                        )[:, tok_ii.shape[1] :][0],
                        skip_special_tokens=True,
                    )
                    for tok_ii in tokenized_input_ids
                ]
            )

        return responses


@registry.llm_models("spacy.Mistral.v1")
def mistral_hf(
    name: Mistral.MODEL_NAMES,
    config_init: Optional[Dict[str, Any]] = SimpleFrozenDict(),
    config_run: Optional[Dict[str, Any]] = SimpleFrozenDict(),
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Generates Mistral instance that can execute a set of prompts and return the raw responses.
    name (Literal): Name of the Mistral model. Has to be one of Mistral.get_model_names().
    config_init (Optional[Dict[str, Any]]): HF config for initializing the model.
    config_run (Optional[Dict[str, Any]]): HF config for running the model.
    RETURNS (Callable[[Iterable[str]], Iterable[str]]): Mistral instance that can execute a set of prompts and return
        the raw responses.
    """
    return Mistral(
        name=name, config_init=config_init, config_run=config_run, context_length=8000
    )



================================================
FILE: spacy_llm/models/hf/openllama.py
================================================
from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple

from confection import SimpleFrozenDict

from ...compat import Literal, transformers
from ...registry.util import registry
from .base import HuggingFace


class OpenLLaMA(HuggingFace):
    MODEL_NAMES = Literal[
        "open_llama_3b",  # noqa: F722
        "open_llama_7b",  # noqa: F722
        "open_llama_7b_v2",  # noqa: F722
        "open_llama_13b",  # noqa: F722
    ]

    def __init__(
        self,
        name: str,
        config_init: Optional[Dict[str, Any]],
        config_run: Optional[Dict[str, Any]],
        context_length: Optional[int],
    ):
        self._tokenizer: Optional["transformers.AutoTokenizer"] = None
        super().__init__(
            name=name,
            config_init=config_init,
            config_run=config_run,
            context_length=context_length,
        )

    def init_model(self) -> "transformers.AutoModelForCausalLM":
        """Sets up HF model and needed utilities.
        RETURNS (Any): HF model.
        """
        # Initialize tokenizer and model.
        self._tokenizer = transformers.AutoTokenizer.from_pretrained(self._name)
        init_cfg = self._config_init
        device: Optional[str] = None
        if "device" in init_cfg:
            device = init_cfg.pop("device")

        model = transformers.AutoModelForCausalLM.from_pretrained(
            self._name, **init_cfg
        )
        if device:
            model.to(device)

        return model

    def __call__(self, prompts: Iterable[Iterable[str]]) -> Iterable[Iterable[str]]:  # type: ignore[override]
        assert callable(self._tokenizer)
        responses: List[List[str]] = []

        for prompts_for_doc in prompts:
            tokenized_input_ids = [
                self._tokenizer(prompt, return_tensors="pt").input_ids
                for prompt in prompts_for_doc
            ]
            tokenized_input_ids = [
                tii.to(self._model.device) for tii in tokenized_input_ids
            ]

            assert hasattr(self._model, "generate")
            responses.append(
                [
                    self._tokenizer.decode(
                        self._model.generate(input_ids=tii, **self._config_run)[
                            :, tii.shape[1] :
                        ][0],
                    )
                    for tii in tokenized_input_ids
                ]
            )

        return responses

    @property
    def hf_account(self) -> str:
        return "openlm-research"

    @staticmethod
    def compile_default_configs() -> Tuple[Dict[str, Any], Dict[str, Any]]:
        default_cfg_init, default_cfg_run = HuggingFace.compile_default_configs()
        return (
            {
                **default_cfg_init,
                "torch_dtype": "float16",
            },
            {**default_cfg_run, "max_new_tokens": 32},
        )


@registry.llm_models("spacy.OpenLLaMA.v1")
def openllama_hf(
    name: OpenLLaMA.MODEL_NAMES,
    config_init: Optional[Dict[str, Any]] = SimpleFrozenDict(),
    config_run: Optional[Dict[str, Any]] = SimpleFrozenDict(),
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Generates OpenLLaMA instance that can execute a set of prompts and return the raw responses.
    name (Literal): Name of the OpenLLaMA model. Has to be one of OpenLLaMA.get_model_names().
    config_init (Optional[Dict[str, Any]]): HF config for initializing the model.
    config_run (Optional[Dict[str, Any]]): HF config for running the model.
    RETURNS (Callable[[Iterable[str]], Iterable[str]]): OpenLLaMA instance that can execute a set of prompts and return
        the raw responses.
    """
    return OpenLLaMA(
        name=name, config_init=config_init, config_run=config_run, context_length=2048
    )



================================================
FILE: spacy_llm/models/hf/stablelm.py
================================================
from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple

from confection import SimpleFrozenDict

from ...compat import Literal, has_torch, has_transformers, torch, transformers
from ...registry.util import registry
from .base import HuggingFace

if has_transformers and has_torch:

    class _StopOnTokens(transformers.StoppingCriteria):
        def __call__(
            self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs
        ) -> bool:
            stop_ids = [50278, 50279, 50277, 1, 0]
            for stop_id in stop_ids:
                if input_ids[0][-1] == stop_id:
                    return True
            return False


class StableLM(HuggingFace):
    MODEL_NAMES = Literal[
        "stablelm-base-alpha-3b",  # noqa: F722
        "stablelm-base-alpha-7b",  # noqa: F722
        "stablelm-tuned-alpha-3b",  # noqa: F722
        "stablelm-tuned-alpha-7b",  # noqa: F722
    ]
    _SYSTEM_PROMPT = """
<|SYSTEM|># StableLM Tuned (Alpha version)
- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.
- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.
- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.
- StableLM will refuse to participate in anything that could harm a human.
"""

    def __init__(
        self,
        name: str,
        config_init: Optional[Dict[str, Any]],
        config_run: Optional[Dict[str, Any]],
        context_length: Optional[int],
    ):
        self._tokenizer: Optional["transformers.AutoTokenizer"] = None
        self._is_tuned = "tuned" in name
        super().__init__(
            name=name,
            config_init=config_init,
            config_run=config_run,
            context_length=context_length,
        )

    def init_model(self) -> "transformers.AutoModelForCausalLM":
        """Sets up HF model and needed utilities.
        RETURNS (Any): HF model.
        """
        self._tokenizer = transformers.AutoTokenizer.from_pretrained(self._name)
        init_cfg = self._config_init
        device: Optional[str] = None
        if "device" in init_cfg:
            device = init_cfg.pop("device")

        model = transformers.AutoModelForCausalLM.from_pretrained(
            self._name, **init_cfg
        )
        if device:
            model.half().to(device)

        return model

    @property
    def hf_account(self) -> str:
        return "stabilityai"

    def __call__(self, prompts: Iterable[Iterable[str]]) -> Iterable[Iterable[str]]:  # type: ignore[override]
        assert callable(self._tokenizer)
        responses: List[List[str]] = []

        for prompts_for_doc in prompts:
            tokenized_input_ids = [
                self._tokenizer(prompt, return_tensors="pt").input_ids
                for prompt in (
                    # Add prompt formatting for tuned model.
                    prompts_for_doc
                    if not self._is_tuned
                    else [
                        f"{StableLM._SYSTEM_PROMPT}<|USER|>{prompt}<|ASSISTANT|>"
                        for prompt in prompts_for_doc
                    ]
                )
            ]
            tokenized_input_ids = [
                tp.to(self._model.device) for tp in tokenized_input_ids
            ]

            assert hasattr(self._model, "generate")
            responses.append(
                [
                    self._tokenizer.decode(
                        self._model.generate(input_ids=tii, **self._config_run)[
                            :, tii.shape[1] :
                        ][0],
                        skip_special_tokens=True,
                    )
                    for tii in tokenized_input_ids
                ]
            )

        return responses

    @staticmethod
    def compile_default_configs() -> Tuple[Dict[str, Any], Dict[str, Any]]:
        default_cfg_init, default_cfg_run = HuggingFace.compile_default_configs()
        return (
            default_cfg_init,
            {
                **default_cfg_run,
                "max_new_tokens": 64,
                "temperature": 0.7,
                "do_sample": True,
            },
        )


@registry.llm_models("spacy.StableLM.v1")
def stablelm_hf(
    name: StableLM.MODEL_NAMES,
    config_init: Optional[Dict[str, Any]] = SimpleFrozenDict(),
    config_run: Optional[Dict[str, Any]] = SimpleFrozenDict(),
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Generates StableLM instance that can execute a set of prompts and return the raw responses.
    name (Literal): Name of the StableLM model. Has to be one of StableLM.get_model_names().
    config_init (Optional[Dict[str, Any]]): HF config for initializing the model.
    config_run (Optional[Dict[str, Any]]): HF config for running the model.
    RETURNS (Callable[[Iterable[str]], Iterable[str]]): StableLM instance that can execute a set of prompts and return
        the raw responses.
    """
    if name not in StableLM.get_model_names():
        raise ValueError(
            f"Expected one of {StableLM.get_model_names()}, but received {name}."
        )
    return StableLM(
        name=name, config_init=config_init, config_run=config_run, context_length=4096
    )



================================================
FILE: spacy_llm/models/langchain/__init__.py
================================================
from .model import LangChain, query_langchain

# Dynamically register LangChain API classes as individual models, if this hasn't been done yet.
LangChain.register_models()

__all__ = [
    "LangChain",
    "query_langchain",
]



================================================
FILE: spacy_llm/models/langchain/model.py
================================================
from typing import Any, Callable, Dict, Iterable, Optional, Type

from confection import SimpleFrozenDict

from ...compat import ExtraError, ValidationError, has_langchain, langchain_community
from ...registry import registry

try:
    from langchain_community import llms  # noqa: F401
except (ImportError, AttributeError):
    llms = None


class LangChain:
    def __init__(
        self,
        name: str,
        api: str,
        config: Dict[Any, Any],
        query: Callable[
            ["langchain_community.llms.BaseLLM", Iterable[Iterable[Any]]],
            Iterable[Iterable[Any]],
        ],
        context_length: Optional[int],
    ):
        """Initializes model instance for integration APIs.
        name (str): Name of LangChain model to instantiate.
        api (str): Name of class/API.
        config (Dict[Any, Any]): Config passed on to LangChain model.
        query (Callable[[langchain_community.llms.BaseLLM, Iterable[Iterable[Any]]], Iterable[Iterable[Any]]]): Callable
            executing LLM prompts when supplied with the model instance.
        context_length (Optional[int]): Context length for this model. Only necessary for sharding. If no context
            length provided, prompts can't be sharded.
        """
        self._langchain_model = LangChain._init_langchain_model(name, api, config)
        self.query = query
        self._context_length = context_length
        self._check_installation()

    @classmethod
    def _init_langchain_model(
        cls, name: str, api: str, config: Dict[Any, Any]
    ) -> "langchain_community.llms.BaseLLM":
        """Initializes langchain model. langchain expects a range of different model ID argument names, depending on the
        model class. There doesn't seem to be a clean way to determine those from the outset, we'll fail our way through
        them.
        Includes error checks for model ID arguments.
        name (str): Name of LangChain model to instantiate.
        api (str): Name of class/API.
        config (Dict[Any, Any]): Config passed on to LangChain model.
        """
        model_init_args = ["model", "model_name", "model_id"]
        for model_init_arg in model_init_args:
            try:
                return cls.get_type_to_cls_dict()[api](
                    **{model_init_arg: name}, **config
                )
            except ValidationError as err:
                if model_init_arg == model_init_args[-1]:
                    # If init error indicates that model ID arg is extraneous: raise error with hint on how to proceed.
                    if any(
                        [
                            rerr
                            for rerr in err.raw_errors
                            if isinstance(rerr.exc, ExtraError)
                            and model_init_arg in rerr.loc_tuple()
                        ]
                    ):
                        raise ValueError(
                            "Couldn't initialize LangChain model with known model ID arguments. Please report this to "
                            "https://github.com/explosion/spacy-llm/issues. Thank you!"
                        ) from err
                    # Otherwise: raise error as-is.
                    raise err

    @staticmethod
    def get_type_to_cls_dict() -> Dict[str, Type["langchain_community.llms.BaseLLM"]]:
        """Returns langchain_community.llms.type_to_cls_dict.
        RETURNS (Dict[str, Type[langchain_community.llms.BaseLLM]]): langchain_community.llms.type_to_cls_dict.
        """
        return {
            llm_id: getattr(langchain_community.llms, llm_id)
            for llm_id in langchain_community.llms.__all__
        }

    def __call__(self, prompts: Iterable[Iterable[Any]]) -> Iterable[Iterable[Any]]:
        """Executes prompts on specified API.
        prompts (Iterable[Iterable[Any]]): Prompts to execute.
        RETURNS (Iterable[Iterable[Any]]): API responses.
        """
        return self.query(self._langchain_model, prompts)

    @staticmethod
    def query_langchain(
        model: "langchain_community.llms.BaseLLM", prompts: Iterable[Iterable[Any]]
    ) -> Iterable[Iterable[Any]]:
        """Query LangChain model naively.
        model (langchain_community.llms.BaseLLM): LangChain model.
        prompts (Iterable[Iterable[Any]]): Prompts to execute.
        RETURNS (Iterable[Iterable[Any]]): LLM responses.
        """
        return [
            [model.invoke(pr) for pr in prompts_for_doc] for prompts_for_doc in prompts
        ]

    @staticmethod
    def _check_installation() -> None:
        """Checks whether `langchain` is installed. Raises an error otherwise."""
        if not has_langchain:
            raise ValueError(
                "The LangChain model requires `langchain` to be installed, which it is not. See "
                "https://github.com/hwchase17/langchain for installation instructions."
            )

    @staticmethod
    def _langchain_model_maker(class_id: str):
        def langchain_model(
            name: str,
            query: Optional[
                Callable[
                    ["langchain_community.llms.BaseLLM", Iterable[Iterable[str]]],
                    Iterable[Iterable[str]],
                ]
            ] = None,
            config: Dict[Any, Any] = SimpleFrozenDict(),
            context_length: Optional[int] = None,
            langchain_class_id: str = class_id,
        ) -> Optional[Callable[[Iterable[Iterable[Any]]], Iterable[Iterable[Any]]]]:
            try:
                return LangChain(
                    name=name,
                    api=langchain_class_id,
                    config=config,
                    query=query_langchain() if query is None else query,
                    context_length=context_length,
                )
            except ImportError as err:
                raise ValueError(
                    f"Failed to instantiate LangChain model {langchain_class_id}. Ensure all necessary dependencies "
                    f"are installed."
                ) from err

        return langchain_model

    @property
    def context_length(self) -> Optional[int]:
        """Returns context length in number of tokens for this model.
        RETURNS (Optional[int]): Max. number of tokens in allowed in prompt for the current model. None if unknown.
        """
        return self._context_length

    @staticmethod
    def register_models() -> None:
        """Registers APIs supported by langchain (one API is registered as one model).
        Doesn't attempt to register anything if LangChain isn't installed or at least one LangChain model has been
        registered already.
        """
        if not has_langchain or any(
            [
                (handle.startswith("langchain"))
                for handle in registry.llm_models.get_all()
            ]
        ):
            return

        for class_id, cls in LangChain.get_type_to_cls_dict().items():
            registry.llm_models.register(
                f"langchain.{cls.__name__}.v1",
                func=LangChain._langchain_model_maker(class_id=class_id),
            )


@registry.llm_queries("spacy.CallLangChain.v1")
def query_langchain() -> (
    Callable[
        ["langchain_community.llms.BaseLLM", Iterable[Iterable[Any]]],
        Iterable[Iterable[Any]],
    ]
):
    """Returns query Callable for LangChain.
    RETURNS (Callable[["langchain_community.llms.BaseLLM", Iterable[Iterable[Any]]], Iterable[Iterable[Any]]]): Callable
        executing simple prompts on the specified LangChain model.
    """
    return LangChain.query_langchain



================================================
FILE: spacy_llm/models/rest/__init__.py
================================================
from . import anthropic, azure, base, cohere, noop, openai

__all__ = [
    "anthropic",
    "azure",
    "base",
    "cohere",
    "openai",
    "noop",
]



================================================
FILE: spacy_llm/models/rest/base.py
================================================
import abc
import time
from enum import Enum
from typing import Any, Callable, Dict, Iterable, Optional

import requests  # type: ignore
from requests import ConnectTimeout, ReadTimeout


class _HTTPRetryErrorCodes(Enum):
    TOO_MANY_REQUESTS = 429
    SERVICE_UNAVAILABLE = 503

    @classmethod
    def has(cls, item: int):
        return item in set(item.value for item in cls)


class REST(abc.ABC):
    """Queries LLMs via their REST APIs."""

    DEFAULT_STRICT = True
    DEFAULT_MAX_TRIES = 5
    DEFAULT_INTERVAL = 1.0
    DEFAULT_MAX_REQUEST_TIME = 30

    def __init__(
        self,
        name: str,
        endpoint: str,
        config: Dict[Any, Any],
        strict: bool,
        max_tries: int,
        interval: float,
        max_request_time: float,
        context_length: Optional[int],
    ):
        """Initializes new instance of REST-based model.
        name (str): Model name.
        endpoint (str): URL of API endpoint.
        config (Dict[Any, Any]): Config passed on to LLM API.
        strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
            or other response object that does not conform to the expectation of how a well-formed response object from
            this API should look like). If False, the API error responses are returned by __call__(), but no error will
            be raised.
            Note that only response object structure will be checked, not the prompt response text per se.
        max_tries (int): Max. number of tries for API request.
        interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential
            backoff at each retry.
        max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
        context_length (Optional[int]): Context length for this model. Only necessary for sharding and if no context
            length natively provided by spacy-llm.
        """
        self._name = name
        self._endpoint = endpoint
        self._config = config
        self._strict = strict
        self._max_tries = max_tries
        self._interval = interval
        self._max_request_time = max_request_time
        self._credentials = self.credentials
        self._context_length = context_length

        assert self._max_tries >= 1
        assert self._interval > 0
        assert self._max_request_time > 0

        self._verify_auth()

    @abc.abstractmethod
    def __call__(self, prompts: Iterable[Iterable[str]]) -> Iterable[Iterable[str]]:
        """Executes prompts on specified API.
        prompts (Iterable[Iterable[str]]): Prompts to execute.
        RETURNS (Iterable[Iterable[str]]): API responses.
        """

    @staticmethod
    @abc.abstractmethod
    def _get_context_lengths() -> Dict[str, int]:
        """Get context lengths per model name.
        RETURNS (Dict[str, int]): Dict with model name -> context length.
        """

    @property
    def context_length(self) -> Optional[int]:
        """Returns context length in number of tokens for this model.
        RETURNS (Optional[int]): Max. number of tokens in allowed in prompt for the current model. None if unknown.
        """
        return (
            self._context_length
            if self._context_length
            else self._get_context_lengths().get(self._name, None)  # type: ignore[arg-type]
        )

    @property
    @abc.abstractmethod
    def credentials(self) -> Dict[str, str]:
        """Get credentials for the LLM API.
        RETURNS (Dict[str, str]): Credentials.
        """

    @abc.abstractmethod
    def _verify_auth(self) -> None:
        """Verifiy API authentication (and model choice, if possible)."""

    def retry(
        self, call_method: Callable[..., requests.Response], url: str, **kwargs
    ) -> requests.Response:
        """Retry a call to an API if we get a non-ok status code.
        This function automatically retries a request if it catches a response with an error code in `error_codes`.
        The time interval also increases exponentially every time we retry.
        call_method (Callable[[str, ...], requests.Response]): Method to use to fetch request. Must accept URL as first
            parameter.
        url (str): URL to address in request.
        kwargs: Keyword args to be passed on to request.
        RETURNS (requests.Response): Response of last call.
        """

        def _call_api(attempt: int) -> Optional[requests.Response]:
            """Calls API with given timeout.
            attempt (int): Reflects the how many-th try at reaching the API this is. If attempt < self._max_tries and
                the call fails, None is returned. If attempt == self._max_tries and the call fails, a TimeoutError is
                raised.
            RETURNS (Optional[requests.Response]): Response object.
            """
            try:
                return call_method(url, **kwargs)
            except (ConnectTimeout, ReadTimeout, TimeoutError) as err:
                if attempt < self._max_tries:
                    return None
                else:
                    raise TimeoutError(
                        "Request time out. Check your network connection and the API's availability."
                    ) from err

        interval = self._interval
        i = 0
        response = _call_api(i)

        # We don't want to retry on every non-ok status code. Some are about
        # incorrect inputs, etc. and we want to terminate on those.
        start_time = time.time()
        while i < self._max_tries and (
            response is None or _HTTPRetryErrorCodes.has(response.status_code)
        ):
            time.sleep(interval)
            response = _call_api(i + 1)
            i += 1
            # Increase timeout everytime you retry
            interval = interval * 2

        assert isinstance(response, requests.Response)
        if _HTTPRetryErrorCodes.has(response.status_code):
            raise ConnectionError(
                f"API could not be reached after {(time.time() - start_time):.3f} seconds in total and attempting to "
                f"connect {self._max_tries} times. Check your network connection and the API's availability.\n"
                f"{response.status_code}\t{response.reason}"
            )

        return response



================================================
FILE: spacy_llm/models/rest/anthropic/__init__.py
================================================
from .model import Anthropic, Endpoints
from .registry import anthropic_claude_1, anthropic_claude_1_0, anthropic_claude_1_0_v2
from .registry import anthropic_claude_1_2, anthropic_claude_1_2_v2
from .registry import anthropic_claude_1_3, anthropic_claude_1_3_v2
from .registry import anthropic_claude_1_v2, anthropic_claude_2, anthropic_claude_2_v2
from .registry import anthropic_claude_instant_1, anthropic_claude_instant_1_1
from .registry import anthropic_claude_instant_1_1_v2, anthropic_claude_instant_1_v2

__all__ = [
    "Anthropic",
    "Endpoints",
    "anthropic_claude_1",
    "anthropic_claude_1_v2",
    "anthropic_claude_1_0",
    "anthropic_claude_1_0_v2",
    "anthropic_claude_1_2",
    "anthropic_claude_1_2_v2",
    "anthropic_claude_1_3",
    "anthropic_claude_1_3_v2",
    "anthropic_claude_instant_1",
    "anthropic_claude_instant_1_v2",
    "anthropic_claude_instant_1_1",
    "anthropic_claude_instant_1_1_v2",
    "anthropic_claude_2",
    "anthropic_claude_2_v2",
]



================================================
FILE: spacy_llm/models/rest/anthropic/model.py
================================================
import os
import warnings
from enum import Enum
from typing import Any, Dict, Iterable, List, Sized

import requests  # type: ignore[import]
import srsly  # type: ignore[import]
from requests import HTTPError

from ..base import REST


class Endpoints(str, Enum):
    COMPLETIONS = "https://api.anthropic.com/v1/complete"


class SystemPrompt(str, Enum):
    """Specifies the system prompt for Claude
    c.f. https://console.anthropic.com/docs/prompt-design#what-is-a-prompt
    """

    HUMAN = "\n\nHuman:"
    ASST = "\n\nAssistant:"


class Anthropic(REST):
    @property
    def credentials(self) -> Dict[str, str]:
        # Fetch and check the key, set up headers
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if api_key is None:
            warnings.warn(
                "Could not find the API key to access the Anthropic Claude API. Ensure you have an API key "
                "set up via the Anthropic console (https://console.anthropic.com/), then make it available as "
                "an environment variable 'ANTHROPIC_API_KEY'."
            )

        return {"X-API-Key": api_key if api_key else ""}

    def _verify_auth(self) -> None:
        # Execute a dummy prompt. If the API setup is incorrect, we should fail at initialization time.
        try:
            self([["test"]])
        except ValueError as err:
            if "authentication_error" in str(err):
                warnings.warn(
                    "Authentication with provided API key failed. Please double-check you provided the correct "
                    "credentials."
                )
            else:
                raise err

    def __call__(self, prompts: Iterable[Iterable[str]]) -> Iterable[Iterable[str]]:
        headers = {
            **self._credentials,
            "model": self._name,
            "anthropic-version": self._config.get("anthropic-version", "2023-06-01"),
            "Content-Type": "application/json",
        }
        all_api_responses: List[List[str]] = []

        for prompts_for_doc in prompts:
            api_responses: List[str] = []
            prompts_for_doc = list(prompts_for_doc)

            def _request(json_data: Dict[str, Any]) -> Dict[str, Any]:
                r = self.retry(
                    call_method=requests.post,
                    url=self._endpoint,
                    headers=headers,
                    json={**json_data, **self._config, "model": self._name},
                    timeout=self._max_request_time,
                )
                try:
                    r.raise_for_status()
                except HTTPError as ex:
                    res_content = srsly.json_loads(r.content.decode("utf-8"))
                    # Include specific error message in exception.
                    error = res_content.get("error", {})
                    error_msg = f"Request to Anthropic API failed: {error}"
                    if error["type"] == "not_found_error":
                        error_msg += f". Ensure that the selected model ({self._name}) is supported by the API."
                    raise ValueError(error_msg) from ex
                response = r.json()

                # c.f. https://console.anthropic.com/docs/api/errors
                if "error" in response:
                    if self._strict:
                        raise ValueError(f"API call failed: {response}.")
                    else:
                        assert isinstance(prompts_for_doc, Sized)
                        return {
                            "error": [srsly.json_dumps(response)] * len(prompts_for_doc)
                        }
                return response

            # Anthropic API currently doesn't accept batch prompts, so we're making
            # a request for each iteration. This approach can be prone to rate limit
            # errors. In practice, you can adjust _max_request_time so that the
            # timeout is larger.
            responses = [
                _request(
                    {"prompt": f"{SystemPrompt.HUMAN} {prompt}{SystemPrompt.ASST}"}
                )
                for prompt in prompts_for_doc
            ]

            for response in responses:
                if "completion" in response:
                    api_responses.append(response["completion"])
                else:
                    api_responses.append(srsly.json_dumps(response))

            assert len(api_responses) == len(prompts_for_doc)
            all_api_responses.append(api_responses)

        return all_api_responses

    @staticmethod
    def _get_context_lengths() -> Dict[str, int]:
        return {
            # claude-2
            "claude-2": 100000,
            "claude-2-100k": 100000,
            # claude-1
            "claude-1": 100000,
            "claude-1-100k": 100000,
            # claude-instant-1
            "claude-instant-1": 100000,
            "claude-instant-1-100k": 100000,
            # claude-instant-1.1
            "claude-instant-1.1": 100000,
            "claude-instant-1.1-100k": 100000,
            # claude-1.3
            "claude-1.3": 100000,
            "claude-1.3-100k": 100000,
            # others
            "claude-1.0": 100000,
            "claude-1.2": 100000,
        }



================================================
FILE: spacy_llm/models/rest/anthropic/registry.py
================================================
from typing import Any, Callable, Dict, Iterable, Optional

from confection import SimpleFrozenDict

from ....compat import Literal
from ....registry import registry
from .model import Anthropic, Endpoints


@registry.llm_models("spacy.Claude-2.v2")
def anthropic_claude_2_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: str = "claude-2",
    strict: bool = Anthropic.DEFAULT_STRICT,
    max_tries: int = Anthropic.DEFAULT_MAX_TRIES,
    interval: float = Anthropic.DEFAULT_INTERVAL,
    max_request_time: float = Anthropic.DEFAULT_MAX_REQUEST_TIME,
    context_length: Optional[int] = None,
) -> Anthropic:
    """Returns Anthropic instance for 'claude-2' model using REST to prompt API.
    config (Dict[Any, Any]): LLM config arguments passed on to the initialization of the model instance.
    name (str): Name of model to use, e.g. "claude-2" or "claude-2-100k".
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    context_length (Optional[int]): Context length for this model. Only necessary for sharding and if no context length
        natively provided by spacy-llm.
    RETURNS (Anthropic): Anthropic instance for 'claude-2' model.
    """
    return Anthropic(
        name=name,
        endpoint=Endpoints.COMPLETIONS.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=context_length,
    )


@registry.llm_models("spacy.Claude-2.v1")
def anthropic_claude_2(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal["claude-2", "claude-2-100k"] = "claude-2",  # noqa: F722
    strict: bool = Anthropic.DEFAULT_STRICT,
    max_tries: int = Anthropic.DEFAULT_MAX_TRIES,
    interval: float = Anthropic.DEFAULT_INTERVAL,
    max_request_time: float = Anthropic.DEFAULT_MAX_REQUEST_TIME,
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns Anthropic instance for 'claude-2' model using REST to prompt API.
    config (Dict[Any, Any]): LLM config arguments passed on to the initialization of the model instance.
    name (Literal["claude-2", "claude-2-100k"]): Model to use.
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    RETURNS (Anthropic): Anthropic instance for 'claude-1'.
    """
    return Anthropic(
        name=name,
        endpoint=Endpoints.COMPLETIONS.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Claude-1.v2")
def anthropic_claude_1_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: str = "claude-1",
    strict: bool = Anthropic.DEFAULT_STRICT,
    max_tries: int = Anthropic.DEFAULT_MAX_TRIES,
    interval: float = Anthropic.DEFAULT_INTERVAL,
    max_request_time: float = Anthropic.DEFAULT_MAX_REQUEST_TIME,
    context_length: Optional[int] = None,
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns Anthropic instance for 'claude-1' model using REST to prompt API.
    config (Dict[Any, Any]): LLM config arguments passed on to the initialization of the model instance.
    name (str): Name of model to use, e. g. "claude-1" or "claude-1-100k".
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    context_length (Optional[int]): Context length for this model. Only necessary for sharding and if no context length
        natively provided by spacy-llm.
    RETURNS (Anthropic): Anthropic instance for 'claude-1'.
    """
    return Anthropic(
        name=name,
        endpoint=Endpoints.COMPLETIONS.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=context_length,
    )


@registry.llm_models("spacy.Claude-1.v1")
def anthropic_claude_1(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal["claude-1", "claude-1-100k"] = "claude-1",  # noqa: F722
    strict: bool = Anthropic.DEFAULT_STRICT,
    max_tries: int = Anthropic.DEFAULT_MAX_TRIES,
    interval: float = Anthropic.DEFAULT_INTERVAL,
    max_request_time: float = Anthropic.DEFAULT_MAX_REQUEST_TIME,
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns Anthropic instance for 'claude-1' model using REST to prompt API.
    config (Dict[Any, Any]): LLM config arguments passed on to the initialization of the model instance.
    name (Literal["claude-1", "claude-1-100k"]): Model to use.
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    RETURNS (Anthropic): Anthropic instance for 'claude-1'.
    """
    return Anthropic(
        name=name,
        endpoint=Endpoints.COMPLETIONS.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Claude-instant-1.v2")
def anthropic_claude_instant_1_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: str = "claude-instant-1",
    strict: bool = Anthropic.DEFAULT_STRICT,
    max_tries: int = Anthropic.DEFAULT_MAX_TRIES,
    interval: float = Anthropic.DEFAULT_INTERVAL,
    max_request_time: float = Anthropic.DEFAULT_MAX_REQUEST_TIME,
    context_length: Optional[int] = None,
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns Anthropic instance for 'claude-instant-1' model using REST to prompt API.
    config (Dict[Any, Any]): LLM config arguments passed on to the initialization of the model instance.
    name (str): Name of model to use, e. g. "claude-instant-1" or "claude-instant-1-100k".
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    context_length (Optional[int]): Context length for this model. Only necessary for sharding and if no context length
        natively provided by spacy-llm.
    RETURNS (Anthropic): Anthropic instance for 'claude-instant-1'.
    """
    return Anthropic(
        name=name,
        endpoint=Endpoints.COMPLETIONS.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=context_length,
    )


@registry.llm_models("spacy.Claude-instant-1.v1")
def anthropic_claude_instant_1(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal[
        "claude-instant-1", "claude-instant-1-100k"
    ] = "claude-instant-1",  # noqa: F722
    strict: bool = Anthropic.DEFAULT_STRICT,
    max_tries: int = Anthropic.DEFAULT_MAX_TRIES,
    interval: float = Anthropic.DEFAULT_INTERVAL,
    max_request_time: float = Anthropic.DEFAULT_MAX_REQUEST_TIME,
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns Anthropic instance for 'claude-instant-1' model using REST to prompt API.
    config (Dict[Any, Any]): LLM config arguments passed on to the initialization of the model instance.
    name (Literal["claude-instant-1", "claude-instant-1-100k"]): Model to use.
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    RETURNS (Anthropic): Anthropic instance for 'claude-instant-1'.
    """
    return Anthropic(
        name=name,
        endpoint=Endpoints.COMPLETIONS.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Claude-instant-1-1.v2")
def anthropic_claude_instant_1_1_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: str = "claude-instant-1.1",
    strict: bool = Anthropic.DEFAULT_STRICT,
    max_tries: int = Anthropic.DEFAULT_MAX_TRIES,
    interval: float = Anthropic.DEFAULT_INTERVAL,
    max_request_time: float = Anthropic.DEFAULT_MAX_REQUEST_TIME,
    context_length: Optional[int] = None,
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns Anthropic instance for 'claude-instant-1.1' model using REST to prompt API.
    config (Dict[Any, Any]): LLM config arguments passed on to the initialization of the model instance.
    name (str): Name of model to use, e. g. "claude-instant-1.1" or "claude-instant-1.1-100k".
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    context_length (Optional[int]): Context length for this model. Only necessary for sharding and if no context length
        natively provided by spacy-llm.
    RETURNS (Anthropic): Anthropic instance for 'claude-instant-1.1'.
    """
    return Anthropic(
        name=name,
        endpoint=Endpoints.COMPLETIONS.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=context_length,
    )


@registry.llm_models("spacy.Claude-instant-1-1.v1")
def anthropic_claude_instant_1_1(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal[
        "claude-instant-1.1", "claude-instant-1.1-100k"
    ] = "claude-instant-1.1",  # noqa: F722
    strict: bool = Anthropic.DEFAULT_STRICT,
    max_tries: int = Anthropic.DEFAULT_MAX_TRIES,
    interval: float = Anthropic.DEFAULT_INTERVAL,
    max_request_time: float = Anthropic.DEFAULT_MAX_REQUEST_TIME,
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns Anthropic instance for 'claude-instant-1.1' model using REST to prompt API.
    config (Dict[Any, Any]): LLM config arguments passed on to the initialization of the model instance.
    name (Literal["claude-instant-1.1", "claude-instant-1.1-100k"]): Model to use.
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    RETURNS (Anthropic): Anthropic instance for 'claude-instant-1.1' model.
    """
    return Anthropic(
        name=name,
        endpoint=Endpoints.COMPLETIONS.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Claude-1-0.v2")
def anthropic_claude_1_0_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: str = "claude-1.0",
    strict: bool = Anthropic.DEFAULT_STRICT,
    max_tries: int = Anthropic.DEFAULT_MAX_TRIES,
    interval: float = Anthropic.DEFAULT_INTERVAL,
    max_request_time: float = Anthropic.DEFAULT_MAX_REQUEST_TIME,
    context_length: Optional[int] = None,
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns Anthropic instance for 'claude-1.0' model using REST to prompt API.
    config (Dict[Any, Any]): LLM config arguments passed on to the initialization of the model instance.
    name (str): Name of model to use, e. g. "claude-1.0".
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    context_length (Optional[int]): Context length for this model. Only necessary for sharding and if no context length
        natively provided by spacy-llm.
    RETURNS (Anthropic): Anthropic instance for 'claude-1.0'.
    """
    return Anthropic(
        name=name,
        endpoint=Endpoints.COMPLETIONS.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=context_length,
    )


@registry.llm_models("spacy.Claude-1-0.v1")
def anthropic_claude_1_0(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal["claude-1.0"] = "claude-1.0",  # noqa: F722
    strict: bool = Anthropic.DEFAULT_STRICT,
    max_tries: int = Anthropic.DEFAULT_MAX_TRIES,
    interval: float = Anthropic.DEFAULT_INTERVAL,
    max_request_time: float = Anthropic.DEFAULT_MAX_REQUEST_TIME,
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns Anthropic instance for 'claude-1.0' model using REST to prompt API.
    config (Dict[Any, Any]): LLM config arguments passed on to the initialization of the model instance.
    name (Literal["claude-1.0"]): Model to use.
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    RETURNS (Anthropic): Anthropic instance for 'claude-1.0' model.
    """
    return Anthropic(
        name=name,
        endpoint=Endpoints.COMPLETIONS.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Claude-1-2.v2")
def anthropic_claude_1_2_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: str = "claude-1.2",
    strict: bool = Anthropic.DEFAULT_STRICT,
    max_tries: int = Anthropic.DEFAULT_MAX_TRIES,
    interval: float = Anthropic.DEFAULT_INTERVAL,
    max_request_time: float = Anthropic.DEFAULT_MAX_REQUEST_TIME,
    context_length: Optional[int] = None,
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns Anthropic instance for 'claude-1.2' model using REST to prompt API.
    config (Dict[Any, Any]): LLM config arguments passed on to the initialization of the model instance.
    name (str): Name of model to use, e. g. "claude-1.2".
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    context_length (Optional[int]): Context length for this model. Only necessary for sharding and if no context length
        natively provided by spacy-llm.
    RETURNS (Anthropic): Anthropic instance for 'claude-1.2'.
    """
    return Anthropic(
        name=name,
        endpoint=Endpoints.COMPLETIONS.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=context_length,
    )


@registry.llm_models("spacy.Claude-1-2.v1")
def anthropic_claude_1_2(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal["claude-1.2"] = "claude-1.2",  # noqa: F722
    strict: bool = Anthropic.DEFAULT_STRICT,
    max_tries: int = Anthropic.DEFAULT_MAX_TRIES,
    interval: float = Anthropic.DEFAULT_INTERVAL,
    max_request_time: float = Anthropic.DEFAULT_MAX_REQUEST_TIME,
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns Anthropic instance for 'claude-1.2' model using REST to prompt API.
    config (Dict[Any, Any]): LLM config arguments passed on to the initialization of the model instance.
    name (Literal["claude-1.2"]): Model to use.
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    RETURNS (Anthropic): Anthropic instance for 'claude-1.2' model.
    """
    return Anthropic(
        name=name,
        endpoint=Endpoints.COMPLETIONS.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Claude-1-3.v2")
def anthropic_claude_1_3_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: str = "claude-1.3",
    strict: bool = Anthropic.DEFAULT_STRICT,
    max_tries: int = Anthropic.DEFAULT_MAX_TRIES,
    interval: float = Anthropic.DEFAULT_INTERVAL,
    max_request_time: float = Anthropic.DEFAULT_MAX_REQUEST_TIME,
    context_length: Optional[int] = None,
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns Anthropic instance for 'claude-1.3' model using REST to prompt API.
    config (Dict[Any, Any]): LLM config arguments passed on to the initialization of the model instance.
    name (str): Name of model variant to use, e. g. "claude-1.3" or "claude-1.3-100k".
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    context_length (Optional[int]): Context length for this model. Only necessary for sharding and if no context length
        natively provided by spacy-llm.
    RETURNS (Anthropic): Anthropic instance for 'claude-1.3' model.
    """
    return Anthropic(
        name=name,
        endpoint=Endpoints.COMPLETIONS.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=context_length,
    )


@registry.llm_models("spacy.Claude-1-3.v1")
def anthropic_claude_1_3(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal["claude-1.3", "claude-1.3-100k"] = "claude-1.3",  # noqa: F722
    strict: bool = Anthropic.DEFAULT_STRICT,
    max_tries: int = Anthropic.DEFAULT_MAX_TRIES,
    interval: float = Anthropic.DEFAULT_INTERVAL,
    max_request_time: float = Anthropic.DEFAULT_MAX_REQUEST_TIME,
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns Anthropic instance for 'claude-1.3' model using REST to prompt API.
    config (Dict[Any, Any]): LLM config arguments passed on to the initialization of the model instance.
    name (Literal["claude-1.3", "claude-1.3-100k"]): Model variant to use.
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    RETURNS (Anthropic): Anthropic instance for 'claude-1.3' model.
    """
    return Anthropic(
        name=name,
        endpoint=Endpoints.COMPLETIONS.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )



================================================
FILE: spacy_llm/models/rest/azure/__init__.py
================================================
from .model import AzureOpenAI
from .registry import azure_openai, azure_openai_v2

__all__ = ["AzureOpenAI", "azure_openai", "azure_openai_v2"]



================================================
FILE: spacy_llm/models/rest/azure/model.py
================================================
import os
import warnings
from enum import Enum
from typing import Any, Dict, Iterable, List, Optional, Sized

import requests  # type: ignore[import]
import srsly  # type: ignore[import]
from requests import HTTPError

from ..base import REST


class ModelType(str, Enum):
    COMPLETION = "completions"
    CHAT = "chat"


class AzureOpenAI(REST):
    def __init__(
        self,
        deployment_name: str,
        name: str,
        endpoint: str,
        config: Dict[Any, Any],
        strict: bool,
        max_tries: int,
        interval: float,
        max_request_time: float,
        model_type: ModelType,
        context_length: Optional[int],
        api_version: str = "2023-05-15",
    ):
        self._model_type = model_type
        self._api_version = api_version
        self._deployment_name = deployment_name
        super().__init__(
            name=name,
            endpoint=endpoint,
            config=config,
            strict=strict,
            max_tries=max_tries,
            interval=interval,
            max_request_time=max_request_time,
            context_length=context_length,
        )

    @property
    def endpoint(self) -> str:
        """Returns fully formed endpoint URL.
        RETURNS (str): Fully formed endpoint URL.
        """
        return (
            self._endpoint
            + ("" if self._endpoint.endswith("/") else "/")
            + f"openai/deployments/{self._deployment_name}/{'' if self._model_type == ModelType.COMPLETION else 'chat/'}"
            f"completions"
        )

    @property
    def credentials(self) -> Dict[str, str]:
        # Fetch and check the key
        api_key = os.getenv("AZURE_OPENAI_KEY")
        if api_key is None:
            warnings.warn(
                "Could not find the API key to access the Azure OpenAI API. Ensure you have an API key "
                "set up (see "
                "https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?pivots=rest-api&tabs=bash#set-up"
                ", then make it available as an environment variable 'AZURE_OPENAI_KEY'."
            )

        # Check the access and get a list of available models to verify the model argument (if not None)
        # Even if the model is None, this call is used as a healthcheck to verify access.
        assert api_key is not None
        return {"api-key": api_key}

    def _verify_auth(self) -> None:
        try:
            self([["test"]])
        except ValueError as err:
            raise err

    def __call__(self, prompts: Iterable[Iterable[str]]) -> Iterable[Iterable[str]]:
        headers = {
            **self._credentials,
            "Content-Type": "application/json",
        }
        all_api_responses: List[List[str]] = []

        for prompts_for_doc in prompts:
            api_responses: List[str] = []
            prompts_for_doc = list(prompts_for_doc)

            def _request(json_data: Dict[str, Any]) -> Dict[str, Any]:
                r = self.retry(
                    call_method=requests.post,
                    url=self.endpoint,
                    headers=headers,
                    json={**json_data, **self._config},
                    timeout=self._max_request_time,
                    params={"api-version": self._api_version},
                )
                try:
                    r.raise_for_status()
                except HTTPError as ex:
                    res_content = srsly.json_loads(r.content.decode("utf-8"))
                    # Include specific error message in exception.
                    raise ValueError(
                        f"Request to Azure OpenAI API failed: "
                        f"{res_content.get('error', {}).get('message', str(res_content))}"
                    ) from ex
                responses = r.json()

                if "error" in responses:
                    if self._strict:
                        raise ValueError(f"API call failed: {responses}.")
                    else:
                        assert isinstance(prompts_for_doc, Sized)
                        return {
                            "error": [srsly.json_dumps(responses)]
                            * len(prompts_for_doc)
                        }

                return responses

            # The (Azure) OpenAI API doesn't support batching yet, so we have to send individual requests.
            # https://learn.microsoft.com/en-us/answers/questions/1334800/batching-requests-in-azure-openai

            if self._model_type == ModelType.CHAT:
                # Note: this is yet (2023-10-05) untested, as Azure doesn't seem to allow the deployment of a chat model
                # yet.
                for prompt in prompts_for_doc:
                    responses = _request(
                        {"messages": [{"role": "user", "content": prompt}]}
                    )
                    if "error" in responses:
                        return responses["error"]

                    # Process responses.
                    assert len(responses["choices"]) == 1
                    response = responses["choices"][0]
                    api_responses.append(
                        response.get("message", {}).get(
                            "content", srsly.json_dumps(response)
                        )
                    )

            elif self._model_type == ModelType.COMPLETION:
                for prompt in prompts_for_doc:
                    responses = _request({"prompt": prompt})
                    if "error" in responses:
                        return responses["error"]

                    # Process responses.
                    assert len(responses["choices"]) == 1
                    response = responses["choices"][0]
                    api_responses.append(
                        response.get("text", srsly.json_dumps(response))
                    )

            all_api_responses.append(api_responses)

        return all_api_responses

    @staticmethod
    def _get_context_lengths() -> Dict[str, int]:
        return {
            # gpt-4
            "gpt-4": 8192,
            "gpt-4-32k": 32768,
            # gpt-3.5
            "gpt-3.5-turbo": 4097,
            "gpt-3.5-turbo-16k": 16385,
            "gpt-3.5-turbo-instruct": 4097,
            # text-davinci
            "text-davinci-002": 4097,
            "text-davinci-003": 4097,
            # others
            "code-davinci-002": 8001,
            "text-curie-001": 2049,
            "text-babbage-001": 2049,
            "text-ada-001": 2049,
        }



================================================
FILE: spacy_llm/models/rest/azure/registry.py
================================================
from typing import Any, Callable, Dict, Iterable, Optional

from confection import SimpleFrozenDict

from ....registry import registry
from .model import AzureOpenAI, ModelType

_DEFAULT_TEMPERATURE = 0.0


@registry.llm_models("spacy.Azure.v2")
def azure_openai_v2(
    deployment_name: str,
    name: str,
    base_url: str,
    model_type: ModelType,
    config: Dict[Any, Any] = SimpleFrozenDict(temperature=_DEFAULT_TEMPERATURE),
    strict: bool = AzureOpenAI.DEFAULT_STRICT,
    max_tries: int = AzureOpenAI.DEFAULT_MAX_TRIES,
    interval: float = AzureOpenAI.DEFAULT_INTERVAL,
    max_request_time: float = AzureOpenAI.DEFAULT_MAX_REQUEST_TIME,
    api_version: str = "2023-05-15",
    context_length: Optional[int] = None,
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns Azure OpenAI instance for models deployed on Azure's OpenAI service using REST to prompt API.

    Docs on OpenAI models supported by Azure:
    https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#model-summary-table-and-region-availability.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    deployment_name (str): Name of the deployment to use. Note that this does not necessarily equal the name of the
        model used by that deployment, as deployment names in Azure OpenAI can be arbitrary.
    name (str): Name of the model used by this deployment. This is required to infer the context length that can be
        assumed for prompting.
    endpoint (str): The URL for your Azure OpenAI endpoint. This is usually something like
        "https://{prefix}.openai.azure.com/".
    model_type (ModelType): Whether the deployed model is a text completetion model (e. g.
        text-davinci-003) or a chat model (e. g. gpt-4).
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    api_version (str): API version to use.
    context_length (Optional[int]): Context length for this model. Only necessary for sharding and if no context length
        natively provided by spacy-llm.
    RETURNS (AzureOpenAI): AzureOpenAI instance for deployed model.

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return AzureOpenAI(
        deployment_name=deployment_name,
        name=name,
        endpoint=base_url,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        api_version=api_version,
        model_type=model_type,
        context_length=context_length,
    )


@registry.llm_models("spacy.Azure.v1")
def azure_openai(
    deployment_name: str,
    name: str,
    base_url: str,
    model_type: ModelType,
    config: Dict[Any, Any] = SimpleFrozenDict(temperature=_DEFAULT_TEMPERATURE),
    strict: bool = AzureOpenAI.DEFAULT_STRICT,
    max_tries: int = AzureOpenAI.DEFAULT_MAX_TRIES,
    interval: float = AzureOpenAI.DEFAULT_INTERVAL,
    max_request_time: float = AzureOpenAI.DEFAULT_MAX_REQUEST_TIME,
    api_version: str = "2023-05-15",
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns Azure OpenAI instance for models deployed on Azure's OpenAI service using REST to prompt API.

    Docs on OpenAI models supported by Azure:
    https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#model-summary-table-and-region-availability.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    deployment_name (str): Name of the deployment to use. Note that this does not necessarily equal the name of the
        model used by that deployment, as deployment names in Azure OpenAI can be arbitrary.
    name (str): Name of the model used by this deployment. This is required to infer the context length that can be
        assumed for prompting.
    endpoint (str): The URL for your Azure OpenAI endpoint. This is usually something like
        "https://{prefix}.openai.azure.com/".
    model_type (ModelType): Whether the deployed model is a text completetion model (e. g.
        text-davinci-003) or a chat model (e. g. gpt-4).
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    api_version (str): API version to use.
    RETURNS (AzureOpenAI): AzureOpenAI instance for deployed model.

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return AzureOpenAI(
        deployment_name=deployment_name,
        name=name,
        endpoint=base_url,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        api_version=api_version,
        model_type=model_type,
        context_length=None,
    )



================================================
FILE: spacy_llm/models/rest/cohere/__init__.py
================================================
from .model import Cohere, Endpoints
from .registry import cohere_command, cohere_command_v2

__all__ = ["Cohere", "Endpoints", "cohere_command", "cohere_command_v2"]



================================================
FILE: spacy_llm/models/rest/cohere/model.py
================================================
import os
import warnings
from enum import Enum
from typing import Any, Dict, Iterable, List, Sized

import requests  # type: ignore[import]
import srsly  # type: ignore[import]
from requests import HTTPError

from ..base import REST


class Endpoints(str, Enum):
    COMPLETION = "https://api.cohere.ai/v1/generate"


class Cohere(REST):
    @property
    def credentials(self) -> Dict[str, str]:
        api_key = os.getenv("CO_API_KEY")
        if api_key is None:
            warnings.warn(
                "Could not find the API key to access the Cohere API. Ensure you have an API key "
                "set up via https://dashboard.cohere.ai/api-keys, then make it available as "
                "an environment variable 'CO_API_KEY'."
            )

        return {"Authorization": f"Bearer {api_key}"}

    def _verify_auth(self) -> None:
        try:
            self([["test"]])
        except ValueError as err:
            if "invalid api token" in str(err):
                warnings.warn(
                    "Authentication with provided API key failed. Please double-check you provided the correct "
                    "credentials."
                )
            else:
                raise err

    def __call__(self, prompts: Iterable[Iterable[str]]) -> Iterable[Iterable[str]]:
        headers = {
            **self._credentials,
            "Content-Type": "application/json",
            "Accept": "application/json",
        }
        all_api_responses: List[List[str]] = []

        for prompts_for_doc in prompts:
            api_responses: List[str] = []
            prompts_for_doc = list(prompts_for_doc)

        def _request(json_data: Dict[str, Any]) -> Dict[str, Any]:
            r = self.retry(
                call_method=requests.post,
                url=self._endpoint,
                headers=headers,
                json={**json_data, **self._config, "model": self._name},
                timeout=self._max_request_time,
            )
            try:
                r.raise_for_status()
            except HTTPError as ex:
                res_content = srsly.json_loads(r.content.decode("utf-8"))
                # Include specific error message in exception.
                error_message = res_content.get("message", {})
                # Catch 'blocked output' and 'blocked input' errors from Cohere
                # This usually happens when it detects violations in their Usage guidelines.
                # Unfortunately Cohere returns this as an HTTPError, so it cannot be caught in the response.
                if "blocked" in error_message:
                    # Only raise an error when strict. If strict is False, do
                    # nothing and parse the response as usual.
                    if self._strict:
                        raise ValueError(
                            f"Cohere API returned a blocking error. {error_message}. "
                            "If you wish to ignore and continue, you can pass 'False' to the 'strict' argument of this model. "
                            "However, note that this will affect how spacy-llm parses the response."
                        ) from ex
                else:
                    # Catching other types of HTTPErrors (e.g., "429: too many requests")
                    raise ValueError(
                        f"Request to Cohere API failed: {error_message}"
                    ) from ex
            response = r.json()

            # Cohere returns a 'message' key when there is an error
            # in the response.
            if "message" in response:
                if self._strict:
                    raise ValueError(f"API call failed: {response}.")
                else:
                    assert isinstance(prompts_for_doc, Sized)
                    return {
                        "error": [srsly.json_dumps(response)] * len(prompts_for_doc)
                    }

            return response

        # Cohere API currently doesn't accept batch prompts, so we're making
        # a request for each iteration. This approach can be prone to rate limit
        # errors. In practice, you can adjust _max_request_time so that the
        # timeout is larger.
        responses = [_request({"prompt": prompt}) for prompt in prompts_for_doc]
        for response in responses:
            if "generations" in response:
                for result in response["generations"]:
                    if "text" in result:
                        # Although you can set the number of completions in Cohere
                        # to be greater than 1, we only need to return a single value.
                        # In this case, we will just return the very first output.
                        api_responses.append(result["text"])
                        break
                    else:
                        api_responses.append(srsly.json_dumps(response))
            else:
                api_responses.append(srsly.json_dumps(response))

        all_api_responses.append(api_responses)

        return all_api_responses

    @staticmethod
    def _get_context_lengths() -> Dict[str, int]:
        return {
            "command": 4096,
            "command-light": 4096,
            "command-light-nightly": 4096,
            "command-nightly": 4096,
        }



================================================
FILE: spacy_llm/models/rest/cohere/registry.py
================================================
from typing import Any, Callable, Dict, Iterable, Optional

from confection import SimpleFrozenDict

from ....compat import Literal
from ....registry import registry
from .model import Cohere, Endpoints


@registry.llm_models("spacy.Command.v2")
def cohere_command_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: str = "command",
    strict: bool = Cohere.DEFAULT_STRICT,
    max_tries: int = Cohere.DEFAULT_MAX_TRIES,
    interval: float = Cohere.DEFAULT_INTERVAL,
    max_request_time: float = Cohere.DEFAULT_MAX_REQUEST_TIME,
    context_length: Optional[int] = None,
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns Cohere instance for 'command' model using REST to prompt API.
    name (str): Name of model to use, e. g. "command" or "command-light".
    config (Dict[Any, Any]): LLM config arguments passed on to the initialization of the model instance.
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    context_length (Optional[int]): Context length for this model. Only necessary for sharding and if no context length
        natively provided by spacy-llm.
    RETURNS (Cohere): Cohere instance for 'command' model.
    """
    return Cohere(
        name=name,
        endpoint=Endpoints.COMPLETION.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=context_length,
    )


@registry.llm_models("spacy.Command.v1")
def cohere_command(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal[
        "command", "command-light", "command-light-nightly", "command-nightly"
    ] = "command",  # noqa: F821
    strict: bool = Cohere.DEFAULT_STRICT,
    max_tries: int = Cohere.DEFAULT_MAX_TRIES,
    interval: float = Cohere.DEFAULT_INTERVAL,
    max_request_time: float = Cohere.DEFAULT_MAX_REQUEST_TIME,
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns Cohere instance for 'command' model using REST to prompt API.
    name (Literal["command", "command-light", "command-light-nightly", "command-nightly"]): Model  to use.
    config (Dict[Any, Any]): LLM config arguments passed on to the initialization of the model instance.
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    RETURNS (Cohere): Cohere instance for 'command' model.
    """
    return Cohere(
        name=name,
        endpoint=Endpoints.COMPLETION.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )



================================================
FILE: spacy_llm/models/rest/noop/__init__.py
================================================
from .model import _NOOP_RESPONSE, NoOpModel
from .registry import noop

__all__ = ["noop", "NoOpModel", "_NOOP_RESPONSE"]



================================================
FILE: spacy_llm/models/rest/noop/model.py
================================================
import sys
import time
from typing import Dict, Iterable

from ..base import REST

_NOOP_RESPONSE = ""


class NoOpModel(REST):
    """NoOp model. Used for tests only."""

    _CALL_TIMEOUT = 0.01

    def __init__(self):
        super().__init__(
            name="NoOp",
            endpoint="NoOp",
            config={},
            strict=True,
            max_tries=1,
            interval=1,
            max_request_time=1,
            context_length=None,
        )

    @property
    def credentials(self) -> Dict[str, str]:
        return {}

    def _verify_auth(self) -> None:
        pass

    def __call__(self, prompts: Iterable[Iterable[str]]) -> Iterable[Iterable[str]]:
        # Assume time penalty for API calls.
        time.sleep(NoOpModel._CALL_TIMEOUT)
        return [[_NOOP_RESPONSE]] * len(list(prompts))

    @staticmethod
    def _get_context_lengths() -> Dict[str, int]:
        return {"NoOp": sys.maxsize}



================================================
FILE: spacy_llm/models/rest/noop/registry.py
================================================
from typing import Callable, Iterable

from ....registry import registry
from .model import NoOpModel


@registry.llm_models("spacy.NoOp.v1")
def noop() -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns NoOpModel.
    RETURNS (Callable[[Iterable[str]], Iterable[str]]]): NoOp model instance for test purposes.
    """
    return NoOpModel()



================================================
FILE: spacy_llm/models/rest/openai/__init__.py
================================================
from .model import Endpoints, OpenAI
from .registry import openai_ada, openai_ada_v2, openai_babbage, openai_babbage_v2
from .registry import openai_code_davinci, openai_code_davinci_v2, openai_curie
from .registry import openai_curie_v2, openai_davinci, openai_davinci_v2
from .registry import openai_gpt_3_5, openai_gpt_3_5_v2, openai_gpt_3_5_v3
from .registry import openai_gpt_4, openai_gpt_4_v2, openai_gpt_4_v3, openai_text_ada
from .registry import openai_text_ada_v2, openai_text_babbage, openai_text_babbage_v2
from .registry import openai_text_curie, openai_text_curie_v2, openai_text_davinci
from .registry import openai_text_davinci_v2, openai_text_davinci_v3

__all__ = [
    "OpenAI",
    "Endpoints",
    "openai_ada",
    "openai_ada_v2",
    "openai_babbage",
    "openai_babbage_v2",
    "openai_code_davinci",
    "openai_code_davinci_v2",
    "openai_curie",
    "openai_curie_v2",
    "openai_davinci",
    "openai_davinci_v2",
    "openai_gpt_3_5",
    "openai_gpt_3_5_v2",
    "openai_gpt_3_5_v3",
    "openai_gpt_4",
    "openai_gpt_4_v2",
    "openai_gpt_4_v3",
    "openai_text_ada",
    "openai_text_ada_v2",
    "openai_text_babbage",
    "openai_text_babbage_v2",
    "openai_text_curie",
    "openai_text_curie_v2",
    "openai_text_davinci",
    "openai_text_davinci_v2",
    "openai_text_davinci_v3",
]



================================================
FILE: spacy_llm/models/rest/openai/model.py
================================================
import os
import warnings
from enum import Enum
from typing import Any, Dict, Iterable, List, Sized

import requests  # type: ignore[import]
import srsly  # type: ignore[import]
from requests import HTTPError

from ..base import REST


class Endpoints(str, Enum):
    CHAT = "https://api.openai.com/v1/chat/completions"
    NON_CHAT = "https://api.openai.com/v1/completions"


class OpenAI(REST):
    @property
    def credentials(self) -> Dict[str, str]:
        # Fetch and check the key
        api_key = os.getenv("OPENAI_API_KEY")
        api_org = os.getenv("OPENAI_API_ORG")
        if api_key is None:
            warnings.warn(
                "Could not find the API key to access the OpenAI API. Ensure you have an API key "
                "set up via https://platform.openai.com/account/api-keys, then make it available as "
                "an environment variable 'OPENAI_API_KEY'."
            )

        # Check the access and get a list of available models to verify the model argument (if not None)
        # Even if the model is None, this call is used as a healthcheck to verify access.
        headers = {
            "Authorization": f"Bearer {api_key}",
        }
        if api_org:
            headers["OpenAI-Organization"] = api_org

        return headers

    def _verify_auth(self) -> None:
        r = self.retry(
            call_method=requests.get,
            url="https://api.openai.com/v1/models",
            headers=self._credentials,
            timeout=self._max_request_time,
        )
        if r.status_code == 422:
            warnings.warn(
                "Could not access api.openai.com -- 422 permission denied."
                "Visit https://platform.openai.com/account/api-keys to check your API keys."
            )
        elif r.status_code != 200:
            if "Incorrect API key" in r.text:
                warnings.warn(
                    "Authentication with provided API key failed. Please double-check you provided the correct "
                    "credentials."
                )
            else:
                warnings.warn(
                    f"Error accessing api.openai.com ({r.status_code}): {r.text}"
                )

        response = r.json()["data"]
        models = [response[i]["id"] for i in range(len(response))]
        if self._name not in models:
            raise ValueError(
                f"The specified model '{self._name}' is not available. Choices are: {sorted(set(models))}"
            )

    def __call__(self, prompts: Iterable[Iterable[str]]) -> Iterable[Iterable[str]]:
        headers = {
            **self._credentials,
            "Content-Type": "application/json",
        }
        all_api_responses: List[List[str]] = []

        for prompts_for_doc in prompts:
            api_responses: List[str] = []
            prompts_for_doc = list(prompts_for_doc)

            def _request(json_data: Dict[str, Any]) -> Dict[str, Any]:
                r = self.retry(
                    call_method=requests.post,
                    url=self._endpoint,
                    headers=headers,
                    json={**json_data, **self._config, "model": self._name},
                    timeout=self._max_request_time,
                )
                try:
                    r.raise_for_status()
                except HTTPError as ex:
                    res_content = srsly.json_loads(r.content.decode("utf-8"))
                    # Include specific error message in exception.
                    raise ValueError(
                        f"Request to OpenAI API failed: {res_content.get('error', {}).get('message', str(res_content))}"
                    ) from ex
                responses = r.json()

                if "error" in responses:
                    if self._strict:
                        raise ValueError(f"API call failed: {responses}.")
                    else:
                        assert isinstance(prompts_for_doc, Sized)
                        return {
                            "error": [srsly.json_dumps(responses)]
                            * len(prompts_for_doc)
                        }

                return responses

            # The OpenAI API doesn't support batching for /chat/completions yet, so we have to send individual requests.

            if self._endpoint == Endpoints.NON_CHAT:
                responses = _request({"prompt": prompts_for_doc})
                if "error" in responses:
                    return responses["error"]
                assert len(responses["choices"]) == len(prompts_for_doc)

                for response in responses["choices"]:
                    if "text" in response:
                        api_responses.append(response["text"])
                    else:
                        api_responses.append(srsly.json_dumps(response))

            else:
                for prompt in prompts_for_doc:
                    responses = _request(
                        {"messages": [{"role": "user", "content": prompt}]}
                    )
                    if "error" in responses:
                        return responses["error"]

                    # Process responses.
                    assert len(responses["choices"]) == 1
                    response = responses["choices"][0]
                    api_responses.append(
                        response.get("message", {}).get(
                            "content", srsly.json_dumps(response)
                        )
                    )

            all_api_responses.append(api_responses)

        return all_api_responses

    @staticmethod
    def _get_context_lengths() -> Dict[str, int]:
        return {
            # gpt-4
            "gpt-4": 8192,
            "gpt-4-0314": 8192,
            "gpt-4-32k": 32768,
            "gpt-4-32k-0314": 32768,
            # gpt-3.5
            "gpt-3.5-turbo": 4097,
            "gpt-3.5-turbo-16k": 16385,
            "gpt-3.5-turbo-0613": 4097,
            "gpt-3.5-turbo-0613-16k": 16385,
            "gpt-3.5-turbo-instruct": 4097,
            # text-davinci
            "text-davinci-002": 4097,
            "text-davinci-003": 4097,
            # others
            "code-davinci-002": 8001,
            "text-curie-001": 2049,
            "text-babbage-001": 2049,
            "text-ada-001": 2049,
            "davinci": 2049,
            "curie": 2049,
            "babbage": 2049,
            "ada": 2049,
        }



================================================
FILE: spacy_llm/models/rest/openai/registry.py
================================================
from typing import Any, Dict, Optional

from confection import SimpleFrozenDict

from ....compat import Literal
from ....registry import registry
from .model import Endpoints, OpenAI

_DEFAULT_TEMPERATURE = 0.0

"""
Parameter explanations:
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    endpoint (Optional[str]): Endpoint to set. Defaults to standard endpoint.
"""


@registry.llm_models("spacy.GPT-4.v3")
def openai_gpt_4_v3(
    config: Dict[Any, Any] = SimpleFrozenDict(temperature=_DEFAULT_TEMPERATURE),
    name: str = "gpt-4",
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
    context_length: Optional[int] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'gpt-4' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (str): Model name to use. Can be any model name supported by the OpenAI API - e. g. 'gpt-4',
        "gpt-4-1106-preview", ....
    context_length (Optional[int]): Context length for this model. Only necessary for sharding and if no context length
        natively provided by spacy-llm.
    RETURNS (OpenAI): OpenAI instance for 'gpt-4' model.

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=context_length,
    )


@registry.llm_models("spacy.GPT-4.v2")
def openai_gpt_4_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(temperature=_DEFAULT_TEMPERATURE),
    name: Literal[
        "gpt-4", "gpt-4-0314", "gpt-4-32k", "gpt-4-32k-0314"
    ] = "gpt-4",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'gpt-4' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Literal["gpt-4", "gpt-4-0314", "gpt-4-32k", "gpt-4-32k-0314"]): Model to use. Base 'gpt-4' model by default.
    RETURNS (OpenAI): OpenAI instance for 'gpt-4' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.GPT-4.v1")
def openai_gpt_4(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal[
        "gpt-4", "gpt-4-0314", "gpt-4-32k", "gpt-4-32k-0314"
    ] = "gpt-4",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'gpt-4' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Literal["gpt-4", "gpt-4-0314", "gpt-4-32k", "gpt-4-32k-0314"]): Model to use. Base 'gpt-4' model by
        default.
    RETURNS (OpenAI): OpenAI instance for 'gpt-4' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.GPT-3-5.v3")
def openai_gpt_3_5_v3(
    config: Dict[Any, Any] = SimpleFrozenDict(temperature=_DEFAULT_TEMPERATURE),
    name: str = "gpt-3.5-turbo",
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
    context_length: Optional[int] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'gpt-3.5' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (str): Name of model to use. Can be any model name supported by the OpenAI API - e. g. 'gpt-3.5',
        "gpt-3.5-turbo", ....
    context_length (Optional[int]): Context length for this model. Only necessary for sharding and if no context length
        natively provided by spacy-llm.
    RETURNS (OpenAI): OpenAI instance for 'gpt-3.5' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.CHAT.value
        # gpt-3.5-turbo-instruct runs on the non-chat endpoint, so we use that one by default to allow batching.
        if name != "gpt-3.5-turbo-instruct" else Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=context_length,
    )


@registry.llm_models("spacy.GPT-3-5.v2")
def openai_gpt_3_5_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(temperature=_DEFAULT_TEMPERATURE),
    name: Literal[
        "gpt-3.5-turbo",
        "gpt-3.5-turbo-16k",
        "gpt-3.5-turbo-0613",
        "gpt-3.5-turbo-0613-16k",
        "gpt-3.5-turbo-instruct",
    ] = "gpt-3.5-turbo",  # noqa: F722,F821
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'gpt-3.5' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Literal[
        "gpt-3.5-turbo", "gpt-3.5-turbo-16k", "gpt-3.5-turbo-0613", "gpt-3.5-turbo-0613-16k", "gpt-3.5-turbo-instruct"
    ]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'gpt-3.5' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.CHAT.value
        # gpt-3.5-turbo-instruct runs on the non-chat endpoint, so we use that one by default to allow batching.
        if name != "gpt-3.5-turbo-instruct" else Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.GPT-3-5.v1")
def openai_gpt_3_5(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal[
        "gpt-3.5-turbo",
        "gpt-3.5-turbo-16k",
        "gpt-3.5-turbo-0613",
        "gpt-3.5-turbo-0613-16k",
        "gpt-3.5-turbo-instruct",
    ] = "gpt-3.5-turbo",  # noqa: F722,F821
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'gpt-3.5' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Literal[
        "gpt-3.5-turbo", "gpt-3.5-turbo-16k", "gpt-3.5-turbo-0613", "gpt-3.5-turbo-0613-16k", "gpt-3.5-turbo-instruct"
    ]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'gpt-3.5' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.CHAT.value
        # gpt-3.5-turbo-instruct runs on the non-chat endpoint, so we use that one by default to allow batching.
        if name != "gpt-3.5-turbo-instruct" else Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Text-Davinci.v3")
def openai_text_davinci_v3(
    config: Dict[Any, Any] = SimpleFrozenDict(
        max_tokens=1000, temperature=_DEFAULT_TEMPERATURE
    ),
    name: str = "text-davinci-003",
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    context_length: Optional[int] = None,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'text-davinci' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (str): Name of model to use, e. g. "text-davinci-002" or "text-davinci-003".
    context_length (Optional[int]): Context length for this model. Only necessary for sharding and if no context length
        natively provided by spacy-llm.
    RETURNS (OpenAI): OpenAI instance for 'text-davinci' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=context_length,
    )


@registry.llm_models("spacy.Text-Davinci.v2")
def openai_text_davinci_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(
        max_tokens=1000, temperature=_DEFAULT_TEMPERATURE
    ),
    name: Literal[
        "text-davinci-002", "text-davinci-003"
    ] = "text-davinci-003",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'text-davinci' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Optional[Literal["text-davinci-002", "text-davinci-003"]]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'text-davinci' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Text-Davinci.v1")
def openai_text_davinci(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal[
        "text-davinci-002", "text-davinci-003"
    ] = "text-davinci-003",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'text-davinci' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Optional[Literal["text-davinci-002", "text-davinci-003"]]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'text-davinci' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Code-Davinci.v2")
def openai_code_davinci_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(
        max_tokens=500, temperature=_DEFAULT_TEMPERATURE
    ),
    name: Literal["code-davinci-002"] = "code-davinci-002",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'code-davinci' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Optional[Literal["code-davinci-002"]]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'code-davinci' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Code-Davinci.v1")
def openai_code_davinci(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal["code-davinci-002"] = "code-davinci-002",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'code-davinci' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Optional[Literal["code-davinci-002"]]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'code-davinci' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Text-Curie.v2")
def openai_text_curie_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(
        max_tokens=500, temperature=_DEFAULT_TEMPERATURE
    ),
    name: Literal["text-curie-001"] = "text-curie-001",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'text-curie' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Optional[Literal["text-curie-001"]]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'text-curie' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Text-Curie.v1")
def openai_text_curie(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal["text-curie-001"] = "text-curie-001",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'text-curie' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Optional[Literal["text-curie-001"]]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'text-curie' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Text-Babbage.v2")
def openai_text_babbage_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(
        max_tokens=500, temperature=_DEFAULT_TEMPERATURE
    ),
    name: Literal["text-babbage-001"] = "text-babbage-001",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'text-babbage' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Optional[Literal["text-babbage-001"]]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'text-babbage' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Text-Babbage.v1")
def openai_text_babbage(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal["text-babbage-001"] = "text-babbage-001",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'text-babbage' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Optional[Literal["text-babbage-001"]]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'text-babbage' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Text-Ada.v2")
def openai_text_ada_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(
        max_tokens=500, temperature=_DEFAULT_TEMPERATURE
    ),
    name: Literal["text-ada-001"] = "text-ada-001",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'text-ada' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Optional[Literal["text-ada-001"]]): Model to use.
    RETURNS (OpenAI): Anthropic instance for 'text-ada' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Text-Ada.v1")
def openai_text_ada(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal["text-ada-001"] = "text-ada-001",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'text-ada' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Optional[Literal["text-ada-001"]]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'text-ada' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Davinci.v2")
def openai_davinci_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(
        max_tokens=500, temperature=_DEFAULT_TEMPERATURE
    ),
    name: Literal["davinci"] = "davinci",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'davinci' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Optional[Literal["davinci"]]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'davinci' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Davinci.v1")
def openai_davinci(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal["davinci"] = "davinci",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'davinci' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Optional[Literal["davinci"]]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'davinci' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Curie.v2")
def openai_curie_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(
        max_tokens=500, temperature=_DEFAULT_TEMPERATURE
    ),
    name: Literal["curie"] = "curie",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'curie' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Optional[Literal["curie"]]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'curie' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Curie.v1")
def openai_curie(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal["curie"] = "curie",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'curie' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Optional[Literal["curie"]]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'curie' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Babbage.v2")
def openai_babbage_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(
        max_tokens=500, temperature=_DEFAULT_TEMPERATURE
    ),
    name: Literal["babbage"] = "babbage",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'babbage' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Optional[Literal["babbage"]]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'babbage' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Babbage.v1")
def openai_babbage(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal["babbage"] = "babbage",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'babbage' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Optional[Literal["babbage"]]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'babbage' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Ada.v2")
def openai_ada_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(
        max_tokens=500, temperature=_DEFAULT_TEMPERATURE
    ),
    name: Literal["ada"] = "ada",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'ada' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Optional[Literal["ada"]]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'ada' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )


@registry.llm_models("spacy.Ada.v1")
def openai_ada(
    config: Dict[Any, Any] = SimpleFrozenDict(),
    name: Literal["ada"] = "ada",  # noqa: F722
    strict: bool = OpenAI.DEFAULT_STRICT,
    max_tries: int = OpenAI.DEFAULT_MAX_TRIES,
    interval: float = OpenAI.DEFAULT_INTERVAL,
    max_request_time: float = OpenAI.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> OpenAI:
    """Returns OpenAI instance for 'ada' model using REST to prompt API.

    config (Dict[Any, Any]): LLM config passed on to the model's initialization.
    name (Optional[Literal["ada"]]): Model to use.
    RETURNS (OpenAI): OpenAI instance for 'ada' model

    DOCS: https://spacy.io/api/large-language-models#models
    """
    return OpenAI(
        name=name,
        endpoint=endpoint or Endpoints.NON_CHAT.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )



================================================
FILE: spacy_llm/models/rest/palm/__init__.py
================================================
from .model import Endpoints, PaLM
from .registry import palm_bison, palm_bison_v2

__all__ = ["palm_bison", "palm_bison_v2", "PaLM", "Endpoints"]



================================================
FILE: spacy_llm/models/rest/palm/model.py
================================================
import os
import warnings
from enum import Enum
from typing import Any, Dict, Iterable, List, Sized

import requests  # type: ignore[import]
import srsly  # type: ignore[import]
from requests import HTTPError

from ..base import REST


class Endpoints(str, Enum):
    TEXT = "https://generativelanguage.googleapis.com/v1beta3/models/{model}:generateText?key={api_key}"
    MSG = "https://generativelanguage.googleapis.com/v1beta3/models/{model}:generateMessage?key={api_key}"


class PaLM(REST):
    @property
    def credentials(self) -> Dict[str, str]:
        api_key = os.getenv("PALM_API_KEY")
        if api_key is None:
            warnings.warn(
                "Could not find the API key to access the Cohere API. Ensure you have an API key "
                "set up via https://cloud.google.com/docs/authentication/api-keys#rest, then make it available as "
                "an environment variable 'PALM_API_KEY'."
            )

        assert api_key is not None
        return {"api_key": api_key}

    def _verify_auth(self) -> None:
        try:
            self([["What's 2+2?"]])
        except ValueError as err:
            if "API key not valid" in str(err):
                warnings.warn(
                    "Authentication with provided API key failed. Please double-check you provided the correct "
                    "credentials."
                )
            else:
                raise err

    def __call__(self, prompts: Iterable[Iterable[str]]) -> Iterable[Iterable[str]]:
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json",
        }
        url = self._endpoint.format(
            model=self._name, api_key=self._credentials["api_key"]
        )
        all_api_responses: List[List[str]] = []

        for prompts_for_doc in prompts:
            api_responses: List[str] = []
            prompts_for_doc = list(prompts_for_doc)

            def _request(json_data: Dict[str, Any]) -> Dict[str, Any]:
                r = self.retry(
                    call_method=requests.post,
                    url=url,
                    headers=headers,
                    json={**json_data, **self._config},
                    timeout=self._max_request_time,
                )
                try:
                    r.raise_for_status()
                except HTTPError as ex:
                    res_content = srsly.json_loads(r.content.decode("utf-8"))
                    # Include specific error message in exception.
                    error_message = res_content.get("error", {}).get("message", {})
                    # Catching other types of HTTPErrors (e.g., "429: too many requests")
                    raise ValueError(
                        f"Request to PaLM API failed: {error_message}"
                    ) from ex
                response = r.json()

                # PaLM returns a 'filter' key when a message was filtered due to safety concerns.
                if "filters" in response:
                    if self._strict:
                        raise ValueError(f"API call failed: {response}.")
                    else:
                        assert isinstance(prompts_for_doc, Sized)
                        return {
                            "error": [srsly.json_dumps(response)] * len(prompts_for_doc)
                        }

                return response

            # PaLM API currently doesn't accept batch prompts, so we're making
            # a request for each iteration. This approach can be prone to rate limit
            # errors. In practice, you can adjust _max_request_time so that the
            # timeout is larger.
            uses_chat = "chat" in self._name
            responses = [
                _request(
                    {
                        "prompt": {"text": prompt}
                        if not uses_chat
                        else {"messages": [{"content": prompt}]}
                    }
                )
                for prompt in prompts_for_doc
            ]
            for response in responses:
                if "candidates" in response:
                    # Although you can set the number of candidates in PaLM to be greater than 1, we only need to return a
                    # single value. In this case, we will just return the very first output.
                    api_responses.append(
                        response["candidates"][0].get(
                            "content" if uses_chat else "output",
                            srsly.json_dumps(response),
                        )
                    )
                else:
                    api_responses.append(srsly.json_dumps(response))

            all_api_responses.append(api_responses)

        return all_api_responses

    @staticmethod
    def _get_context_lengths() -> Dict[str, int]:
        return {
            "text-bison-001": 8192,
            "chat-bison-001": 8192,
        }



================================================
FILE: spacy_llm/models/rest/palm/registry.py
================================================
from typing import Any, Callable, Dict, Iterable, Optional

from confection import SimpleFrozenDict

from ....compat import Literal
from ....registry import registry
from .model import Endpoints, PaLM


@registry.llm_models("spacy.PaLM.v2")
def palm_bison_v2(
    config: Dict[Any, Any] = SimpleFrozenDict(temperature=0),
    name: Literal["chat-bison-001", "text-bison-001"] = "text-bison-001",  # noqa: F821
    strict: bool = PaLM.DEFAULT_STRICT,
    max_tries: int = PaLM.DEFAULT_MAX_TRIES,
    interval: float = PaLM.DEFAULT_INTERVAL,
    max_request_time: float = PaLM.DEFAULT_MAX_REQUEST_TIME,
    context_length: Optional[int] = None,
) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
    """Returns Google instance for PaLM Bison model using REST to prompt API.
    name (Literal["chat-bison-001", "text-bison-001"]): Model  to use.
    config (Dict[Any, Any]): LLM config arguments passed on to the initialization of the model instance.
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    context_length (Optional[int]): Context length for this model. Only necessary for sharding and if no context length
        natively provided by spacy-llm.
    RETURNS (PaLM): PaLM instance for Bison model.
    """
    return PaLM(
        name=name,
        endpoint=Endpoints.TEXT.value
        if name in {"text-bison-001"}
        else Endpoints.MSG.value,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=context_length,
    )


@registry.llm_models("spacy.PaLM.v1")
def palm_bison(
    config: Dict[Any, Any] = SimpleFrozenDict(temperature=0),
    name: Literal["chat-bison-001", "text-bison-001"] = "text-bison-001",  # noqa: F821
    strict: bool = PaLM.DEFAULT_STRICT,
    max_tries: int = PaLM.DEFAULT_MAX_TRIES,
    interval: float = PaLM.DEFAULT_INTERVAL,
    max_request_time: float = PaLM.DEFAULT_MAX_REQUEST_TIME,
    endpoint: Optional[str] = None,
) -> PaLM:
    """Returns Google instance for PaLM Bison model using REST to prompt API.
    name (Literal["chat-bison-001", "text-bison-001"]): Model  to use.
    config (Dict[Any, Any]): LLM config arguments passed on to the initialization of the model instance.
    strict (bool): If True, ValueError is raised if the LLM API returns a malformed response (i. e. any kind of JSON
        or other response object that does not conform to the expectation of how a well-formed response object from
        this API should look like). If False, the API error responses are returned by __call__(), but no error will
        be raised.
    max_tries (int): Max. number of tries for API request.
    interval (float): Time interval (in seconds) for API retries in seconds. We implement a base 2 exponential backoff
        at each retry.
    max_request_time (float): Max. time (in seconds) to wait for request to terminate before raising an exception.
    endpoint (Optional[str]): Endpoint to use. Defaults to standard endpoint.
    RETURNS (PaLM): PaLM instance for Bison model.
    """
    default_endpoint = (
        Endpoints.TEXT.value if name in {"text-bison-001"} else Endpoints.MSG.value
    )
    return PaLM(
        name=name,
        endpoint=endpoint or default_endpoint,
        config=config,
        strict=strict,
        max_tries=max_tries,
        interval=interval,
        max_request_time=max_request_time,
        context_length=None,
    )



================================================
FILE: spacy_llm/pipeline/__init__.py
================================================
from .llm import LLMWrapper

__all__ = ["LLMWrapper"]



================================================
FILE: spacy_llm/pipeline/llm.py
================================================
import logging
import warnings
from collections import defaultdict
from itertools import tee
from pathlib import Path
from typing import Dict, Iterable, Iterator, List, Optional, Tuple, Union, cast

import spacy
from spacy import util
from spacy.language import Language
from spacy.pipeline import Pipe
from spacy.tokens import Doc
from spacy.training import Example
from spacy.ty import InitializableComponent as Initializable
from spacy.vocab import Vocab

from .. import registry  # noqa: F401
from ..compat import TypedDict
from ..ty import Cache, LabeledTask, LLMTask, ModelWithContextLength
from ..ty import PromptExecutorType, ScorableTask, Serializable, supports_sharding
from ..ty import validate_type_consistency

logger = logging.getLogger("spacy_llm")
logger.addHandler(logging.NullHandler())

DEFAULT_MODEL_CONFIG = {
    "@llm_models": "spacy.GPT-3-5.v2",
    "strict": True,
}
DEFAULT_CACHE_CONFIG = {
    "@llm_misc": "spacy.BatchCache.v1",
    "path": None,
    "batch_size": 64,
    "max_batches_in_mem": 4,
}

DEFAULT_SAVE_IO = False
DEFAULT_VALIDATE_TYPES = True


class CacheConfigType(TypedDict):
    path: Optional[Path]
    batch_size: int
    max_batches_in_mem: int


@Language.factory(
    "llm",
    requires=[],
    assigns=[],
    default_config={
        "task": None,
        "model": DEFAULT_MODEL_CONFIG,
        "cache": DEFAULT_CACHE_CONFIG,
        "save_io": DEFAULT_SAVE_IO,
        "validate_types": DEFAULT_VALIDATE_TYPES,
    },
)
def make_llm(
    nlp: Language,
    name: str,
    task: Optional[LLMTask],
    model: PromptExecutorType,
    cache: Cache,
    save_io: bool,
    validate_types: bool,
) -> "LLMWrapper":
    """Construct an LLM component.

    nlp (Language): Pipeline.
    name (str): The component instance name, used to add entries to the
        losses during training.
    task (Optional[LLMTask]): An LLMTask can generate prompts for given docs, and can parse the LLM's responses into
        structured information and set that back on the docs.
    model (Callable[[Iterable[Any]], Iterable[Any]]]): Callable querying the specified LLM API.
    cache (Cache): Cache to use for caching prompts and responses per doc (batch).
    save_io (bool): Whether to save LLM I/O (prompts and responses) in the `Doc._.llm_io` custom extension.
    validate_types (bool): Whether to check if signatures of configured model and task are consistent.
    """
    if task is None:
        raise ValueError(
            "Argument `task` has not been specified, but is required (e. g. {'@llm_tasks': "
            "'spacy.NER.v3'})."
        )
    if validate_types:
        validate_type_consistency(task, model)

    return LLMWrapper(
        name=name,
        task=task,
        save_io=save_io,
        model=model,
        cache=cache,
        vocab=nlp.vocab,
    )


class LLMWrapper(Pipe):
    """Pipeline component for wrapping LLMs."""

    def __init__(
        self,
        name: str = "LLMWrapper",
        *,
        vocab: Vocab,
        task: LLMTask,
        model: PromptExecutorType,
        cache: Cache,
        save_io: bool,
    ) -> None:
        """
        Component managing execution of prompts to LLM APIs and mapping responses back to Doc/Span instances.

        name (str): The component instance name, used to add entries to the
            losses during training.
        vocab (Vocab): Pipeline vocabulary.
        task (LLMTask): An LLMTask can generate prompts for given docs, and can parse the LLM's responses into
            structured information and set that back on the docs.
        model (Callable[[Iterable[Any]], Iterable[Any]]]): Callable querying the specified LLM API.
        cache (Cache): Cache to use for caching prompts and responses per doc (batch).
        save_io (bool): Whether to save LLM I/O (prompts and responses) in the `Doc._.llm_io` custom extension.
        """
        self._name = name
        self._task = task
        self._model = model
        self._cache = cache
        self._save_io = save_io
        self._cache.initialize(vocab, self._task)

        # This is done this way because spaCy's `validate_init_settings` function
        # does not support `**kwargs: Any`.
        # See https://github.com/explosion/spaCy/blob/master/spacy/schemas.py#L111
        if isinstance(self._task, Initializable):
            self.initialize = self._task.initialize

        self._check_sharding()

    def _check_sharding(self):
        context_length: Optional[int] = None
        if isinstance(self._model, ModelWithContextLength):
            context_length = self._model.context_length
        if supports_sharding(self._task) and context_length is None:
            warnings.warn(
                "Task supports sharding, but model does not provide context length. Data won't be sharded, prompt "
                "might exceed the model's context length. Set context length in your config. If you think spacy-llm"
                " should provide the context length for this model automatically, report this to "
                "https://github.com/explosion/spacy-llm/issues."
            )

    @property
    def labels(self) -> Tuple[str, ...]:
        labels: Tuple[str, ...] = tuple()
        if isinstance(self._task, LabeledTask):
            labels = self._task.labels
        return labels

    def add_label(self, label: str, label_definition: Optional[str] = None) -> int:
        if not isinstance(self._task, LabeledTask):
            raise ValueError("The task of this LLM component does not have labels.")
        return self._task.add_label(label, label_definition)

    def clear(self) -> None:
        if not isinstance(self._task, LabeledTask):
            raise ValueError("The task of this LLM component does not have labels.")
        return self._task.clear()

    @property
    def task(self) -> LLMTask:
        return self._task

    def __call__(self, doc: Doc) -> Doc:
        """Apply the LLM wrapper to a Doc instance.

        doc (Doc): The Doc instance to process.
        RETURNS (Doc): The processed Doc.
        """
        docs = self._process_docs([doc])
        assert isinstance(docs[0], Doc)
        return docs[0]

    def score(
        self, examples: Iterable[Example], **kwargs
    ) -> Dict[str, Union[float, Dict[str, float]]]:
        """Score a batch of examples.

        examples (Iterable[Example]): The examples to score.
        RETURNS (Dict[str, Any]): The scores.

        DOCS: https://spacy.io/api/pipe#score
        """
        if isinstance(self._task, ScorableTask):
            return self._task.scorer(examples)
        return {}

    def pipe(self, stream: Iterable[Doc], *, batch_size: int = 128) -> Iterator[Doc]:
        """Apply the LLM prompt to a stream of documents.

        stream (Iterable[Doc]): A stream of documents.
        batch_size (int): The number of documents to buffer.
        YIELDS (Doc): Processed documents in order.
        """
        error_handler = self.get_error_handler()
        for doc_batch in spacy.util.minibatch(stream, batch_size):
            try:
                yield from iter(self._process_docs(doc_batch))
            except Exception as e:
                error_handler(self._name, self, doc_batch, e)

    def _process_docs(self, docs: List[Doc]) -> List[Doc]:
        """Process a batch of docs with the configured LLM model and task.
        If a cache is configured, only sends prompts to model for docs not found in cache.

        docs (List[Doc]): Input batch of docs.
        RETURNS (List[Doc]): Processed batch of docs with task annotations set.
        """
        support_sharding = supports_sharding(self._task)
        is_cached = [doc in self._cache for doc in docs]
        noncached_doc_batch = [doc for i, doc in enumerate(docs) if not is_cached[i]]
        if len(noncached_doc_batch) < len(docs):
            logger.debug(
                "Found %d docs in cache. Processing %d docs not found in cache",
                len(docs) - len(noncached_doc_batch),
                len(noncached_doc_batch),
            )

        # Process uncached docs.
        modified_docs: Iterator[Doc] = iter(())
        if len(noncached_doc_batch) > 0:
            n_iters = 3 if self._save_io else 2
            context_length: Optional[int] = None
            if isinstance(self._model, ModelWithContextLength):
                context_length = self._model.context_length

            # Only pass context length if this is a sharding task.
            prompts_iters = tee(
                self._task.generate_prompts(noncached_doc_batch, context_length)  # type: ignore[call-arg]
                if support_sharding
                else self._task.generate_prompts(noncached_doc_batch),
                n_iters + 1,
            )
            responses_iters = tee(
                self._model(
                    # Ensure that model receives Iterable[Iterable[Any]]. If task doesn't shard, its prompt is wrapped
                    # in a list to conform to the nested structure.
                    (
                        elem[0] if support_sharding else [elem]
                        for elem in prompts_iters[0]
                    )
                ),
                n_iters,
            )

            for prompt_data, response, doc in zip(
                prompts_iters[1], responses_iters[0], noncached_doc_batch
            ):
                logger.debug(
                    "Generated prompt for doc: %s\n%s",
                    doc.text,
                    prompt_data[0] if support_sharding else prompt_data,
                )
                logger.debug("LLM response for doc: %s\n%s", doc.text, response)

            modified_docs = iter(
                self._task.parse_responses(
                    (
                        elem[1] if support_sharding else noncached_doc_batch[i]
                        for i, elem in enumerate(prompts_iters[2])
                    ),
                    responses_iters[1],
                )
            )

        noncached_doc_batch_iter = iter(noncached_doc_batch)
        final_docs: List[Doc] = []
        for i, doc in enumerate(docs):
            if is_cached[i]:
                cached_doc = self._cache[doc]
                assert cached_doc is not None
                cached_doc._context = doc._context
                final_docs.append(cached_doc)
            else:
                doc = next(modified_docs)

                # Merge with doc's prior custom data.
                noncached_doc = next(noncached_doc_batch_iter)
                for extension in dir(noncached_doc._):
                    if not Doc.has_extension(extension):
                        Doc.set_extension(extension, default=None)
                    # Don't overwrite any non-None extension values in new doc.
                    if getattr(doc._, extension) is None:
                        setattr(doc._, extension, getattr(noncached_doc._, extension))
                doc.user_data = {**noncached_doc.user_data, **doc.user_data}
                doc._context = noncached_doc._context

                # Save raw IO (prompt and response), if save_io is True.
                if self._save_io:
                    # Make sure the `llm_io` field is set
                    doc.user_data["llm_io"] = doc.user_data.get(
                        "llm_io", defaultdict(dict)
                    )
                    llm_io = doc.user_data["llm_io"][self._name]
                    next_prompt = next(prompts_iters[-1])
                    if support_sharding:
                        llm_io["prompt"] = [
                            str(shard_prompt) for shard_prompt in next_prompt[0]
                        ]
                        llm_io["response"] = [
                            str(shard_response)
                            for shard_response in next(responses_iters[-1])
                        ]
                    else:
                        llm_io["prompt"] = str(next_prompt)
                        # Models always return nested responses. For non-sharding tasks this will always be a 1-list.
                        llm_io["response"] = str(next(responses_iters[-1])[0])

                self._cache.add(doc)
                final_docs.append(doc)

        return final_docs

    def to_bytes(
        self,
        *,
        exclude: Tuple[str] = cast(Tuple[str], tuple()),
    ) -> bytes:
        """Serialize the LLMWrapper to a bytestring.

        exclude (Tuple): Names of properties to exclude from serialization.
        RETURNS (bytes): The serialized object.
        """

        serialize = {}

        if isinstance(self._task, Serializable):
            serialize["task"] = lambda: self._task.to_bytes(exclude=exclude)  # type: ignore[attr-defined, union-attr]
        if isinstance(self._model, Serializable):
            serialize["model"] = lambda: self._model.to_bytes(exclude=exclude)  # type: ignore[attr-defined]

        return util.to_bytes(serialize, exclude)

    def from_bytes(
        self,
        bytes_data: bytes,
        *,
        exclude: Tuple[str] = cast(Tuple[str], tuple()),
    ) -> "LLMWrapper":
        """Load the LLMWrapper from a bytestring.

        bytes_data (bytes): The data to load.
        exclude (Tuple[str]): Names of properties to exclude from deserialization.
        RETURNS (LLMWrapper): Modified LLMWrapper instance.
        """

        deserialize = {}

        if isinstance(self._task, Serializable):
            deserialize["task"] = lambda b: self._task.from_bytes(b, exclude=exclude)  # type: ignore[attr-defined,union-attr]
        if isinstance(self._model, Serializable):
            deserialize["model"] = lambda b: self._model.from_bytes(b, exclude=exclude)  # type: ignore[attr-defined,union-attr]

        util.from_bytes(bytes_data, deserialize, exclude)
        return self

    def to_disk(
        self, path: Path, *, exclude: Tuple[str] = cast(Tuple[str], tuple())
    ) -> None:
        """Serialize the LLMWrapper to disk.
        path (Path): A path (currently unused).
        exclude (Tuple): Names of properties to exclude from serialization.
        """

        serialize = {}

        if isinstance(self._task, Serializable):
            serialize["task"] = lambda p: self._task.to_disk(p, exclude=exclude)  # type: ignore[attr-defined,union-attr]
        if isinstance(self._model, Serializable):
            serialize["model"] = lambda p: self._model.to_disk(p, exclude=exclude)  # type: ignore[attr-defined,union-attr]

        util.to_disk(path, serialize, exclude)

    def from_disk(
        self, path: Path, *, exclude: Tuple[str] = cast(Tuple[str], tuple())
    ) -> "LLMWrapper":
        """Load the LLMWrapper from disk.
        path (Path): A path (currently unused).
        exclude (Tuple): Names of properties to exclude from deserialization.
        RETURNS (LLMWrapper): Modified LLMWrapper instance.
        """

        serialize = {}

        if isinstance(self._task, Serializable):
            serialize["task"] = lambda p: self._task.from_disk(p, exclude=exclude)  # type: ignore[attr-defined,union-attr]
        if isinstance(self._model, Serializable):
            serialize["model"] = lambda p: self._model.from_disk(p, exclude=exclude)  # type: ignore[attr-defined,union-attr]

        util.from_disk(path, serialize, exclude)
        return self



================================================
FILE: spacy_llm/registry/__init__.py
================================================
from .normalizer import lowercase_normalizer, strip_normalizer
from .reader import fewshot_reader, file_reader
from .util import registry

__all__ = [
    "lowercase_normalizer",
    "strip_normalizer",
    "fewshot_reader",
    "file_reader",
    "registry",
]



================================================
FILE: spacy_llm/registry/normalizer.py
================================================
from typing import Callable

from .util import registry


@registry.misc("spacy.StripNormalizer.v1")
def strip_normalizer() -> Callable[[str], str]:
    """Return the labels as-is with stripped whitespaces

    RETURNS (Callable[[str], str])
    """
    return _strip


def _strip(s: str) -> str:
    return s.strip()


@registry.misc("spacy.LowercaseNormalizer.v1")
def lowercase_normalizer() -> Callable[[str], str]:
    """Return lowercase versions of the labels

    RETURNS (Callable[[str], str])
    """
    return _lowercase_strip


def _lowercase_strip(s: str) -> str:
    return s.strip().lower()



================================================
FILE: spacy_llm/registry/reader.py
================================================
import functools
from pathlib import Path
from typing import Any, Callable, Dict, Iterable, List, Optional, Union, cast

import srsly


def file_reader(path: Union[str, Path]) -> str:
    """Read a file from a path and return its contents.

    path (Union[str, Path]): path to the file

    RETURNS (str): the contents of the file
    """
    tpl_path = Path(path) if isinstance(path, str) else path
    with tpl_path.open("r", encoding="utf8") as tpl_file:
        tpl_text = tpl_file.read()

    return tpl_text


def fewshot_reader(path: Union[str, Path]) -> Callable[[], Iterable[Dict[str, Any]]]:
    """Read a file containing examples to include in few-shot learning

    path (Union[str,Path]): path to an examples file (.yml, .yaml, .json, .jsonl)

    RETURNS (Iterable[Dict[str, Any]]): an iterable of examples to be parsed by the template
    """
    eg_path = Path(path) if isinstance(path, str) else path
    return functools.partial(_fewshot_reader, eg_path=eg_path)


def _fewshot_reader(eg_path: Path) -> Iterable[Dict[str, Any]]:
    data: Optional[List] = None

    if eg_path is None:
        data = []

    else:
        if not eg_path.exists():
            raise ValueError(
                f"Specified file path: {str(eg_path)} doesn't exist. "
                "Please ensure to provide a valid file path."
            )

        suffix = eg_path.suffix.replace("yaml", "yml")
        readers = {
            ".yml": srsly.read_yaml,
            ".json": srsly.read_json,
            ".jsonl": lambda path: list(srsly.read_jsonl(eg_path)),
        }

        # Try to read in indicated format.
        success = False
        if suffix in readers:
            try:
                data = readers[suffix](eg_path)
                success = True
            except Exception:
                pass
        if not success:
            # Try to read file in all supported formats.
            for file_format, reader in readers.items():
                if file_format == suffix:
                    continue
                try:
                    data = reader(eg_path)
                    success = True
                    break
                except Exception:
                    pass

        # Raise error if reading file didn't work.
        if not success:
            raise ValueError(
                "The examples file expects a .yml, .yaml, .json, or .jsonl file type. Ensure that your file "
                "corresponds to one of these file formats."
            )

    if not isinstance(data, list) or not all(isinstance(d, dict) for d in data):
        raise ValueError(
            f"Cannot interpret prompt examples from {str(eg_path)}. Please check your formatting to ensure that the "
            f"examples specified in {eg_path} are described as list of dictionaries that fit the structure described by"
            f" the prompt example class for the corresponding class."
        )

    return cast(Iterable[Dict[str, Any]], data)



================================================
FILE: spacy_llm/registry/util.py
================================================
import catalogue  # type: ignore[import]
import spacy

for registry_name in ("queries", "models", "tasks", "misc"):
    if f"llm_{registry_name}" not in spacy.util.registry.get_registry_names():
        spacy.util.registry.create(f"llm_{registry_name}", entry_points=True)


class registry(spacy.util.registry):
    llm_models: catalogue.Registry
    llm_queries: catalogue.Registry
    llm_tasks: catalogue.Registry
    llm_misc: catalogue.Registry



================================================
FILE: spacy_llm/tasks/__init__.py
================================================
from spacy import Language

from ..pipeline.llm import DEFAULT_CACHE_CONFIG, DEFAULT_MODEL_CONFIG, DEFAULT_SAVE_IO
from ..pipeline.llm import DEFAULT_VALIDATE_TYPES, make_llm
from .builtin_task import BuiltinTask
from .entity_linker import EntityLinkerTask, make_entitylinker_task
from .lemma import LemmaTask, make_lemma_task
from .ner import NERTask, make_ner_task_v3
from .noop import NoopTask, ShardingNoopTask, make_noop_task, make_noopnoshards_task
from .raw import RawTask, make_raw_task
from .rel import RELTask, make_rel_task
from .sentiment import SentimentTask, make_sentiment_task
from .spancat import SpanCatTask, make_spancat_task_v3
from .summarization import SummarizationTask, make_summarization_task
from .textcat import TextCatTask, make_textcat_task
from .translation import TranslationTask, make_translation_task

_LATEST_TASKS = (
    "spacy.EntityLinker.v1",
    "spacy.NER.v3",
    "spacy.Raw.v1",
    "spacy.REL.v1",
    "spacy.Sentiment.v1",
    "spacy.SpanCat.v3",
    "spacy.Summarization.v1",
    "spacy.TextCat.v3",
    "spacy.Translation.v1",
)

# Register llm_TASK factories with default models.
for task_handle in _LATEST_TASKS:
    Language.factory(
        name=f"llm_{task_handle.split('.')[1].lower()}",
        default_config={
            "task": {"@llm_tasks": task_handle},
            "model": DEFAULT_MODEL_CONFIG,
            "cache": DEFAULT_CACHE_CONFIG,
            "save_io": DEFAULT_SAVE_IO,
            "validate_types": DEFAULT_VALIDATE_TYPES,
        },
        func=make_llm,
    )

__all__ = [
    "make_entitylinker_task",
    "make_lemma_task",
    "make_ner_task_v3",
    "make_noop_task",
    "make_noopnoshards_task",
    "make_raw_task",
    "make_rel_task",
    "make_sentiment_task",
    "make_spancat_task_v3",
    "make_summarization_task",
    "make_textcat_task",
    "make_translation_task",
    "BuiltinTask",
    "EntityLinkerTask",
    "LemmaTask",
    "NERTask",
    "NoopTask",
    "RawTask",
    "RELTask",
    "SentimentTask",
    "ShardingNoopTask",
    "SpanCatTask",
    "SummarizationTask",
    "TextCatTask",
    "TranslationTask",
]



================================================
FILE: spacy_llm/tasks/builtin_task.py
================================================
import abc
from itertools import tee
from pathlib import Path
from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Type, cast

import jinja2
import jinja2.sandbox
import srsly
from spacy import Errors, Language, util
from spacy.tokens import Doc
from spacy.training import Example

from ..compat import Self
from ..registry import lowercase_normalizer
from ..ty import FewshotExample, ShardMapper, ShardReducer, TaskResponseParser


class BuiltinTask(abc.ABC):
    """Abstract base task implementing interfaces and/or functionality expected from all built-in tasks:
        - working prompt template strings
        - swappable response parsers
        - swappable prompt example type
        - integration of fewshot example into the fully rendered prompt
        - initializable (in line with other spaCy components)
        - (de-)serialization

    On the relation of BuiltinTask to ShardingLLMTask: the latter specifies the minimal contract a task implementation
    has to fulfill, whereas a BuiltinTask requires (and offers) functionality beyond that. The rationale behind that is
    that built-in tasks should provide as smooth a usage experience as possible while still making it as easy as
    possible for users to write their own, custom tasks.
    """

    def __init__(
        self,
        parse_responses: TaskResponseParser,
        prompt_example_type: Type[FewshotExample[Self]],
        template: str,
        prompt_examples: Optional[List[FewshotExample[Self]]],
        shard_mapper: ShardMapper,
        shard_reducer: ShardReducer[Self],
    ):
        """Initializes task.
        parse_responses (TaskResponseParser[Self]): Callable for parsing LLM responses for this task.
        prompt_example_type (Type[FewshotExample[Self]): Type to use for fewshot examples.
        template (str): Prompt template passed to the model.
        prompt_examples (Optional[List[FewshotExample[Self]]]): Optional list of few-shot examples to include in prompts.
        shard_mapper (ShardMapper): Maps docs to shards if they don't fit into the model context.
        shard_reducer (ShardReducer[Self]): Reduces doc shards back into one doc instance.
        """
        self._parse_responses = parse_responses
        self._prompt_examples = prompt_examples or []
        self._template = template
        self._prompt_example_type = prompt_example_type
        self._shard_mapper = shard_mapper
        self._shard_reducer = shard_reducer

    def generate_prompts(
        self, docs: Iterable[Doc], context_length: Optional[int] = None
    ) -> Iterable[Tuple[Iterable[Any], Iterable[Doc]]]:
        """Generate prompts from docs.
        docs (Iterable[Doc]): Docs to generate prompts from.
        ontext_length (int): Context length for model this task is executed with. Needed for sharding and fusing docs,
            if the corresponding prompts exceed the context length. If None, context length is assumed to be infinite.
        RETURNS (Iterable[Tuple[Iterable[Any], Iterable[Doc]]]): Iterable with one to n prompts per doc (multiple
            prompts in case of multiple shards) and the corresponding shards. The relationship between shard and prompt
            is 1:1.
        """
        environment = jinja2.sandbox.SandboxedEnvironment()
        _template = environment.from_string(self._template)

        def render_template(shard: Doc, i_shard: int, i_doc: int, n_shards: int) -> str:
            """Renders template for a given doc (shard).
            shard (Doc): Doc shard. Note that if the prompt is small enough to fit within the model's context window,
                there will only be one shard, which is identical to the original doc.
            i_shard (int): Shard index (w.r.t. shard's Doc instance).
            i_doc (int): Doc index.
            n_shards (int): Total number of shards.
            RETURNS (str): Rendered template.
            """
            return _template.render(
                text=shard.text,
                prompt_examples=self._prompt_examples,
                **self._get_prompt_data(shard, i_shard, i_doc, n_shards),
            )

        for _i_doc, _doc in enumerate(self._preprocess_docs_for_prompt(docs)):
            # If no context length provided (e. g. because models don't provide it): don't shard.
            shards = (
                self._shard_mapper(_doc, _i_doc, context_length, render_template)
                if context_length is not None
                else [_doc]
            )
            shards = list(shards)
            yield [
                render_template(_shard, _i_shard, _i_doc, len(shards))
                for _i_shard, _shard in enumerate(shards)
            ], shards

    def _get_prompt_data(
        self, shard: Doc, i_shard: int, i_doc: int, n_shards: int
    ) -> Dict[str, Any]:
        """Returns data injected into prompt template. No-op if not overridden by inheriting task class. The data
        returned by this might be static (i. e. the same for all doc shards) or dynamic (contingent on the doc shard).
        shard (Doc): Doc (shard) for which prompt data should be fetched.
        i_shard (int): Shard index (w.r.t. shard's Doc instance).
        i_doc (int): Doc index.
        n_shards (int): Total number of shards.
        RETURNS (Dict[str, Any]): Data injected into prompt template.
        """
        return {}

    def _preprocess_docs_for_prompt(self, docs: Iterable[Doc]) -> Iterable[Doc]:
        """Preprocesses docs before injection into prompt template. No-op if not overridden by inheriting task class.
        docs (Iterable[Doc]): Docs to generate prompts from.
        RETURNS (Iterable[Doc]): Preprocessed docs.
        """
        return docs

    @abc.abstractmethod
    def parse_responses(
        self, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[Any]]
    ) -> Iterable[Doc]:
        """
        Parses LLM responses.
        shards (Iterable[Iterable[Doc]]): Doc shards to map responses into.
        responses ([Iterable[Iterable[Any]]]): LLM responses per doc.
        RETURNS (Iterable[Doc]]): Updated docs.
        """

    def _initialize(
        self,
        get_examples: Callable[[], Iterable["Example"]],
        nlp: Language,
        n_prompt_examples: int = 0,
        **kwargs: Any,
    ) -> None:
        """Initializes prompt examples from Doc examples.
        get_examples (Callable[[], Iterable["Example"]]): Callable that provides examples
            for initialization.
        nlp (Language): Language instance.
        n_prompt_examples (int): How many prompt examples to infer from the provided Example objects.
            0 by default. Takes all examples if set to -1.
        """
        for eg in get_examples():
            if n_prompt_examples < 0 or len(self._prompt_examples) < n_prompt_examples:
                prompt_example = self._prompt_example_type.generate(eg, self)  # type: ignore[arg-type]
                if prompt_example:
                    self._prompt_examples.append(prompt_example)

    def get_cfg(self) -> Dict[str, Any]:
        """Serialize the task's configuration attributes."""
        cfg = {key: getattr(self, key) for key in self._cfg_keys}
        return cfg

    def set_cfg(self, cfg: Dict[str, Any]) -> None:
        """Deserialize the task's configuration attributes.
        cfg (Dict[str, Any]): dictionary containing configuration attributes.
        """
        for key, value in cfg.items():
            setattr(self, key, value)

    def _get_prompt_examples(self) -> List[Dict[str, Any]]:
        """Serialize examples."""
        examples = [eg.dict() for eg in self._prompt_examples]
        return examples

    def _set_prompt_examples(self, examples: List[Dict[str, Any]]) -> None:
        """Set prompt examples.
        examples (List[Dict[str, Any]]): prompt examples.
        """
        self._prompt_examples = [
            self._prompt_example_type.parse_obj(eg) for eg in examples
        ]

    def to_bytes(
        self,
        *,
        exclude: Tuple[str] = cast(Tuple[str], tuple()),
    ) -> bytes:
        """Serialize the BuiltinTask to a bytestring.

        exclude (Tuple): Names of properties to exclude from serialization.
        RETURNS (bytes): The serialized object.
        """
        serialize = {
            "cfg": lambda: srsly.json_dumps(self.get_cfg()),
            "prompt_examples": lambda: srsly.msgpack_dumps(self._get_prompt_examples()),
        }

        return util.to_bytes(serialize, exclude)

    def from_bytes(
        self,
        bytes_data: bytes,
        *,
        exclude: Tuple[str] = cast(Tuple[str], tuple()),
    ) -> "BuiltinTask":
        """Load the BuiltinTask from a bytestring.

        bytes_data (bytes): The data to load.
        exclude (Tuple[str]): Names of properties to exclude from deserialization.
        RETURNS (BuiltinTask): Modified BuiltinTask instance.
        """
        deserialize = {
            "cfg": lambda b: self.set_cfg(srsly.json_loads(b)),
            "prompt_examples": lambda b: self._set_prompt_examples(
                srsly.msgpack_loads(b)
            ),
        }

        util.from_bytes(bytes_data, deserialize, exclude)
        return self

    def to_disk(
        self,
        path: Path,
        *,
        exclude: Tuple[str] = cast(Tuple[str], tuple()),
    ) -> None:
        """Serialize the task to disk.

        path (Path): A path (currently unused).
        exclude (Tuple): Names of properties to exclude from serialization.
        """
        serialize = {
            "cfg": lambda p: srsly.write_json(p, self.get_cfg()),
            "prompt_examples": lambda p: srsly.write_msgpack(
                p, self._get_prompt_examples()
            ),
        }

        util.to_disk(path, serialize, exclude)

    def from_disk(
        self,
        path: Path,
        *,
        exclude: Tuple[str] = cast(Tuple[str], tuple()),
    ) -> "BuiltinTask":
        """Deserialize the task from disk.

        path (Path): A path (currently unused).
        exclude (Tuple): Names of properties to exclude from serialization.
        RETURNS (BuiltinTask): The BuiltinTask instance.
        """

        deserialize = {
            "cfg": lambda p: self.set_cfg(srsly.read_json(p)),
            "prompt_examples": lambda p: self._set_prompt_examples(
                srsly.read_msgpack(p)
            ),
        }

        util.from_disk(path, deserialize, exclude)
        return self

    @property
    def prompt_template(self) -> str:
        return self._template

    @property
    @abc.abstractmethod
    def _cfg_keys(self) -> List[str]:
        """A list of configuration attributes to serialize."""
        pass

    @classmethod
    def _check_extension(cls, extension: str) -> None:
        """Add extension if need be.
        extension (str): Extension to check/add.
        """
        if not Doc.has_extension(extension):
            Doc.set_extension(extension, default=[])

    @staticmethod
    def _tee_2d_iterable(
        data: Iterable[Iterable[Any]], n: int
    ) -> Tuple[Iterable[List[Doc]], ...]:
        """Tees two-dimensional Iterable. As Iterables in the nested iterables get consumed with the first access, we
        need to materialize them - this is done by converting them to a list.
        data (Iterable[Iterable[Any]]): Data to tee.
        n (int): Number of tees to return.
        RETURNS (Tuple[Iterable[List[Doc]], ...]): n-sized tuple of Iterables with inner Iterables converted to Lists.
        """
        return tee((list(inner_data) for inner_data in data), n)


class BuiltinTaskWithLabels(BuiltinTask, abc.ABC):
    """Built-in tasks with labels."""

    def __init__(
        self,
        parse_responses: TaskResponseParser,
        prompt_example_type: Type[FewshotExample[Self]],
        template: str,
        prompt_examples: Optional[List[FewshotExample[Self]]],
        shard_mapper: ShardMapper,
        shard_reducer: ShardReducer[Self],
        labels: List[str],
        label_definitions: Optional[Dict[str, str]],
        normalizer: Optional[Callable[[str], str]],
    ):
        """Built-in task with labels.

        parse_responses (TaskResponseParser[Self]): Callable for parsing LLM responses for this task.
        prompt_example_type (Type[FewshotExample[Self]): Type to use for fewshot examples.
        template (str): Prompt template passed to the model.
        prompt_examples (Optional[List[FewshotExample[Self]]]): Optional list of few-shot examples to include in prompts.
        shard_mapper (ShardMapper): Maps docs to shards if they don't fit into the model context.
        shard_reducer (ShardReducer[Self]): Reduces doc shards back into one doc instance.
        labels (List[str]): List of labels to pass to the template.
            Leave empty to (optionally) populate it at initialization time.
        label_definitions (Optional[Dict[str, str]]): Map of label -> description
            of the label to help the language model output the entities wanted.
            It is usually easier to provide these definitions rather than
            full examples, although both can be provided.
        normalizer (Optional[Callable[[str], str]]): optional normalizer function.
        """
        super().__init__(
            parse_responses=parse_responses,
            prompt_example_type=prompt_example_type,
            template=template,
            prompt_examples=prompt_examples,
            shard_mapper=shard_mapper,
            shard_reducer=shard_reducer,
        )
        self._normalizer = normalizer if normalizer else lowercase_normalizer()
        self._label_dict = {
            self._normalizer(label): label for label in sorted(set(labels))
        }
        self._label_definitions = label_definitions

    def _initialize(  # type: ignore[override]
        self,
        get_examples: Callable[[], Iterable["Example"]],
        nlp: Language,
        labels: List[str] = [],
        n_prompt_examples: int = 0,
        **kwargs,
    ) -> None:
        """Supports initialization of tasks with labels by auto-discovering labels and returning the derived few-shot
        examples and label dict.

        Labels can be set through, by order of precedence:

        - the `[initialize]` section of the pipeline configuration
        - the `labels` argument supplied to the task factory
        - the labels found in the examples

        get_examples (Callable[[], Iterable["Example"]]): Callable that provides examples
            for initialization.
        nlp (Language): Language instance.
        labels (List[str]): Optional list of labels.
        n_prompt_examples (int): How many prompt examples to infer from the Example objects.
            0 by default. Takes all examples if set to -1.
        """
        if not labels:
            labels = list(self._label_dict.values())
        infer_labels = not labels

        if infer_labels:
            labels = []

        for eg in get_examples():
            if infer_labels:
                labels.extend(self._extract_labels_from_example(eg))
            if n_prompt_examples < 0 or len(self._prompt_examples) < n_prompt_examples:
                prompt_example = self._prompt_example_type.generate(eg, self)  # type: ignore[arg-type]
                if prompt_example:
                    self._prompt_examples.append(prompt_example)

        self._label_dict = {
            self._normalizer(label): label for label in sorted(set(labels))
        }

    @abc.abstractmethod
    def _extract_labels_from_example(self, example: Example) -> List[str]:
        """Extracts labels from Example instance.
        example (Example): Example to extract labels from.
        RETURNS (List[str]): Labels extracted from Example instance.
        """

    @property
    def labels(self) -> Tuple[str, ...]:
        return tuple(self._label_dict.values())

    def add_label(self, label: str, label_definition: Optional[str] = None) -> int:
        """Add a label to the task"""
        if not isinstance(label, str):
            raise ValueError(Errors.E187)
        if label in self.labels:
            return 0
        self._label_dict[self._normalizer(label)] = label
        if label_definition is None:
            return 1
        if self._label_definitions is None:
            self._label_definitions = {}
        self._label_definitions[label] = label_definition
        return 1

    def clear(self) -> None:
        """Reset all labels."""
        self._label_dict = {}
        self._label_definitions = None

    @property
    def normalizer(self) -> Callable[[str], str]:
        return self._normalizer

    @property
    def label_dict(self) -> Dict[str, str]:
        return self._label_dict

    @property
    def label_definitions(self) -> Optional[Dict[str, str]]:
        return self._label_definitions



================================================
FILE: spacy_llm/tasks/noop.py
================================================
import warnings
from typing import Iterable, Optional, Tuple

from spacy.tokens import Doc

from ..registry import registry

_NOOP_PROMPT = "Don't do anything."


@registry.llm_tasks("spacy.NoOp.v1")
def make_noop_task():
    return ShardingNoopTask()


@registry.llm_tasks("spacy.NoOpNoShards.v1")
def make_noopnoshards_task():
    return NoopTask()


class ShardingNoopTask:
    def generate_prompts(
        self, docs: Iterable[Doc], context_length: Optional[int] = None
    ) -> Iterable[Tuple[Iterable[str], Iterable[Doc]]]:
        for doc in docs:
            yield [_NOOP_PROMPT], [doc]

    def parse_responses(
        self, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[str]]
    ) -> Iterable[Doc]:
        with warnings.catch_warnings():
            warnings.filterwarnings(
                "ignore",
                category=UserWarning,
                message=".*Skipping .* while merging docs.",
            )
            docs = [
                Doc.from_docs(list(shards_for_doc), ensure_whitespace=True)
                for shards_for_doc in shards
            ]
        return docs

    @property
    def prompt_template(self) -> str:
        return """
        This is the NoOp
        prompt template
        """


class NoopTask:
    def generate_prompts(self, docs: Iterable[Doc]) -> Iterable[str]:
        for doc in docs:
            yield _NOOP_PROMPT

    def parse_responses(
        self, docs: Iterable[Doc], responses: Iterable[str]
    ) -> Iterable[Doc]:
        return docs

    @property
    def prompt_template(self) -> str:
        return """
        This is the NoOp
        prompt template
        """



================================================
FILE: spacy_llm/tasks/entity_linker/__init__.py
================================================
from .registry import make_entitylinker_task
from .task import EntityLinkerTask
from .util import ELExample

__all__ = ["make_entitylinker_task", "EntityLinkerTask", "ELExample"]



================================================
FILE: spacy_llm/tasks/entity_linker/candidate_selector.py
================================================
import warnings
from typing import Dict, Iterable, Optional

from spacy import Vocab
from spacy.kb import InMemoryLookupKB
from spacy.pipeline import EntityLinker
from spacy.tokens import Span

from .ty import Entity, InMemoryLookupKBLoader
from .util import UNAVAILABLE_ENTITY_DESC


class KBCandidateSelector:
    """Initializes a spaCy InMemoryLookupKB and uses its candidate selection mechanism to return entity candidates."""

    def __init__(
        self,
        kb_loader: InMemoryLookupKBLoader,
        top_n: int,
    ):
        """Generates KBCandidateSelector. Note that this class has to be initialized (.initialize()) before being used.
        kb_loader (InMemoryLookupKBLoader): KB loader.
        top_n (int): Top n candidates to include in prompt.
        """
        self._kb_loader = kb_loader
        self._kb: Optional[InMemoryLookupKB] = None
        self._descs: Dict[str, str] = {}
        self._top_n = top_n

    def initialize(self, vocab: Vocab) -> None:
        """Initialize instance with vocabulary.
        vocab (Vocab): Vocabulary.
        """
        self._kb, self._descs = self._kb_loader(vocab)

    def __call__(self, mentions: Iterable[Span]) -> Iterable[Iterable[Entity]]:
        """Retrieves top n candidates using spaCy's entity linker's .get_candidates_batch().
        mentions (Iterable[Span]): Mentions to look up entity candidates for.
        RETURNS (Iterable[Iterable[Entity]]): Top n entity candidates per mention.
        """
        if self._kb is None:
            raise ValueError("CandidateSelector has to be initialized before usage.")

        all_cands = self._kb.get_candidates_batch(mentions)
        for cands in all_cands:
            assert isinstance(cands, list)
            cands.sort(key=lambda x: x.prior_prob, reverse=True)

        return [
            [
                Entity(
                    id=cand.entity_,
                    description=self.get_entity_description(cand.entity_),
                )
                for cand in cands[: self._top_n]
            ]
            if len(cands) > 0
            else [Entity(id=EntityLinker.NIL, description=UNAVAILABLE_ENTITY_DESC)]
            for cands in all_cands
        ]

    def get_entity_description(self, entity_id: str) -> str:
        """Returns entity description for entity ID. If none found, a warning is emitted and
            spacy_llm.tasks.entity_linker.util.UNAVAILABLE_ENTITY_DESC is returned.
        entity_id (str): Entity whose ID should be looked up.
        RETURNS (str): Entity description for entity with specfied ID. If no description found, returned string equals
            spacy_llm.tasks.entity_linker.util.UNAVAILABLE_ENTITY_DESC.
        """
        if entity_id not in self._descs:
            warnings.warn(
                f"Entity with ID {entity_id} is not in provided descriptions."
            )

        return self._descs.get(entity_id, UNAVAILABLE_ENTITY_DESC)



================================================
FILE: spacy_llm/tasks/entity_linker/parser.py
================================================
import re
from typing import Iterable, List

from spacy.pipeline import EntityLinker
from spacy.tokens import Doc, Span

from .task import EntityLinkerTask


def parse_responses_v1(
    task: EntityLinkerTask,
    shards: Iterable[Iterable[Doc]],
    responses: Iterable[Iterable[str]],
) -> Iterable[List[List[Span]]]:
    """Parses LLM responses for spacy.EntityLinker.v1.
    task (EntityLinkerTask): Task instance.
    shards (Iterable[Iterable[Doc]]): Doc shards.
    responses (Iterable[Iterable[str]]): LLM responses.
    RETURNS (Iterable[List[List[Span]]): Entity spans per shard.
    """

    for i_doc, (shards_for_doc, responses_for_doc) in enumerate(zip(shards, responses)):
        results_for_doc: List[List[Span]] = []
        for i_shard, (shard, response) in enumerate(
            zip(shards_for_doc, responses_for_doc)
        ):
            solutions = [
                sol.replace("::: ", "")[1:-1]
                for sol in re.findall(r"::: <.*>", response)
            ]

            # Set ents anew by copying them and specifying the KB ID.
            ents = [
                ent
                for i_ent, ent in enumerate(shard.ents)
                if task.has_ent_cands_by_shard[i_doc][i_shard][i_ent]
            ]

            results_for_doc.append(
                [
                    Span(
                        doc=shard,
                        start=ent.start,
                        end=ent.end,
                        label=ent.label,
                        vector=ent.vector,
                        vector_norm=ent.vector_norm,
                        kb_id=solution if solution != "NIL" else EntityLinker.NIL,
                    )
                    for ent, solution in zip(ents, solutions)
                ]
            )

        yield results_for_doc



================================================
FILE: spacy_llm/tasks/entity_linker/registry.py
================================================
import warnings
from pathlib import Path
from typing import Optional, Type, Union

from spacy.scorer import Scorer

from ...registry import registry
from ...ty import ExamplesConfigType, FewshotExample, ShardMapper, ShardReducer
from ...ty import TaskResponseParser
from ..util.sharding import make_shard_mapper
from .candidate_selector import KBCandidateSelector
from .parser import parse_responses_v1
from .task import DEFAULT_EL_TEMPLATE_V1, EntityLinkerTask
from .ty import EntDescReader, InMemoryLookupKBLoader
from .util import ELExample, KBFileLoader, KBObjectLoader, ent_desc_reader_csv
from .util import reduce_shards_to_doc, score


@registry.llm_tasks("spacy.EntityLinker.v1")
def make_entitylinker_task(
    template: str = DEFAULT_EL_TEMPLATE_V1,
    parse_responses: Optional[TaskResponseParser[EntityLinkerTask]] = None,
    prompt_example_type: Optional[Type[FewshotExample]] = None,
    examples: ExamplesConfigType = None,
    shard_mapper: Optional[ShardMapper] = None,
    shard_reducer: Optional[ShardReducer] = None,
    scorer: Optional[Scorer] = None,
):
    """EntityLinker.v1 task factory.

    template (str): Prompt template passed to the model.
    parse_responses (Optional[TaskResponseParser]): Callable for parsing LLM responses for this task.
    prompt_example_type (Optional[Type[FewshotExample]]): Type to use for fewshot examples.
    examples (ExamplesConfigType): Optional callable that reads a file containing task examples for few-shot learning.
        If None is passed, then zero-shot learning will be used.
    shard_mapper (Optional[ShardMapper]): Maps docs to shards if they don't fit into the model context.
    shard_reducer (Optional[ShardReducer]): Reduces doc shards back into one doc instance.
    scorer (Optional[Scorer]): Scorer function.
    """
    raw_examples = examples() if callable(examples) else examples
    example_type = prompt_example_type or ELExample
    examples = [example_type(**eg) for eg in raw_examples] if raw_examples else []
    # Ensure there is a reason for every solution, even if it's empty. This makes templating easier.
    for example in examples:
        if example.reasons is None:
            example.reasons = [""] * len(example.solutions)
        elif 0 < len(example.reasons) < len(example.solutions):
            warnings.warn(
                f"The number of reasons doesn't match the number of solutions ({len(example.reasons)} "
                f"vs. {len(example.solutions)}). There must be one reason per solution for an entity "
                f"linking example, or no reasons at all. Ignoring all specified reasons."
            )
            example.reasons = [""] * len(example.solutions)

    return EntityLinkerTask(
        template=template,
        parse_responses=parse_responses or parse_responses_v1,
        prompt_example_type=example_type,
        prompt_examples=examples,
        shard_mapper=shard_mapper or make_shard_mapper(),
        shard_reducer=shard_reducer or make_shard_reducer(),
        scorer=scorer or score,
    )


@registry.llm_misc("spacy.CandidateSelector.v1")
def make_candidate_selector_pipeline(
    kb_loader: InMemoryLookupKBLoader,
    top_n: int = 5,
) -> KBCandidateSelector:
    """Generates CandidateSelector. Note that this class has to be initialized (.initialize()) before being used.
    kb_loader (InMemoryLookupKBLoader): KB loader.
    top_n (int): Top n candidates to include in prompt.
    """
    # Note: we could also move the class implementation here directly. This was just done to separate registration from
    # implementation code.
    return KBCandidateSelector(
        kb_loader=kb_loader,
        top_n=top_n,
    )


@registry.llm_misc("spacy.EntityDescriptionReader.v1")
def make_ent_desc_reader() -> EntDescReader:
    """Instantiates entity description reader with two columns: ID and description.
    RETURNS (Dict[str, str]): Dict with ID -> description.
    """
    return ent_desc_reader_csv


@registry.llm_misc("spacy.KBObjectLoader.v1")
def make_kb_object_loader(
    path: Union[str, Path],
    nlp_path: Optional[Union[str, Path]] = None,
    desc_path: Optional[Union[str, Path]] = None,
    ent_desc_reader: Optional[EntDescReader] = None,
) -> KBObjectLoader:
    """Instantiates KBObjectLoader for loading KBs from serialized directories (as done during spaCy pipeline
    serialization).
    path (Union[str, Path]): Path to KB directory.
    nlp_path (Optional[Union[str, Path]]): Path to NLP pipeline whose vocab data to use. If this is None, the loader
        will try to load the serialized pipeline surrounding the KB directory.
    desc_path (Optional[Union[Path, str]]): Path to .csv file with descriptions for entities. Has to have two
        columns with the first one being the entity ID, the second one being the description. The entity ID has to
        match with the entity ID in the stored knowledge base.
        If not specified, all entity descriptions provided in prompts will be a generic "No description available"
        or something else to this effect.
    ent_desc_reader (Optional[EntDescReader]): Entity description reader.
    RETURNS (KBObjectLoader): Loader instance.
    """
    return KBObjectLoader(
        path=path,
        nlp_path=nlp_path,
        desc_path=desc_path,
        ent_desc_reader=ent_desc_reader or ent_desc_reader_csv,
    )


@registry.llm_misc("spacy.KBFileLoader.v1")
def make_kb_file_loader(path: Union[str, Path]) -> KBFileLoader:
    """Instantiates KBFileLoader for generating KBs from a file containing entity data.
    path (Union[str, Path]): Path to KB directory.
    RETURNS (KBFileLoader): Loader instance.
    """
    return KBFileLoader(path=path)


@registry.llm_misc("spacy.EntityLinkerShardReducer.v1")
def make_shard_reducer() -> ShardReducer:
    return reduce_shards_to_doc



================================================
FILE: spacy_llm/tasks/entity_linker/task.py
================================================
from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Type

from spacy import Language, Vocab
from spacy.pipeline import EntityLinker
from spacy.tokens import Doc, Span
from spacy.training import Example

from ...compat import Self
from ...ty import FewshotExample, Scorer, ShardMapper, ShardReducer, TaskResponseParser
from ..builtin_task import BuiltinTask
from ..templates import read_template
from .ty import CandidateSelector, Entity, InitializableCandidateSelector

DEFAULT_EL_TEMPLATE_V1 = read_template("entity_linker.v1")


class EntityLinkerTask(BuiltinTask):
    def __init__(
        self,
        parse_responses: TaskResponseParser[Self],
        prompt_example_type: Type[FewshotExample[Self]],
        prompt_examples: Optional[List[FewshotExample[Self]]],
        template: str,
        shard_mapper: ShardMapper,
        shard_reducer: ShardReducer[Self],
        scorer: Scorer,
    ):
        """Default entity linking task.

        parse_responses (TaskResponseParser[Self]): Callable for parsing LLM responses for this task.
        prompt_example_type (Type[FewshotExample[Self]]): Type to use for fewshot examples.
        prompt_examples (Optional[List[FewshotExample[Self]]]): Optional list of few-shot examples to include in prompts.
        template (str): Prompt template passed to the model.
        shard_mapper (ShardMapper): Maps docs to shards if they don't fit into the model context.
        shard_reducer (ShardReducer[Self]): Reduces doc shards back into one doc instance.
        scorer (Scorer): Scorer function.
        """
        super().__init__(
            parse_responses=parse_responses,
            prompt_example_type=prompt_example_type,
            template=template,
            prompt_examples=prompt_examples,
            shard_mapper=shard_mapper,
            shard_reducer=shard_reducer,
        )
        self._scorer = scorer
        self._candidate_selector: Optional[CandidateSelector] = None

        # Exclude mentions without candidates from prompt, if set. Mostly used for internal debugging.
        self._auto_nil = True
        # Store, per doc and entity, whether candidates could be found and candidates themselves.
        self._has_ent_cands_by_doc: List[List[bool]] = []
        self._ents_cands_by_doc: List[List[List[Entity]]] = []
        self._has_ent_cands_by_shard: List[List[List[bool]]] = []
        self._ents_cands_by_shard: List[List[List[List[Entity]]]] = []
        self._n_shards: Optional[int] = None

    def initialize(
        self,
        get_examples: Callable[[], Iterable["Example"]],
        nlp: Language,
        candidate_selector: Optional[CandidateSelector] = None,
        n_prompt_examples: int = 0,
    ) -> None:
        """Initialize entity linking task.
        get_examples (Callable[[], Iterable["Example"]]): Callable that provides examples
            for initialization.
        nlp (Language): Language instance.
        candidate_selector (Optional[CandidateSelector]): Factory for a candidate selection callable returning
            candidates for a given Span and context. If candidate selector hasn't been set explicitly before with
            .set_candidate_selector(), it has to be provided here - otherwise an error will be raised.
        n_prompt_examples (int): How many prompt examples to infer from the provided Example objects.
            0 by default. Takes all examples if set to -1.

        """
        super()._initialize(
            get_examples=get_examples,
            nlp=nlp,
            n_prompt_examples=n_prompt_examples,
            fetch_entity_info=self.fetch_entity_info,
        )
        if candidate_selector:
            self.set_candidate_selector(candidate_selector, nlp.vocab)
        elif self._candidate_selector is None:
            raise ValueError(
                "candidate_selector has to be provided when initializing the LLM component with the "
                "entity_linking task."
            )

    def set_candidate_selector(
        self, candidate_selector: CandidateSelector, vocab: Vocab
    ) -> None:
        """Sets candidate selector instance."""
        self._candidate_selector = candidate_selector
        if isinstance(self._candidate_selector, InitializableCandidateSelector):
            self._candidate_selector.initialize(vocab)

    def _preprocess_docs_for_prompt(self, docs: Iterable[Doc]) -> Iterable[Doc]:
        (
            self._ents_cands_by_doc,
            self._has_ent_cands_by_doc,
        ) = self._find_entity_candidates(docs)
        # Reset shard-wise candidate info. Will be set for each shard individually in _get_prompt_data(). We cannot
        # update it here, as we don't know yet how the shards will look like.
        self._ents_cands_by_shard = [[] * len(self._ents_cands_by_doc)]
        self._has_ent_cands_by_shard = [[] * len(self._ents_cands_by_doc)]
        self._n_shards = None
        return [
            EntityLinkerTask.highlight_ents_in_doc(doc, self._has_ent_cands_by_doc[i])
            for i, doc in enumerate(docs)
        ]

    def _find_entity_candidates(
        self, docs: Iterable[Doc]
    ) -> Tuple[List[List[List[Entity]]], List[List[bool]]]:
        """Determine entity candidates for all entity mentions in docs.
        docs (Iterable[Doc]): Docs with entities to select candidates for.
        RETURNS (Tuple[List[List[List[Entity]]], List[List[bool]]]): (1) list of candidate entities for each doc and
            entity, (2) list of flag whether candidates could be found per each doc and entitiy.
        """
        ents_cands: List[List[List[Entity]]] = []
        has_cands: List[List[bool]] = []

        for doc in docs:
            ents_cands.append(self.fetch_entity_info(doc)[0])
            # Determine which ents have candidates and should be included in prompt.
            has_cands.append(
                [
                    {cand_ent.id for cand_ent in cand_ents} != {EntityLinker.NIL}
                    or not self._auto_nil
                    for cand_ents in ents_cands[-1]
                ]
            )

        return ents_cands, has_cands

    def _get_prompt_data(
        self, shard: Doc, i_shard: int, i_doc: int, n_shards: int
    ) -> Dict[str, Any]:
        # n_shards changes before reset happens in _preprocess_docs() whenever sharding mechanism varies number of
        # shards. In this case we have to reset task state as well.
        if n_shards != self._n_shards:
            self._n_shards = n_shards
            self._ents_cands_by_shard = [[] * len(self._ents_cands_by_doc)]
            self._has_ent_cands_by_shard = [[] * len(self._ents_cands_by_doc)]

        # It's not ideal that we have to run candidate selection again here - but due to (1) us wanting to know whether
        # all entities have candidates before sharding and, more importantly, (2) some entities maybe being split up in
        # the sharding process it's cleaner to look for candidates again.
        if n_shards == 1:
            # If only one shard: shard is identical to original doc, so we don't have to rerun candidate search.
            ents_cands, has_cands = (
                self._ents_cands_by_doc[i_doc],
                self._has_ent_cands_by_doc[i_doc],
            )
        else:
            cands_info = self._find_entity_candidates([shard])
            ents_cands, has_cands = cands_info[0][0], cands_info[1][0]

        # Update shard-wise candidate info so it can be reused during parsing.
        if len(self._ents_cands_by_shard[i_doc]) == 0:
            self._ents_cands_by_shard[i_doc] = [[] for _ in range(n_shards)]
            self._has_ent_cands_by_shard[i_doc] = [[] for _ in range(n_shards)]
        self._ents_cands_by_shard[i_doc][i_shard] = ents_cands
        self._has_ent_cands_by_shard[i_doc][i_shard] = has_cands

        return {
            "mentions_str": ", ".join(
                [
                    f"*{mention.text}*"
                    for hc, mention in zip(has_cands, shard.ents)
                    if hc
                ]
            ),
            "mentions": [ent.text for hc, ent in zip(has_cands, shard.ents) if hc],
            "entity_descriptions": [
                [ent.description for ent in ents]
                for hc, ents in zip(has_cands, ents_cands)
                if hc
            ],
            "entity_ids": [
                [ent.id for ent in ents]
                for hc, ents in zip(has_cands, ents_cands)
                if hc
            ],
        }

    def parse_responses(
        self, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[str]]
    ) -> Iterable[Doc]:
        shards_teed = self._tee_2d_iterable(shards, 2)
        parsed_responses = self._parse_responses(self, shards_teed[1], responses)

        for i_doc, (shards_for_doc, ent_spans_for_doc) in enumerate(
            zip(shards_teed[0], parsed_responses)
        ):
            updated_shards_for_doc: List[Doc] = []
            for i_shard, (shard, ent_spans) in enumerate(
                zip(shards_for_doc, ent_spans_for_doc)
            ):
                gen_nil_span: Callable[[Span], Span] = lambda ent: Span(  # noqa: E731
                    doc=shard,
                    start=ent.start,
                    end=ent.end,
                    label=ent.label,
                    vector=ent.vector,
                    vector_norm=ent.vector_norm,
                    kb_id=EntityLinker.NIL,
                )

                # If numbers of ents parsed from LLM response + ents without candidates and number of ents in doc don't
                # align, skip doc (most likely LLM parsing failed, no guarantee KB IDs can be assigned to correct ents).
                # This can happen when the LLM fails to list solutions for all entities.
                all_entities_resolved = len(ent_spans) + sum(
                    [
                        not is_in_prompt
                        for is_in_prompt in self._has_ent_cands_by_shard[i_doc][i_shard]
                    ]
                ) == len(shard.ents)

                # Fuse entities with (i. e. inferred by the LLM) and without candidates (i. e. auto-niled).
                # If entity was not included in prompt, as there were no candidates - fill in NIL for this entity.
                # If numbers of inferred and auto-niled entities don't line up with total number of entities, there is
                # no guaranteed way to assign a partially resolved list of entities
                # correctly.
                # Else: entity had candidates and was included in prompt - fill in resolved KB ID.
                ent_spans_iter = iter(ent_spans)
                shard.ents = [
                    gen_nil_span(ent)
                    if not (
                        all_entities_resolved
                        and self._has_ent_cands_by_shard[i_doc][i_shard][i_ent]
                    )
                    else next(ent_spans_iter)
                    for i_ent, ent in enumerate(shard.ents)
                ]

                # Remove entity highlights in shards.
                updated_shards_for_doc.append(
                    EntityLinkerTask.unhighlight_ents_in_doc(shard)
                )

            yield self._shard_reducer(self, updated_shards_for_doc)  # type: ignore[arg-type]

    def scorer(self, examples: Iterable[Example]) -> Dict[str, Any]:
        return self._scorer(examples)

    @property
    def _cfg_keys(self) -> List[str]:
        return ["_template"]

    @staticmethod
    def highlight_ents_in_doc(
        doc: Doc, include_ents: Optional[List[bool]] = None
    ) -> Doc:
        """Highlights entities in doc by wrapping them in **.
        doc (Doc): Doc whose entities are to be highlighted.
        include_ents (Optional[List[bool]]): Whether to include entities with the corresponding indices. If None, all
            are included.
        RETURNS (Doc): Doc with highlighted entities.
        """
        if include_ents is not None and len(include_ents) != len(doc.ents):
            raise ValueError(
                f"`include_ents` has {len(include_ents)} entries, but {len(doc.ents)} are required."
            )

        ents_to_highlight_idx = [
            i
            for i, ent in enumerate(doc.ents)
            if (include_ents is None or include_ents[i])
        ]
        ents_idx = [(ent.start, ent.end) for ent in doc.ents]

        # Include *-marker as tokens. Update entity indices.
        i_ent = 0
        new_ent_idx: List[Tuple[int, int]] = []
        token_texts: List[str] = []
        spaces: List[bool] = []
        to_highlight = i_ent in ents_to_highlight_idx
        offset = 0

        for token in doc:
            if i_ent < len(ents_idx) and token.i == ents_idx[i_ent][1]:
                if to_highlight:
                    token_texts.append("*")
                    spaces.append(spaces[-1])
                    spaces[-2] = False
                    offset += 1
                i_ent += 1
                to_highlight = i_ent in ents_to_highlight_idx
            if i_ent < len(ents_idx) and token.i == ents_idx[i_ent][0]:
                if to_highlight:
                    token_texts.append("*")
                    spaces.append(False)
                    offset += 1
                new_ent_idx.append(
                    (ents_idx[i_ent][0] + offset, ents_idx[i_ent][1] + offset)
                )
            token_texts.append(token.text)
            spaces.append(token.whitespace_ != "")

        # Cover edge case of doc ending with entity, in which case we need to close the * wrapping.
        if len(ents_to_highlight_idx) and doc.ents[
            ents_to_highlight_idx[-1]
        ].end == len(doc):
            token_texts.append("*")
            spaces.append(False)

        # Create doc with new tokens and entities.
        highlighted_doc = Doc(doc.vocab, words=token_texts, spaces=spaces)
        highlighted_doc.ents = [
            Span(
                doc=highlighted_doc,
                start=new_ent_idx[i][0],
                end=new_ent_idx[i][1],
                label=ent.label,
                vector=ent.vector,
                vector_norm=ent.vector_norm,
                kb_id=ent.kb_id_,
            )
            for i, ent in enumerate(doc.ents)
        ]

        return highlighted_doc

    @staticmethod
    def unhighlight_ents_in_doc(doc: Doc) -> Doc:
        """Remove entity highlighting (* wrapping)  in doc.
        doc (Doc): Doc whose entities are to be highlighted.
        RETURNS (Doc): Doc with highlighted entities.
        """
        highlight_start_idx = {
            ent.start - 1
            for ent in doc.ents
            if ent.start - 1 > 0 and doc[ent.start - 1].text == "*"
        }
        highlight_end_idx = {
            ent.end
            for ent in doc.ents
            if ent.end < len(doc) and doc[ent.end].text == "*"
        }
        highlight_idx = highlight_start_idx | highlight_end_idx

        # Compute entity indices with removed highlights.
        ent_idx: List[Tuple[int, int]] = []
        offset = 0
        for ent in doc.ents:
            is_highlighted = ent.start - 1 in highlight_start_idx
            ent_idx.append(
                (ent.start + offset - is_highlighted, ent.end + offset - is_highlighted)
            )
            offset -= 2 * is_highlighted

        # Create doc with new tokens and entities.
        tokens = [
            token
            for token in doc
            if not (token.i in highlight_idx and token.text == "*")
        ]
        unhighlighted_doc = Doc(
            doc.vocab,
            words=[token.text for token in tokens],
            # Use original token space, if token doesn't appear after * highlight. If so, insert space unconditionally.
            spaces=[
                token.whitespace_ != "" or token.i + 1 in highlight_idx
                for i, token in enumerate(tokens)
            ],
        )

        unhighlighted_doc.ents = [
            Span(
                doc=unhighlighted_doc,
                start=ent_idx[i][0],
                end=ent_idx[i][1],
                label=ent.label,
                vector=ent.vector,
                vector_norm=ent.vector_norm,
                kb_id=ent.kb_id_,
            )
            for i, ent in enumerate(doc.ents)
        ]

        return unhighlighted_doc

    def _require_candidate_selector(self) -> None:
        """Raises an error if candidate selector is not available."""
        if not self._candidate_selector:
            raise ValueError(
                "Candidate selector hasn't been initialized. Pass the corresponding config to "
                "[initialize.components.COMPONENT_NAME.candidate_selector]."
            )

    def fetch_entity_info(
        self, doc: Doc
    ) -> Tuple[List[List[Entity]], List[Optional[str]]]:
        """Fetches entity IDs & descriptions and determines solution numbers for entities in doc.
        doc (Doc): Doc to fetch entity descriptions and solution numbers for. If entities' KB IDs are not set,
            corresponding solution number will be None.
        Tuple[List[List[Entity]], List[Optional[str]]]: For each mention in doc: list of entity candidates,
            list of correct entity IDs.
        """
        self._require_candidate_selector()
        assert self._candidate_selector

        cands_per_ent: Iterable[Iterable[Entity]] = self._candidate_selector(doc.ents)
        cand_entity_info: List[List[Entity]] = []
        correct_ent_ids: List[Optional[str]] = []

        for ent, cands in zip(doc.ents, cands_per_ent):
            cands_for_ent: List[Entity] = list(cands)

            # No KB ID known: In this case there is no guarantee that the correct entity description will be included.
            if ent.kb_id == 0:
                correct_ent_ids.append(None)
            # Correct entity not in suggested candidates: fetch description explicitly.
            elif ent.kb_id not in {cand.id for cand in cands_for_ent}:
                cands_for_ent.append(
                    Entity(
                        id=ent.kb_id_,
                        description=self._candidate_selector.get_entity_description(
                            ent.kb_id_
                        ),
                    )
                )
                correct_ent_ids.append(ent.kb_id_)

            cand_entity_info.append(cands_for_ent)

        return cand_entity_info, correct_ent_ids

    @property
    def has_ent_cands_by_shard(self) -> List[List[List[bool]]]:
        """Returns flags indicating whether shards' entities' have candidates in KB.
        RETURNS (List[List[List[bool]]]): Flags indicating whether shards' entities' have candidates in KB.
        """
        return self._has_ent_cands_by_shard



================================================
FILE: spacy_llm/tasks/entity_linker/ty.py
================================================
from pathlib import Path
from typing import Callable, Dict, Iterable, Tuple, Union

from pydantic import BaseModel
from spacy import Vocab
from spacy.kb import InMemoryLookupKB
from spacy.tokens import Span

from ...compat import Protocol, runtime_checkable


class Entity(BaseModel):
    """Represents one entity."""

    id: str
    description: str


@runtime_checkable
class CandidateSelector(Protocol):
    def __call__(self, mentions: Iterable[Span]) -> Iterable[Iterable[Entity]]:
        """Return list of Candidates with their descriptions for given mention and context.
        mentions (Iterable[Span]): Entity mentions.
        RETURNS (Iterable[Iterable[Entity]]): Top n entity candidates per mention.
        """

    def get_entity_description(self, entity_id: str) -> str:
        """Returns entity description for entity ID. If none found, a warning is emitted and
        spacy_llm.tasks.entity_linker.util.UNAVAILABLE_ENTITY_DES is returned.
        entity_id (str): Entity whose ID should be looked up.
        RETURNS (str): Entity description for entity with specfied ID. If no description found, returned string equals
            spacy_llm.tasks.entity_linker.util.UNAVAILABLE_ENTITY_DESC.
        """


@runtime_checkable
class InitializableCandidateSelector(Protocol):
    def initialize(self, vocab: Vocab):
        """Initialize instance with vocabulary.
        vocab (Vocab): Vocabulary.
        """
        ...


DescFormat = Dict[str, str]
EntDescReader = Callable[[Union[Path, str]], DescFormat]
InMemoryLookupKBLoader = Callable[[Vocab], Tuple[InMemoryLookupKB, DescFormat]]



================================================
FILE: spacy_llm/tasks/entity_linker/util.py
================================================
import abc
import csv
import dataclasses
import warnings
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple, Union

import spacy
import srsly
from spacy import Vocab
from spacy.kb import InMemoryLookupKB
from spacy.pipeline import EntityLinker
from spacy.scorer import Scorer
from spacy.tokens import Doc
from spacy.training import Example

from ...compat import Self
from ...ty import FewshotExample
from .task import EntityLinkerTask
from .ty import DescFormat, EntDescReader

UNAVAILABLE_ENTITY_DESC: str = "This entity doesn't have a description."


class ELExample(FewshotExample):
    text: str
    mentions: List[str]
    entity_descriptions: List[List[str]]
    entity_ids: List[List[str]]
    solutions: List[str]
    reasons: Optional[List[str]]

    @property
    def mentions_str(self) -> str:
        """Returns stringified version of all mentions.
        RETURNS (str): Stringified version of all mentions.
        """
        return ", ".join([f"*{mention}*" for mention in self.mentions])

    @classmethod
    def generate(cls, example: Example, task: EntityLinkerTask) -> Optional[Self]:
        # Check whether all entities have their knowledge base IDs set.
        n_ents = len(example.reference.ents)
        n_set_kb_ids = sum([ent.kb_id != 0 for ent in example.reference.ents])
        if n_ents and n_ents != n_set_kb_ids:
            warnings.warn(
                f"Not all entities in this document have their knowledge base IDs set ({n_set_kb_ids} out of "
                f"{n_ents}). Ignoring {n_set_kb_ids - n_ents} entities in example:\n{example.reference}"
            )
        example.reference.ents = [
            ent for ent in example.reference.ents if ent.kb_id != 0
        ]
        if len(example.reference.ents) == 0:
            return None

        # Assemble example.
        mentions = [ent.text for ent in example.reference.ents]
        # Fetch candidates. If true entity not among candidates: fetch description separately and add manually.
        cands_ents, solutions = task.fetch_entity_info(example.reference)
        # If we are to use available docs as examples, they have to have KB IDs set and hence available solutions.
        assert all([sol is not None for sol in solutions])

        return ELExample(
            text=EntityLinkerTask.highlight_ents_in_doc(example.reference).text,
            mentions=mentions,
            entity_descriptions=[
                [ent.description for ent in ents] for ents in cands_ents
            ],
            entity_ids=[[ent.id for ent in ents] for ents in cands_ents],
            solutions=solutions,
            reasons=[""] * len(mentions),
        )


def score(examples: Iterable[Example], **kwargs) -> Dict[str, Any]:
    """Score entity linking accuracy in examples.
    examples (Iterable[Example]): Examples to score.
    RETURNS (Dict[str, Any]): Dict with metric name -> score.
    """
    return Scorer.score_links(examples, negative_labels=[EntityLinker.NIL])


def ent_desc_reader_csv(path: Union[Path, str]) -> Dict[str, str]:
    """Instantiates entity description reader with two columns: ID and description.
    path (Union[Path, str]): File path.
    RETURNS (Dict[str, str]): Dict with ID -> description.
    """
    with open(path) as csvfile:
        descs: Dict[str, str] = {}
        for row in csv.reader(csvfile, quoting=csv.QUOTE_ALL, delimiter=";"):
            if len(row) != 2:
                continue
            descs[row[0]] = row[1]

        if len(descs) == 0:
            raise ValueError(
                "Format of CSV file with entity descriptions is wrong. CSV has to be formatted as "
                "semicolon-delimited CSV with two columns. The first columns has to contain the entity"
                " ID, the second the entity description."
            )

    return descs


@dataclasses.dataclass
class BaseInMemoryLookupKBLoader:
    path: Union[str, Path]
    """Path to artefact containing knowledge base data."""

    def __post_init__(self):
        if isinstance(self.path, str):
            self.path = Path(self.path)

    @abc.abstractmethod
    def __call__(self, vocab: Vocab) -> Tuple[InMemoryLookupKB, DescFormat]:
        """Loads KB instance.
        vocab (Vocab): Vocab instance of executing pipeline.
        RETURNS (Tuple[InMemoryLookupKB, DescFormat]): Loaded/generated KB instance; descriptions for entities.
        """
        ...


@dataclasses.dataclass
class KBObjectLoader(BaseInMemoryLookupKBLoader):
    """Config/init helper class for loading InMemoryLookupKB instance from a serialized KB directory."""

    # Path to serialized NLP pipeline. If None, path will be guessed.
    nlp_path: Optional[Union[str, Path]]
    # Path to .csv file with descriptions for entities. Has to have two columns with the first one being the entity ID,
    # the second one being the description. The entity ID has to match with the entity ID in the stored knowledge base.
    # If not specified, all entity descriptions provided in prompts will be a generic "No description available" or
    # something else to this effect.
    desc_path: Optional[Union[Path, str]]
    # Entity description file reader.
    ent_desc_reader: EntDescReader

    def __post_init__(self):
        super().__post_init__()
        if self.nlp_path and isinstance(self.nlp_path, str):
            self.nlp_path = Path(self.nlp_path)

    def __call__(self, vocab: Vocab) -> Tuple[InMemoryLookupKB, DescFormat]:
        assert isinstance(self.path, Path)

        # Load pipeline, use its vocab. If pipeline path isn't set, try loading the surrounding pipeline
        # (which might fail).
        nlp_path = self.nlp_path or self.path.parent.parent
        try:
            nlp = spacy.load(nlp_path)
        except IOError as err:
            raise ValueError(
                f"Pipeline at path {nlp_path} could not be loaded. Make sure to specify the correct path."
            ) from err
        kb = InMemoryLookupKB(nlp.vocab, entity_vector_length=1)
        kb.from_disk(self.path)

        return kb, self.ent_desc_reader(self.desc_path) if self.desc_path else {}


@dataclasses.dataclass
class KBFileLoader(BaseInMemoryLookupKBLoader):
    """Config/init helper class for generating an InMemoryLookupKB instance from a file.
    Currently supports only .yaml files."""

    def __call__(self, vocab: Vocab) -> Tuple[InMemoryLookupKB, DescFormat]:
        assert isinstance(self.path, Path)

        kb_data = srsly.read_yaml(self.path)
        entities = kb_data["entities"]
        qids = list(entities.keys())
        kb = InMemoryLookupKB(
            vocab=vocab,
            entity_vector_length=len(
                kb_data["entities"][qids[0]].get("embedding", [0])
            ),
        )

        # Set entities (with dummy values for frequencies).
        kb.set_entities(
            entity_list=qids,
            # Use [0] as default embedding if no embeddings are specified.
            vector_list=[entities[qid].get("embedding", [0]) for qid in qids],
            freq_list=[1] * len(qids),
        )

        # Add aliases and prior probabilities.
        for alias_data in kb_data["aliases"]:
            try:
                kb.add_alias(**alias_data)
            except ValueError as err:
                if "E134" in str(err):
                    raise ValueError(
                        f"Parsing of YAML file for knowledge base creation failed due to entity in "
                        f"`aliases` section not declared in `entities` section: {alias_data}. Double-"
                        f"check your .yaml file is correct."
                    ) from err
                raise err

        return kb, {qid: entities[qid].get("desc") for qid in qids}


def reduce_shards_to_doc(task: EntityLinkerTask, shards: Iterable[Doc]) -> Doc:
    """Reduces shards to docs for EntityLinkerTask.
    task (EntityLinkerTask): Task.
    shards (Iterable[Doc]): Shards to reduce to single doc instance.
    RETURNS (Doc): Fused doc instance.
    """
    # Entities are additive, so we can just merge shards.
    return Doc.from_docs(list(shards), ensure_whitespace=True)



================================================
FILE: spacy_llm/tasks/lemma/__init__.py
================================================
from .registry import make_lemma_task
from .task import LemmaTask
from .util import LemmaExample

__all__ = ["make_lemma_task", "LemmaExample", "LemmaTask"]



================================================
FILE: spacy_llm/tasks/lemma/parser.py
================================================
from typing import Iterable, List

from spacy.tokens import Doc

from .task import LemmaTask


def parse_responses_v1(
    task: LemmaTask, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[str]]
) -> Iterable[List[List[List[str]]]]:
    """Parses LLM responses for spacy.Lemma.v1.
    task (LemmaTask): Task instance.
    shards (Iterable[Iterable[Doc]]): Doc shards.
    responses (Iterable[Iterable[str]]): LLM responses.
    RETURNS (Iterable[List[List[List[str]]]]): Lists of 2-lists per token (token: lemmatized token) and shard/response
        and doc.
    """
    for responses_for_doc in responses:
        results_for_doc: List[List[List[str]]] = []
        for response in responses_for_doc:
            results_for_shard = [
                [pr_part.strip() for pr_part in pr.split(":")]
                for pr in response.replace("Lemmatized text:", "")
                .replace("'''", "")
                .strip()
                .split("\n")
            ]
            results_for_doc.append(
                # Malformed responses might have a length != 2, in which case they are discarded.
                [
                    result_for_token
                    for result_for_token in results_for_shard
                    if len(result_for_token) == 2
                ]
            )

        yield results_for_doc



================================================
FILE: spacy_llm/tasks/lemma/registry.py
================================================
from typing import Optional, Type

from ...registry import registry
from ...ty import ExamplesConfigType, FewshotExample, Scorer, ShardMapper, ShardReducer
from ...ty import TaskResponseParser
from ..util.sharding import make_shard_mapper
from .parser import parse_responses_v1
from .task import DEFAULT_LEMMA_TEMPLATE_V1, LemmaTask
from .util import LemmaExample, reduce_shards_to_doc, score


@registry.llm_misc("spacy.LemmaParser.v1")
def make_lemma_parser() -> TaskResponseParser[LemmaTask]:
    return parse_responses_v1


@registry.llm_misc("spacy.LemmaScorer.v1")
def make_lemma_scorer() -> Scorer:
    return score


@registry.llm_misc("spacy.LemmaShardReducer.v1")
def make_shard_reducer() -> ShardReducer:
    return reduce_shards_to_doc


@registry.llm_tasks("spacy.Lemma.v1")
def make_lemma_task(
    template: str = DEFAULT_LEMMA_TEMPLATE_V1,
    parse_responses: Optional[TaskResponseParser[LemmaTask]] = None,
    prompt_example_type: Optional[Type[FewshotExample]] = None,
    examples: ExamplesConfigType = None,
    shard_mapper: Optional[ShardMapper] = None,
    shard_reducer: Optional[ShardReducer] = None,
    scorer: Optional[Scorer] = None,
):
    """Lemma.v1 task factory.

    template (str): Prompt template passed to the model.
    parse_responses (Optional[TaskResponseParser]): Callable for parsing LLM responses for this task.
    prompt_example_type (Optional[Type[FewshotExample]]): Type to use for fewshot examples.
    examples (ExamplesConfigType): Optional callable that reads a file containing task examples for
        few-shot learning. If None is passed, then zero-shot learning will be used.
    shard_mapper (Optional[ShardMapper]): Maps docs to shards if they don't fit into the model context.
    shard_reducer (Optional[ShardReducer]): Reduces doc shards back into one doc instance.
    scorer (Optional[Scorer]): Scorer function.
    """
    raw_examples = examples() if callable(examples) else examples
    example_type = prompt_example_type or LemmaExample
    lemma_examples = (
        [example_type(**eg) for eg in raw_examples] if raw_examples else None
    )

    return LemmaTask(
        template=template,
        parse_responses=parse_responses or parse_responses_v1,
        prompt_example_type=example_type,
        prompt_examples=lemma_examples,
        shard_mapper=shard_mapper or make_shard_mapper(),
        shard_reducer=shard_reducer or make_shard_reducer(),
        scorer=scorer or score,
    )



================================================
FILE: spacy_llm/tasks/lemma/task.py
================================================
from typing import Any, Callable, Dict, Iterable, List, Optional, Type

from spacy import Language
from spacy.tokens import Doc
from spacy.training import Example

from ...compat import Self
from ...ty import FewshotExample, Scorer, ShardMapper, ShardReducer, TaskResponseParser
from ..builtin_task import BuiltinTask
from ..templates import read_template

DEFAULT_LEMMA_TEMPLATE_V1 = read_template("lemma.v1")


class LemmaTask(BuiltinTask):
    def __init__(
        self,
        parse_responses: TaskResponseParser[Self],
        prompt_example_type: Type[FewshotExample[Self]],
        prompt_examples: Optional[List[FewshotExample[Self]]],
        template: str,
        shard_mapper: ShardMapper,
        shard_reducer: ShardReducer[Self],
        scorer: Scorer,
    ):
        """Default lemmatization task.

        parse_responses (TaskResponseParser[Self]): Callable for parsing LLM responses for this task.
        prompt_example_type (Type[FewshotExample[Self]): Type to use for fewshot examples.
        prompt_examples (Optional[List[FewshotExample[Self]]]): Optional list of few-shot examples to include in prompts.
        template (str): Prompt template passed to the model.
        shard_mapper (ShardMapper): Maps docs to shards if they don't fit into the model context.
        shard_reducer (ShardReducer[Self]): Reduces doc shards back into one doc instance.
        scorer (Scorer): Scorer function.
        """
        super().__init__(
            parse_responses=parse_responses,
            prompt_example_type=prompt_example_type,
            template=template,
            prompt_examples=prompt_examples,
            shard_mapper=shard_mapper,
            shard_reducer=shard_reducer,
        )
        self._scorer = scorer

    def parse_responses(
        self, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[str]]
    ) -> Iterable[Doc]:
        shards_teed = self._tee_2d_iterable(shards, 2)
        for shards_for_doc, lemmas_for_doc in zip(
            shards_teed[0], self._parse_responses(self, shards_teed[1], responses)
        ):
            updated_shards_for_doc: List[Doc] = []

            for shard, lemmas in zip(shards_for_doc, lemmas_for_doc):
                tokens = [token for token in shard]
                # If numbers of tokens recognized by spaCy and returned by LLM don't match, we don't attempt a partial
                # match.
                if len(tokens) != len(lemmas):
                    updated_shards_for_doc.append(shard)
                    continue

                # Assign lemmas.
                for token, lemma_info in zip(tokens, lemmas):
                    if len(lemma_info) > 0:
                        token.lemma_ = lemma_info[1]

                updated_shards_for_doc.append(shard)

            yield self._shard_reducer(self, updated_shards_for_doc)  # type: ignore[arg-type]

    def initialize(
        self,
        get_examples: Callable[[], Iterable["Example"]],
        nlp: Language,
        n_prompt_examples: int = 0,
    ) -> None:
        super()._initialize(
            get_examples=get_examples, nlp=nlp, n_prompt_examples=n_prompt_examples
        )

    def scorer(self, examples: Iterable[Example]) -> Dict[str, Any]:
        return self._scorer(examples)

    @property
    def _cfg_keys(self) -> List[str]:
        return ["_template"]



================================================
FILE: spacy_llm/tasks/lemma/util.py
================================================
import warnings
from typing import Any, Dict, Iterable, List, Optional

from spacy.scorer import Scorer
from spacy.tokens import Doc
from spacy.training import Example

from ...compat import Self
from ...ty import FewshotExample
from .task import LemmaTask


class LemmaExample(FewshotExample[LemmaTask]):
    text: str
    lemmas: List[Dict[str, str]]

    @classmethod
    def generate(cls, example: Example, task: LemmaTask) -> Optional[Self]:
        lemma_dict = [{t.text: t.lemma_} for t in example.reference]
        return cls(text=example.reference.text, lemmas=lemma_dict)


def score(examples: Iterable[Example], **kwargs) -> Dict[str, Any]:
    """Score lemmatization accuracy in examples.
    examples (Iterable[Example]): Examples to score.
    RETURNS (Dict[str, Any]): Dict with metric name -> score.
    """
    return Scorer.score_token_attr(examples, "lemma")


def reduce_shards_to_doc(task: LemmaTask, shards: Iterable[Doc]) -> Doc:
    """Reduces shards to docs for LemmaTask.
    task (LemmaTask): Task.
    shards (Iterable[Doc]): Shards to reduce to single doc instance.
    RETURNS (Doc): Fused doc instance.
    """
    # Lemmas are token-specific, so we can just merge shards.
    with warnings.catch_warnings():
        warnings.filterwarnings(
            "ignore",
            category=UserWarning,
            message=".*Skipping .* while merging docs.",
        )
        return Doc.from_docs(list(shards), ensure_whitespace=True)



================================================
FILE: spacy_llm/tasks/ner/__init__.py
================================================
from .registry import make_ner_task, make_ner_task_v2, make_ner_task_v3
from .task import NERTask
from .util import NERExample

__all__ = [
    "make_ner_task",
    "make_ner_task_v2",
    "make_ner_task_v3",
    "NERExample",
    "NERTask",
]



================================================
FILE: spacy_llm/tasks/ner/registry.py
================================================
from typing import Any, Callable, Dict, Iterable, List, Optional, Type, Union

from ...compat import Literal
from ...registry import registry
from ...ty import ExamplesConfigType, FewshotExample, Scorer, ShardMapper, ShardReducer
from ...ty import TaskResponseParser
from ...util import split_labels
from ..span import parse_responses as parse_span_responses
from ..span import parse_responses_cot as parse_span_responses_cot
from ..span.util import check_label_consistency, check_label_consistency_cot
from ..util.sharding import make_shard_mapper
from .task import DEFAULT_NER_TEMPLATE_V1, DEFAULT_NER_TEMPLATE_V2
from .task import DEFAULT_NER_TEMPLATE_V3, NERTask, SpanTask
from .util import NERCoTExample, NERExample, reduce_shards_to_doc, score


@registry.llm_misc("spacy.NERShardReducer.v1")
def make_shard_reducer() -> ShardReducer:
    return reduce_shards_to_doc


@registry.llm_tasks("spacy.NER.v1")
def make_ner_task(
    parse_responses: Optional[TaskResponseParser[SpanTask]] = None,
    prompt_example_type: Optional[Type[FewshotExample]] = None,
    labels: str = "",
    examples: Optional[Callable[[], Iterable[Any]]] = None,
    normalizer: Optional[Callable[[str], str]] = None,
    alignment_mode: Literal["strict", "contract", "expand"] = "contract",
    case_sensitive_matching: bool = False,
    single_match: bool = False,
    scorer: Optional[Scorer] = None,
):
    """NER.v1 task factory.

    parse_responses (Optional[TaskResponseParser[SpanTask]]): Callable for parsing LLM responses for this task.
    prompt_example_type (Optional[Type[FewshotExample]]): Type to use for fewshot examples.
    labels (str): Comma-separated list of labels to pass to the template.
        Leave empty to populate it at initialization time (only if examples are provided).
    examples (ExamplesConfigType): Optional callable that reads a file containing task examples for
        few-shot learning. If None is passed, then zero-shot learning will be used.
    normalizer (Optional[Callable[[str], str]]): optional normalizer function.
    alignment_mode (str): "strict", "contract" or "expand".
    case_sensitive_matching: Whether to search without case sensitivity.
    single_match (bool): If False, allow one substring to match multiple times in
        the text. If True, returns the first hit.
    scorer (Optional[Scorer]): Scorer function.
    """
    labels_list = split_labels(labels)
    example_type = prompt_example_type or NERExample
    span_examples = (
        [example_type(**eg) for eg in examples()] if callable(examples) else examples
    )

    return NERTask(
        parse_responses=parse_responses or parse_span_responses,
        prompt_example_type=example_type,
        labels=labels_list,
        template=DEFAULT_NER_TEMPLATE_V1,
        prompt_examples=span_examples,
        shard_mapper=make_shard_mapper(),
        shard_reducer=make_shard_reducer(),
        normalizer=normalizer,
        alignment_mode=alignment_mode,
        case_sensitive_matching=case_sensitive_matching,
        single_match=single_match,
        label_definitions=None,
        scorer=scorer or score,
        description=None,
        check_label_consistency=check_label_consistency,
    )


@registry.llm_tasks("spacy.NER.v2")
def make_ner_task_v2(
    parse_responses: Optional[TaskResponseParser[SpanTask]] = None,
    prompt_example_type: Optional[Type[FewshotExample]] = None,
    labels: Union[List[str], str] = [],
    template: str = DEFAULT_NER_TEMPLATE_V2,
    label_definitions: Optional[Dict[str, str]] = None,
    examples: ExamplesConfigType = None,
    normalizer: Optional[Callable[[str], str]] = None,
    alignment_mode: Literal["strict", "contract", "expand"] = "contract",
    case_sensitive_matching: bool = False,
    single_match: bool = False,
    scorer: Optional[Scorer] = None,
):
    """NER.v2 task factory.

    parse_responses (Optional[TaskResponseParser[SpanTask]]): Callable for parsing LLM responses for this task.
    prompt_example_type (Optional[Type[FewshotExample]]): Type to use for fewshot examples.
    labels (Union[str, List[str]]): List of labels to pass to the template,
        either an actual list or a comma-separated string.
        Leave empty to populate it at initialization time (only if examples are provided).
    template (str): Prompt template passed to the model.
    label_definitions (Optional[Dict[str, str]]): Map of label -> description
        of the label to help the language model output the entities wanted.
        It is usually easier to provide these definitions rather than
        full examples, although both can be provided.
    examples (ExamplesConfigType): Optional callable that reads a file containing task examples for
        few-shot learning. If None is passed, then zero-shot learning will be used.
    normalizer (Optional[Callable[[str], str]]): optional normalizer function.
    alignment_mode (str): "strict", "contract" or "expand".
    case_sensitive_matching (bool): Whether to search without case sensitivity.
    single_match (bool): If False, allow one substring to match multiple times in
        the text. If True, returns the first hit.
    scorer (Optional[Scorer]): Scorer function.
    """
    labels_list = split_labels(labels)
    raw_examples = examples() if callable(examples) else examples
    example_type = prompt_example_type or NERExample
    span_examples = (
        [example_type(**eg) for eg in raw_examples] if raw_examples else None
    )

    return NERTask(
        parse_responses=parse_responses or parse_span_responses,
        prompt_example_type=example_type,
        labels=labels_list,
        template=template,
        label_definitions=label_definitions,
        prompt_examples=span_examples,
        shard_mapper=make_shard_mapper(),
        shard_reducer=make_shard_reducer(),
        normalizer=normalizer,
        alignment_mode=alignment_mode,
        case_sensitive_matching=case_sensitive_matching,
        single_match=single_match,
        scorer=scorer or score,
        description=None,
        check_label_consistency=check_label_consistency,
    )


@registry.llm_tasks("spacy.NER.v3")
def make_ner_task_v3(
    parse_responses: Optional[TaskResponseParser[SpanTask]] = None,
    prompt_example_type: Optional[Type[FewshotExample]] = None,
    labels: Union[List[str], str] = [],
    template: str = DEFAULT_NER_TEMPLATE_V3,
    label_definitions: Optional[Dict[str, str]] = None,
    examples: ExamplesConfigType = None,
    shard_mapper: Optional[ShardMapper] = None,
    shard_reducer: Optional[ShardReducer] = None,
    normalizer: Optional[Callable[[str], str]] = None,
    alignment_mode: Literal["strict", "contract", "expand"] = "contract",
    case_sensitive_matching: bool = False,
    scorer: Optional[Scorer] = None,
    description: Optional[str] = None,
):
    """NER.v3 task factory, with chain-of-thought prompting.

    parse_responses (Optional[TaskResponseParser[SpanTask]]): Callable for parsing LLM responses for this task.
    prompt_example_type (Optional[Type[FewshotExample]]): Type to use for fewshot examples.
    labels (Union[str, List[str]]): List of labels to pass to the template,
        either an actual list or a comma-separated string.
        Leave empty to populate it at initialization time (only if examples are provided).
    template (str): Prompt template passed to the model.
    description (str): A description of what to recognize or not recognize as entities.
    label_definitions (Optional[Dict[str, str]]): Map of label -> description
        of the label to help the language model output the entities wanted.
        It is usually easier to provide these definitions rather than
        full examples, although both can be provided.
    examples (ExamplesConfigType): Optional callable that reads a file containing task examples for
        few-shot learning. If None is passed, then zero-shot learning will be used.
    shard_mapper (Optional[ShardMapper]): Maps docs to shards if they don't fit into the model context.
    shard_reducer (Optional[ShardReducer]): Reduces doc shards back into one doc instance.
    normalizer (Optional[Callable[[str], str]]): optional normalizer function.
    alignment_mode (str): "strict", "contract" or "expand".
    case_sensitive_matching (bool): Whether to search without case sensitivity.
    scorer (Optional[Scorer]): Scorer function.
    """
    labels_list = split_labels(labels)
    raw_examples = examples() if callable(examples) else examples
    example_type = prompt_example_type or NERCoTExample
    span_examples = (
        [example_type(**eg) for eg in raw_examples] if raw_examples else None
    )

    return NERTask(
        parse_responses=parse_responses or parse_span_responses_cot,
        prompt_example_type=example_type,
        labels=labels_list,
        template=template,
        label_definitions=label_definitions,
        prompt_examples=span_examples,
        shard_mapper=shard_mapper or make_shard_mapper(),
        shard_reducer=shard_reducer or make_shard_reducer(),
        normalizer=normalizer,
        alignment_mode=alignment_mode,
        case_sensitive_matching=case_sensitive_matching,
        single_match=False,
        scorer=scorer or score,
        description=description,
        check_label_consistency=check_label_consistency_cot,
    )



================================================
FILE: spacy_llm/tasks/ner/task.py
================================================
from typing import Any, Callable, Dict, Iterable, List, Optional, Type

from spacy.language import Language
from spacy.tokens import Doc, Span
from spacy.training import Example
from spacy.util import filter_spans

from ...compat import Literal, Self
from ...ty import FewshotExample, Scorer, ShardMapper, ShardReducer, TaskResponseParser
from ..span import SpanTask
from ..span.task import SpanTaskLabelCheck
from ..templates import read_template

DEFAULT_NER_TEMPLATE_V1 = read_template("ner.v1")
DEFAULT_NER_TEMPLATE_V2 = read_template("ner.v2")
DEFAULT_NER_TEMPLATE_V3 = read_template("ner.v3")


class NERTask(SpanTask):
    def __init__(
        self,
        labels: List[str],
        template: str,
        parse_responses: TaskResponseParser[Self],
        prompt_example_type: Type[FewshotExample[Self]],
        label_definitions: Optional[Dict[str, str]],
        prompt_examples: Optional[List[FewshotExample[Self]]],
        shard_mapper: ShardMapper,
        shard_reducer: ShardReducer[Self],
        normalizer: Optional[Callable[[str], str]],
        alignment_mode: Literal["strict", "contract", "expand"],
        case_sensitive_matching: bool,
        single_match: bool,
        scorer: Scorer,
        description: Optional[str],
        check_label_consistency: SpanTaskLabelCheck[Self],
    ):
        """Default NER task.

        labels (List[str]): List of labels to pass to the template.
            Leave empty to populate it at initialization time (only if examples are provided).
        template (str): Prompt template passed to the model.
        parse_responses (TaskResponseParser[SpanTask]): Callable for parsing LLM responses for this task.
        prompt_example_type (Type[FewshotExample[Self]): Type to use for fewshot examples.
        shard_mapper (ShardMapper): Maps docs to shards if they don't fit into the model context.
        shard_reducer (ShardReducer[Self]): Reduces doc shards back into one doc instance.
        label_definitions (Optional[Dict[str, str]]): Map of label -> description
            of the label to help the language model output the entities wanted.
            It is usually easier to provide these definitions rather than
            full examples, although both can be provided.
        prompt_examples (Optional[List[FewshotExample[Self]]]): Optional list of few-shot examples to include in prompts.
        normalizer (Optional[Callable[[str], str]]): optional normalizer function.
        alignment_mode (str): "strict", "contract" or "expand".
        case_sensitive_matching (bool): Whether to search without case sensitivity.
        single_match (bool): If False, allow one substring to match multiple times in
            the text. If True, returns the first hit.
        scorer (Scorer): Scorer function.
        description (str): A description of what to recognize or not recognize as entities.
        check_label_consistency (SpanTaskLabelCheck): Callable to check label consistency.
        """
        super().__init__(
            labels=labels,
            template=template,
            parse_responses=parse_responses,
            prompt_example_type=prompt_example_type,
            shard_mapper=shard_mapper,
            shard_reducer=shard_reducer,
            label_definitions=label_definitions,
            prompt_examples=prompt_examples,
            normalizer=normalizer,
            alignment_mode=alignment_mode,
            case_sensitive_matching=case_sensitive_matching,
            single_match=single_match,
            description=description,
            allow_overlap=False,
            check_label_consistency=check_label_consistency,
        )
        self._scorer = scorer

    def initialize(
        self,
        get_examples: Callable[[], Iterable["Example"]],
        nlp: Language,
        labels: List[str] = [],
        n_prompt_examples: int = 0,
    ) -> None:
        super()._initialize(
            get_examples=get_examples,
            nlp=nlp,
            labels=labels,
            n_prompt_examples=n_prompt_examples,
        )

    def assign_spans(
        self,
        doc: Doc,
        spans: List[Span],
    ) -> None:
        """Assign spans to the document."""
        doc.set_ents(filter_spans(spans))

    def scorer(self, examples: Iterable[Example]) -> Dict[str, Any]:
        return self._scorer(examples)

    def _extract_labels_from_example(self, example: Example) -> List[str]:
        return [ent.label_ for ent in example.reference.ents]



================================================
FILE: spacy_llm/tasks/ner/util.py
================================================
from collections import defaultdict
from typing import Any, Dict, Iterable, Optional

from spacy.scorer import get_ner_prf
from spacy.tokens import Doc
from spacy.training import Example

from ...compat import Self
from ..span import SpanExample
from ..span.examples import SpanCoTExample
from .task import NERTask


class NERExample(SpanExample[NERTask]):
    @classmethod
    def generate(cls, example: Example, task: NERTask) -> Optional[Self]:
        entities = defaultdict(list)
        for ent in example.reference.ents:
            entities[ent.label_].append(ent.text)

        return cls(text=example.reference.text, entities=entities)


class NERCoTExample(SpanCoTExample[NERTask]):
    @classmethod
    def generate(cls, example: Example, task: NERTask) -> Optional[Self]:
        return cls(
            text=example.reference.text,
            spans=SpanCoTExample._extract_span_reasons(example.reference.ents),
        )


def score(examples: Iterable[Example], **kwargs) -> Dict[str, Any]:
    """Score NER accuracy in examples.
    examples (Iterable[Example]): Examples to score.
    RETURNS (Dict[str, Any]): Dict with metric name -> score.
    """
    return get_ner_prf(examples)


def reduce_shards_to_doc(task: NERTask, shards: Iterable[Doc]) -> Doc:
    """Reduces shards to docs for NERTask.
    task (NERTask): Task.
    shards (Iterable[Doc]): Shards to reduce to single doc instance.
    RETURNS (Doc): Fused doc instance.
    """
    # NERTask only affects span-specific information, so we can just merge shards.
    return Doc.from_docs(list(shards), ensure_whitespace=True)



================================================
FILE: spacy_llm/tasks/raw/__init__.py
================================================
from .registry import make_raw_task
from .task import RawTask
from .util import RawExample

__all__ = ["make_raw_task", "RawExample", "RawTask"]



================================================
FILE: spacy_llm/tasks/raw/parser.py
================================================
from typing import Iterable, List

from spacy.tokens import Doc

from .task import RawTask


def parse_responses_v1(
    task: RawTask, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[str]]
) -> Iterable[List[str]]:
    """Parses LLM responses for spacy.Raw.v1. Note that no parsing happens here, as we don't know what the result is
        expected to look like.
    task (RawTask): Task instance.
    shards (Iterable[Iterable[Doc]]): Doc shards.
    responses (Iterable[Iterable[str]]): LLM responses.
    RETURNS (Iterable[List[str]]): Reply as string per shard and doc.
    """
    for responses_for_doc in responses:
        yield list(responses_for_doc)



================================================
FILE: spacy_llm/tasks/raw/registry.py
================================================
from typing import Optional, Type

from ...registry import registry
from ...ty import ExamplesConfigType, FewshotExample, ShardMapper, ShardReducer
from ...ty import TaskResponseParser
from ..util.sharding import make_shard_mapper
from .parser import parse_responses_v1
from .task import DEFAULT_RAW_TEMPLATE_V1, RawTask
from .util import RawExample, reduce_shards_to_doc


@registry.llm_misc("spacy.RawParser.v1")
def make_raw_parser() -> TaskResponseParser[RawTask]:
    return parse_responses_v1


@registry.llm_misc("spacy.RawShardReducer.v1")
def make_shard_reducer() -> ShardReducer:
    return reduce_shards_to_doc


@registry.llm_tasks("spacy.Raw.v1")
def make_raw_task(
    template: str = DEFAULT_RAW_TEMPLATE_V1,
    field: str = "llm_reply",
    parse_responses: Optional[TaskResponseParser[RawTask]] = None,
    prompt_example_type: Optional[Type[FewshotExample]] = None,
    examples: ExamplesConfigType = None,
    shard_mapper: Optional[ShardMapper] = None,
    shard_reducer: Optional[ShardReducer] = None,
):
    """Raw.v1 task factory.

    template (str): Prompt template passed to the model.
    field (str): Field to store replies in.
    parse_responses (Optional[TaskResponseParser]): Callable for parsing LLM responses for this task.
    prompt_example_type (Optional[Type[FewshotExample]]): Type to use for fewshot examples.
    examples (ExamplesConfigType): Optional callable that reads a file containing task examples for
        few-shot learning. If None is passed, then zero-shot learning will be used.
    shard_mapper (Optional[ShardMapper]): Maps docs to shards if they don't fit into the model context.
    shard_reducer (Optional[ShardReducer]): Reduces doc shards back into one doc instance.
    """
    raw_examples = examples() if callable(examples) else examples
    example_type = prompt_example_type or RawExample
    raw_examples = [example_type(**eg) for eg in raw_examples] if raw_examples else None

    return RawTask(
        template=template,
        field=field,
        parse_responses=parse_responses or parse_responses_v1,
        prompt_example_type=example_type,
        prompt_examples=raw_examples,
        shard_mapper=shard_mapper or make_shard_mapper(),
        shard_reducer=shard_reducer or make_shard_reducer(),
    )



================================================
FILE: spacy_llm/tasks/raw/task.py
================================================
from typing import Callable, Iterable, List, Optional, Type

from spacy import Language
from spacy.tokens import Doc
from spacy.training import Example

from ...compat import Self
from ...ty import FewshotExample, ShardMapper, ShardReducer, TaskResponseParser
from ..builtin_task import BuiltinTask
from ..templates import read_template

DEFAULT_RAW_TEMPLATE_V1 = read_template("raw.v1")


class RawTask(BuiltinTask):
    def __init__(
        self,
        parse_responses: TaskResponseParser[Self],
        prompt_example_type: Type[FewshotExample[Self]],
        prompt_examples: Optional[List[FewshotExample[Self]]],
        template: str,
        field: str,
        shard_mapper: ShardMapper,
        shard_reducer: ShardReducer[Self],
    ):
        """Raw task. Expects prompt template without instructions for LLM, i. e. docs have to provide instructions
            themselves.

        parse_responses (TaskResponseParser[Self]): Callable for parsing LLM responses for this task.
        prompt_example_type (Type[FewshotExample[Self]): Type to use for fewshot examples.
        prompt_examples (Optional[List[FewshotExample[Self]]]): Optional list of few-shot examples to include in prompts.
        template (str): Prompt template passed to the model.
        field (str): Field to store responses in.
        shard_mapper (ShardMapper): Maps docs to shards if they don't fit into the model context.
        shard_reducer (ShardReducer[Self]): Reduces doc shards back into one doc instance.
        """
        super().__init__(
            parse_responses=parse_responses,
            prompt_example_type=prompt_example_type,
            template=template,
            prompt_examples=prompt_examples,
            shard_mapper=shard_mapper,
            shard_reducer=shard_reducer,
        )
        self._field = field
        self._check_doc_extension()

    def parse_responses(
        self, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[str]]
    ) -> Iterable[Doc]:
        shards_teed = self._tee_2d_iterable(shards, 2)
        for shards_for_doc, responses_for_doc in zip(
            shards_teed[0], self._parse_responses(self, shards_teed[1], responses)
        ):
            updated_shards_for_doc: List[Doc] = []
            for shard, response in zip(shards_for_doc, responses_for_doc):
                setattr(shard._, self._field, response)
                updated_shards_for_doc.append(shard)

            yield self._shard_reducer(self, updated_shards_for_doc)  # type: ignore[arg-type]

    def initialize(
        self,
        get_examples: Callable[[], Iterable["Example"]],
        nlp: Language,
        n_prompt_examples: int = 0,
    ) -> None:
        super()._initialize(
            get_examples=get_examples, nlp=nlp, n_prompt_examples=n_prompt_examples
        )

    def _check_doc_extension(self):
        """Add extension if need be."""
        if not Doc.has_extension(self._field):
            Doc.set_extension(self._field, default=None)

    @property
    def _cfg_keys(self) -> List[str]:
        return ["_template"]

    @property
    def field(self) -> str:
        """Return field used to store replies in docs.
        RETURNS (str): Field used to store replies in docs.
        """
        return self._field



================================================
FILE: spacy_llm/tasks/raw/util.py
================================================
import warnings
from typing import Iterable, Optional

from spacy.tokens import Doc
from spacy.training import Example

from ...compat import Self
from ...ty import FewshotExample
from .task import RawTask


class RawExample(FewshotExample[RawTask]):
    text: str
    reply: str

    @classmethod
    def generate(cls, example: Example, task: RawTask) -> Optional[Self]:
        return cls(
            text=example.reference.text, reply=getattr(example.reference._, task.field)
        )


def reduce_shards_to_doc(task: RawTask, shards: Iterable[Doc]) -> Doc:
    """Reduces shards to docs for RawTask.
    task (RawTask): Task.
    shards (Iterable[Doc]): Shards to reduce to single doc instance.
    RETURNS (Doc): Fused doc instance.
    """
    shards = list(shards)

    with warnings.catch_warnings():
        warnings.filterwarnings(
            "ignore",
            category=UserWarning,
            message=".*Skipping .* while merging docs.",
        )
        doc = Doc.from_docs(shards, ensure_whitespace=True)
        setattr(
            doc._,
            task.field,
            " ".join([getattr(shard._, task.field) for shard in shards]),
        )

    return doc



================================================
FILE: spacy_llm/tasks/rel/__init__.py
================================================
from .registry import make_rel_task
from .task import DEFAULT_REL_TEMPLATE, RELTask
from .util import RelationItem, RELExample

__all__ = [
    "DEFAULT_REL_TEMPLATE",
    "make_rel_task",
    "RelationItem",
    "RELExample",
    "RELTask",
]



================================================
FILE: spacy_llm/tasks/rel/items.py
================================================
from ...compat import BaseModel, validator


class RelationItem(BaseModel):
    dep: int
    dest: int
    relation: str

    @validator("dep", "dest", pre=True)
    def clean_ent(cls, value):
        if isinstance(value, str):
            value = value.strip("ENT")
        return value


class EntityItem(BaseModel):
    start_char: int
    end_char: int
    label: str



================================================
FILE: spacy_llm/tasks/rel/parser.py
================================================
from typing import Iterable, List

from spacy.tokens import Doc
from wasabi import msg

from ...compat import ValidationError
from .task import RELTask
from .util import RelationItem


def parse_responses_v1(
    task: RELTask, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[str]]
) -> Iterable[Iterable[List[RelationItem]]]:
    """Parses LLM responses for spacy.REL.v1.
    task (RELTask): Task instance.
    docs (Iterable[Iterable[Doc]]): Doc shards.
    responses (Iterable[Iterable[str]]): LLM responses.
    RETURNS (Iterable[Iterable[List[RelationItem]]]): List of RelationItem instances per shard/response.
    """
    for responses_for_doc, shards_for_doc in zip(responses, shards):
        results_for_doc: List[List[RelationItem]] = []
        for response, shard in zip(responses_for_doc, shards_for_doc):
            relations: List[RelationItem] = []
            for line in response.strip().split("\n"):
                try:
                    rel_item = RelationItem.parse_raw(line)
                    if 0 <= rel_item.dep < len(shard.ents) and 0 <= rel_item.dest < len(
                        shard.ents
                    ):
                        relations.append(rel_item)
                except ValidationError:
                    msg.warn(
                        "Validation issue",
                        line,
                        show=task.verbose,
                    )

            results_for_doc.append(relations)

        yield results_for_doc



================================================
FILE: spacy_llm/tasks/rel/registry.py
================================================
from typing import Callable, Dict, List, Optional, Type, Union

from ...registry import registry
from ...ty import ExamplesConfigType, FewshotExample, ShardMapper, ShardReducer
from ...ty import TaskResponseParser
from ...util import split_labels
from ..util.sharding import make_shard_mapper
from .parser import parse_responses_v1
from .task import DEFAULT_REL_TEMPLATE, RELTask
from .util import RELExample, reduce_shards_to_doc


@registry.llm_misc("spacy.RELShardReducer.v1")
def make_shard_reducer() -> ShardReducer:
    return reduce_shards_to_doc


@registry.llm_tasks("spacy.REL.v1")
def make_rel_task(
    labels: Union[List[str], str] = [],
    template: str = DEFAULT_REL_TEMPLATE,
    parse_responses: Optional[TaskResponseParser[RELTask]] = None,
    prompt_example_type: Optional[Type[FewshotExample]] = None,
    label_definitions: Optional[Dict[str, str]] = None,
    examples: ExamplesConfigType = None,
    shard_mapper: Optional[ShardMapper] = None,
    shard_reducer: Optional[ShardReducer] = None,
    normalizer: Optional[Callable[[str], str]] = None,
    verbose: bool = False,
) -> "RELTask":
    """REL.v1 task factory.

    The REL task populates a `Doc._.rel` custom attribute.

    labels (List[str]): List of labels to pass to the template,
        either an actual list or a comma-separated string.
        Leave empty to populate it at initialization time (only if examples are provided).
    template (str): Prompt template passed to the model.
    parse_responses (Optional[TaskResponseParser[RELTask]]): Callable for parsing LLM responses for this task.
    prompt_example_type (Optional[Type[FewshotExample]]): Type to use for fewshot examples.
    label_definitions (Optional[Dict[str, str]]): Map of label -> description
        of the label to help the language model output the entities wanted.
        It is usually easier to provide these definitions rather than
        full examples, although both can be provided.
    examples (ExamplesConfigType): Optional callable that reads a file containing task examples for
        few-shot learning. If None is passed, then zero-shot learning will be used.
    shard_mapper (Optional[ShardMapper]): Maps docs to shards if they don't fit into the model context.
    shard_reducer (Optional[ShardReducer]): Reduces doc shards back into one doc instance.
    normalizer (Optional[Callable[[str], str]]): Optional normalizer function.
    verbose (bool): Controls the verbosity of the task.
    """
    labels_list = split_labels(labels)
    example_type = prompt_example_type or RELExample
    raw_examples = examples() if callable(examples) else examples
    rel_examples = [example_type(**eg) for eg in raw_examples] if raw_examples else None

    return RELTask(
        parse_responses=parse_responses or parse_responses_v1,
        prompt_example_type=example_type,
        labels=labels_list,
        template=template,
        label_definitions=label_definitions,
        prompt_examples=rel_examples,
        shard_mapper=shard_mapper or make_shard_mapper(),
        shard_reducer=shard_reducer or make_shard_reducer(),
        normalizer=normalizer,
        verbose=verbose,
    )



================================================
FILE: spacy_llm/tasks/rel/task.py
================================================
from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Type, Union

from spacy.language import Language
from spacy.tokens import Doc, Span
from spacy.training import Example

from ...compat import Self
from ...ty import FewshotExample, ShardMapper, ShardReducer, TaskResponseParser
from ..builtin_task import BuiltinTaskWithLabels
from ..templates import read_template
from .items import RelationItem

DEFAULT_REL_TEMPLATE: str = read_template("rel.v1")


class RELTask(BuiltinTaskWithLabels):
    def __init__(
        self,
        parse_responses: TaskResponseParser[Self],
        prompt_example_type: Type[FewshotExample[Self]],
        labels: List[str],
        template: str,
        label_definitions: Optional[Dict[str, str]],
        prompt_examples: Optional[List[FewshotExample[Self]]],
        shard_mapper: ShardMapper,
        shard_reducer: ShardReducer[Self],
        normalizer: Optional[Callable[[str], str]],
        verbose: bool,
    ):
        super().__init__(
            parse_responses=parse_responses,
            prompt_example_type=prompt_example_type,
            template=template,
            prompt_examples=prompt_examples,
            shard_mapper=shard_mapper,
            shard_reducer=shard_reducer,
            labels=labels,
            label_definitions=label_definitions,
            normalizer=normalizer,
        )
        """Default REL task. Populates a `Doc._.rel` custom attribute.

        parse_responses (TaskResponseParser[Self]): Callable for parsing LLM responses for this task.
        prompt_example_type (Type[FewshotExample[Self]): Type to use for fewshot examples.
        labels (List[str]): List of labels to pass to the template.
            Leave empty to populate it at initialization time (only if examples are provided).
        template (str): Prompt template passed to the model.
        label_definitions (Optional[Dict[str, str]]): Map of label -> description
            of the label to help the language model output the entities wanted.
            It is usually easier to provide these definitions rather than
            full examples, although both can be provided.
        prompt_examples (Optional[List[FewshotExample[Self]]]): Optional list of few-shot examples to include in
            prompts.
        shard_mapper (ShardMapper): Maps docs to shards if they don't fit into the model context.
        shard_reducer (ShardReducer[Self]): Reduces doc shards back into one doc instance.
        normalizer (Optional[Callable[[str], str]]): Optional normalizer function.
        verbose (bool): Controls the verbosity of the task.
        """
        self._verbose = verbose
        self._field = "rel"

    def _preprocess_docs_for_prompt(self, docs: Iterable[Doc]) -> Iterable[Doc]:
        return [RELTask._preannotate(doc, True) for doc in docs]

    def _get_prompt_data(
        self, shard: Doc, i_shard: int, i_doc: int, n_shards: int
    ) -> Dict[str, Any]:
        return {
            "labels": list(self._label_dict.values()),
            "label_definitions": self._label_definitions,
            "preannotate": RELTask._preannotate,
        }

    @staticmethod
    def _preannotate(
        doc: Union[Doc, FewshotExample], return_as_doc: bool = False
    ) -> Union[str, Doc]:
        """Creates a text version of the document with annotated entities.
        doc (Union[Doc, FewshotExample]): Doc to preannotate.
        return_as_doc (bool): Whether to return as doc (by default returned as text).
        """
        words: List[str] = [] if len(doc.ents) else [t.text for t in doc]
        spaces: List[bool] = [] if len(doc.ents) else [t.whitespace_ != "" for t in doc]
        ent_indices: List[Tuple[int, int]] = []

        # Convert RELExample into Doc for easier subsequent processing.
        # todo Solve import cycle so we can expect RELExample here.
        if not isinstance(doc, Doc):
            assert hasattr(doc, "to_doc") and callable(doc.to_doc)
            doc = doc.to_doc()

        if not hasattr(doc, "ents"):
            raise ValueError(
                "Prompt example type used in RELTask has to expose entities via an .ents attribute."
            )

        # Update token information for doc reconstruction.
        last_ent_end = -1
        for i, ent in enumerate(doc.ents):
            annotation = f"[ENT{i}:{ent.label_ if isinstance(doc, Doc) else ent.label}]"
            tokens_since_last_ent = [
                *[t for t in doc if last_ent_end <= t.i < ent.start],
                *[t for t in ent],
            ]
            words.extend([*[t.text for t in tokens_since_last_ent], annotation])
            spaces.extend([t.whitespace_ != "" for t in tokens_since_last_ent])

            # Adjust spaces w.r.t. added annotations, which should appear directly after entity.
            spaces.append(spaces[-1])
            spaces[-2] = False
            ent_indices.append((ent.start + i, ent.end + i))

            last_ent_end = ent.end

        # Include chars after last ent.
        if len(doc.ents):
            tokens_since_last_ent = [t for t in doc if last_ent_end <= t.i]
            words.extend([t.text for t in tokens_since_last_ent])
            spaces.extend([t.whitespace_ != "" for t in tokens_since_last_ent])

        # Reconstruct doc.
        annotated_doc = Doc(words=words, spaces=spaces, vocab=doc.vocab)
        annotated_doc.ents = [
            Span(  # noqa: E731
                doc=annotated_doc,
                start=ent_idx[0],
                end=ent_idx[1],
                label=doc.ents[i].label,
                vector=doc.ents[i].vector,
                vector_norm=doc.ents[i].vector_norm,
                kb_id=doc.ents[i].kb_id_,
            )
            for i, ent_idx in enumerate(ent_indices)
        ]

        return annotated_doc.text if not return_as_doc else annotated_doc

    def parse_responses(
        self, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[str]]
    ) -> Iterable[Doc]:
        self._check_extension(self._field)
        shards_teed = self._tee_2d_iterable(shards, 2)
        for shards_for_doc, rel_items_for_doc in zip(
            shards_teed[0], self._parse_responses(self, shards_teed[1], responses)
        ):
            shards_for_doc = list(shards_for_doc)
            for shard, rel_items in zip(shards_for_doc, rel_items_for_doc):
                shard._.rel = rel_items

            yield self._shard_reducer(self, shards_for_doc)  # type: ignore[arg-type]

    def initialize(
        self,
        get_examples: Callable[[], Iterable["Example"]],
        nlp: Language,
        labels: List[str] = [],
        n_prompt_examples: int = 0,
    ) -> None:
        self._check_extension(self._field)
        super()._initialize(
            get_examples=get_examples,
            nlp=nlp,
            labels=labels,
            n_prompt_examples=n_prompt_examples,
        )

    @property
    def _cfg_keys(self) -> List[str]:
        return [
            "_label_dict",
            "_template",
            "_label_definitions",
            "_verbose",
        ]

    def _extract_labels_from_example(self, example: Example) -> List[str]:
        rels: List[RelationItem] = example.reference._.rel
        return [rel.relation for rel in rels]

    @property
    def verbose(self) -> bool:
        return self._verbose

    @property
    def field(self) -> str:
        return self._field



================================================
FILE: spacy_llm/tasks/rel/util.py
================================================
import re
import warnings
from typing import Iterable, List, Optional, Tuple

from spacy import Vocab
from spacy.tokens import Doc, Span
from spacy.training import Example

from ...compat import Self
from ...ty import FewshotExample
from .items import EntityItem, RelationItem
from .task import RELTask


class RELExample(FewshotExample[RELTask]):
    text: str
    ents: List[EntityItem]
    relations: List[RelationItem]

    @classmethod
    def generate(cls, example: Example, task: RELTask) -> Optional[Self]:
        entities = [
            EntityItem(
                start_char=ent.start_char,
                end_char=ent.end_char,
                label=ent.label_,
            )
            for ent in example.reference.ents
        ]

        return cls(
            text=example.reference.text,
            ents=entities,
            relations=example.reference._.rel,
        )

    def to_doc(self) -> Doc:
        """Returns Doc representation of example instance. Note that relations are in user_data["rel"].
        field (str): Doc field to store relations in.
        RETURNS (Doc): Representation as doc.
        """
        punct_chars_pattern = r'[]!"$%&\'()*+,./:;=#@?[\\^_`{|}~-]+'
        text = re.sub(punct_chars_pattern, r" \g<0> ", self.text)
        doc_words = text.split()
        doc_spaces = [
            i < len(doc_words) - 1
            and not re.match(punct_chars_pattern, doc_words[i + 1])
            for i, word in enumerate(doc_words)
        ]
        doc = Doc(words=doc_words, spaces=doc_spaces, vocab=Vocab(strings=doc_words))

        # Set entities after finding correct indices.
        conv_ent_indices: List[Tuple[int, int]] = []
        if len(self.ents):
            ent_idx = 0
            for token in doc:
                if token.idx == self.ents[ent_idx].start_char:
                    conv_ent_indices.append((token.i, -1))
                if token.idx + len(token.text) == self.ents[ent_idx].end_char:
                    conv_ent_indices[-1] = (conv_ent_indices[-1][0], token.i + 1)
                    ent_idx += 1
                if ent_idx == len(self.ents):
                    break

        doc.ents = [
            Span(  # noqa: E731
                doc=doc,
                start=ent_idx[0],
                end=ent_idx[1],
                label=self.ents[i].label,
            )
            for i, ent_idx in enumerate(conv_ent_indices)
        ]
        doc.user_data["rel"] = self.relations

        return doc


def reduce_shards_to_doc(task: RELTask, shards: Iterable[Doc]) -> Doc:
    """Reduces shards to docs for RELTask.
    task (RELTask): Task.
    shards (Iterable[Doc]): Shards to reduce to single doc instance.
    RETURNS (Doc): Fused doc instance.
    """
    shards = list(shards)

    with warnings.catch_warnings():
        warnings.filterwarnings(
            "ignore",
            category=UserWarning,
            message=".*Skipping .* while merging docs.",
        )
        doc = Doc.from_docs(shards, ensure_whitespace=True)

    # REL information from shards can be simply appended.
    setattr(
        doc._,
        task.field,
        [rel_items for shard in shards for rel_items in getattr(shard._, task.field)],
    )

    return doc



================================================
FILE: spacy_llm/tasks/sentiment/__init__.py
================================================
from .registry import make_sentiment_task
from .task import SentimentTask
from .util import SentimentExample

__all__ = ["make_sentiment_task", "SentimentExample", "SentimentTask"]



================================================
FILE: spacy_llm/tasks/sentiment/parser.py
================================================
from typing import Iterable, List, Optional

from spacy.tokens import Doc

from .task import SentimentTask


def parse_responses_v1(
    task: SentimentTask,
    shards: Iterable[Iterable[Doc]],
    responses: Iterable[Iterable[str]],
) -> Iterable[Iterable[Optional[float]]]:
    """Parses LLM responses for spacy.Sentiment.v1.
    task (SentimentTask): Task instance.
    shards (Iterable[Iterable[Doc]]): Doc shards.
    responses (Iterable[Iterable[str]]): LLM responses.
    RETURNS (Iterable[Iterable[Optional[float]]]): Sentiment score per shard/response. None on parsing error.
    """
    for responses_for_doc in responses:
        results_for_doc: List[Optional[float]] = []
        for response in responses_for_doc:
            try:
                results_for_doc.append(
                    float("".join(response.replace("Answer:", "").strip().split()))
                )
            except ValueError:
                results_for_doc.append(None)

        yield results_for_doc



================================================
FILE: spacy_llm/tasks/sentiment/registry.py
================================================
from typing import Optional, Type

from ...registry import registry
from ...ty import ExamplesConfigType, FewshotExample, Scorer, ShardMapper, ShardReducer
from ...ty import TaskResponseParser
from ..util.sharding import make_shard_mapper
from .parser import parse_responses_v1
from .task import DEFAULT_SENTIMENT_TEMPLATE_V1, SentimentTask
from .util import SentimentExample, reduce_shards_to_doc, score


@registry.llm_misc("spacy.SentimentShardReducer.v1")
def make_shard_reducer() -> ShardReducer:
    return reduce_shards_to_doc


@registry.llm_tasks("spacy.Sentiment.v1")
def make_sentiment_task(
    template: str = DEFAULT_SENTIMENT_TEMPLATE_V1,
    parse_responses: Optional[TaskResponseParser[SentimentTask]] = None,
    prompt_example_type: Optional[Type[FewshotExample]] = None,
    examples: ExamplesConfigType = None,
    shard_mapper: Optional[ShardMapper] = None,
    shard_reducer: Optional[ShardReducer] = None,
    field: str = "sentiment",
    scorer: Optional[Scorer] = None,
):
    """Sentiment.v1 task factory.

    template (str): Prompt template passed to the model.
    parse_responses (Optional[TaskResponseParser[SentimentTask]]): Callable for parsing LLM responses for this
        task.
    prompt_example_type (Optional[Type[FewshotExample]]): Type to use for fewshot examples.
    examples (ExamplesConfigType): Optional callable that reads a file containing task examples for
        few-shot learning. If None is passed, then zero-shot learning will be used.
    shard_mapper (Optional[ShardMapper]): Maps docs to shards if they don't fit into the model context.
    shard_reducer (Optional[ShardReducer]): Reduces doc shards back into one doc instance.
    field (str): The name of the doc extension in which to store the summary.
    scorer (Optional[Scorer]): Scorer function.
    """
    raw_examples = examples() if callable(examples) else examples
    example_type = prompt_example_type or SentimentExample
    sentiment_examples = (
        [example_type(**eg) for eg in raw_examples] if raw_examples else None
    )

    return SentimentTask(
        template=template,
        parse_responses=parse_responses or parse_responses_v1,
        prompt_example_type=example_type,
        prompt_examples=sentiment_examples,
        shard_mapper=shard_mapper or make_shard_mapper(),
        shard_reducer=shard_reducer or make_shard_reducer(),
        field=field,
        scorer=scorer or score,
    )



================================================
FILE: spacy_llm/tasks/sentiment/task.py
================================================
from typing import Any, Callable, Dict, Iterable, List, Optional, Type

from spacy.language import Language
from spacy.tokens import Doc
from spacy.training import Example

from ...ty import FewshotExample, Scorer, Self, ShardMapper, ShardReducer
from ...ty import TaskResponseParser
from ..builtin_task import BuiltinTask
from ..templates import read_template

DEFAULT_SENTIMENT_TEMPLATE_V1 = read_template("sentiment.v1")


class SentimentTask(BuiltinTask):
    def __init__(
        self,
        template: str,
        parse_responses: TaskResponseParser[Self],
        prompt_example_type: Type[FewshotExample[Self]],
        field: str,
        prompt_examples: Optional[List[FewshotExample[Self]]],
        shard_mapper: ShardMapper,
        shard_reducer: ShardReducer[Self],
        scorer: Scorer,
    ):
        """Sentiment analysis task.

        template (str): Prompt template passed to the model.
        parse_responses (TaskResponseParser[Self]): Callable for parsing LLM responses for this task.
        prompt_example_type (Type[FewshotExample[Self]): Type to use for fewshot examples.
        field (str): The name of the doc extension in which to store the sentiment score.
        prompt_examples (Optional[List[FewshotExample[Self]]]): Optional list of few-shot examples to include in
            prompts.
        shard_mapper (ShardMapper): Maps docs to shards if they don't fit into the model context.
        shard_reducer (ShardReducer[Self]): Reduces doc shards back into one doc instance.
        """
        super().__init__(
            parse_responses=parse_responses,
            prompt_example_type=prompt_example_type,
            template=template,
            prompt_examples=prompt_examples,
            shard_mapper=shard_mapper,
            shard_reducer=shard_reducer,
        )
        self._field = field
        self._scorer = scorer
        self._check_doc_extension()

    def _check_doc_extension(self):
        """Add extension if need be."""
        if not Doc.has_extension(self._field):
            Doc.set_extension(self._field, default=None)

    def initialize(
        self,
        get_examples: Callable[[], Iterable["Example"]],
        nlp: Language,
        n_prompt_examples: int = 0,
    ) -> None:
        """Initialize sentiment task.
        get_examples (Callable[[], Iterable["Example"]]): Callable that provides examples
            for initialization.
        nlp (Language): Language instance.
        n_prompt_examples (int): How many prompt examples to infer from the provided Example objects.
            0 by default. Takes all examples if set to -1.
        """
        self._check_doc_extension()
        super()._initialize(
            get_examples=get_examples, nlp=nlp, n_prompt_examples=n_prompt_examples
        )

    def parse_responses(
        self, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[str]]
    ) -> Iterable[Doc]:
        self._check_doc_extension()
        shards_teed = self._tee_2d_iterable(shards, 2)

        for shards_for_doc, scores_for_doc in zip(
            shards_teed[0], self._parse_responses(self, shards_teed[1], responses)
        ):
            shards_for_doc = list(shards_for_doc)
            for shard, score in zip(shards_for_doc, scores_for_doc):
                try:
                    setattr(shard._, self._field, score)
                except ValueError:
                    setattr(shard._, self._field, None)

            yield self._shard_reducer(self, shards_for_doc)  # type: ignore[arg-type]

    def scorer(self, examples: Iterable[Example]) -> Dict[str, Any]:
        return self._scorer(examples, field=self._field)

    @property
    def _cfg_keys(self) -> List[str]:
        return ["_template"]

    @property
    def field(self) -> str:
        return self._field



================================================
FILE: spacy_llm/tasks/sentiment/util.py
================================================
import warnings
from typing import Any, Dict, Iterable, Optional

from spacy.tokens import Doc
from spacy.training import Example

from ...compat import Self
from ...ty import FewshotExample
from .task import SentimentTask


class SentimentExample(FewshotExample[SentimentTask]):
    text: str
    score: float

    @classmethod
    def generate(cls, example: Example, task: SentimentTask) -> Optional[Self]:
        return cls(
            text=example.reference.text,
            score=getattr(example.reference._, task.field),
        )


def reduce_shards_to_doc(task: SentimentTask, shards: Iterable[Doc]) -> Doc:
    """Reduces shards to docs for SentimentTask by computing an average sentiment score weighted by shard lengths.
    task (SentimentTask): Task.
    shards (Iterable[Doc]): Shards to reduce to single doc instance.
    RETURNS (Doc): Fused doc instance.
    """
    shards = list(shards)
    weights = [len(shard) for shard in shards]
    weights = [n_tokens / sum(weights) for n_tokens in weights]
    sent_scores = [getattr(shard._, task.field) for shard in shards]

    with warnings.catch_warnings():
        warnings.filterwarnings(
            "ignore",
            category=UserWarning,
            message=".*Skipping .* while merging docs.",
        )
        doc = Doc.from_docs(shards, ensure_whitespace=True)
    setattr(
        doc._,
        task.field,
        sum([score * weight for score, weight in zip(sent_scores, weights)]),
    )

    return doc


def score(examples: Iterable[Example], **kwargs) -> Dict[str, Any]:
    """Score sentiment accuracy in examples.
    examples (Iterable[Example]): Examples to score.
    RETURNS (Dict[str, Any]): Dict with metric name -> score.
    """
    score_diffs = [
        abs(
            getattr(example.predicted._, kwargs["field"])
            - getattr(example.reference._, kwargs["field"])
        )
        for example in examples
    ]

    return {"acc_sentiment": 1 - (sum(score_diffs) / len(score_diffs))}



================================================
FILE: spacy_llm/tasks/span/__init__.py
================================================
from .examples import SpanExample, SpanReason
from .parser import parse_responses, parse_responses_cot
from .registry import make_label_check, make_label_check_cot
from .task import SpanTask

__all__ = [
    "make_label_check",
    "make_label_check_cot",
    "parse_responses",
    "parse_responses_cot",
    "SpanExample",
    "SpanReason",
    "SpanTask",
]



================================================
FILE: spacy_llm/tasks/span/examples.py
================================================
import abc
from typing import Dict, Generic, Iterable, List

from spacy.tokens import Span

from ...compat import BaseModel, Self
from ...ty import FewshotExample, TaskContraT


class SpanExample(FewshotExample[TaskContraT], abc.ABC, Generic[TaskContraT]):
    """Example for span tasks not using CoT.
    Note: this should be SpanTaskContraT instead of TaskContraT, but this would entail a circular import.
    """

    text: str
    entities: Dict[str, List[str]]


class SpanReason(BaseModel):
    text: str
    is_entity: bool
    label: str
    reason: str

    @classmethod
    def from_str(cls, line: str, sep: str = "|") -> Self:
        """Parse a single line of LLM output which identifies a span of text,
        whether it's an entity, a label to assign to it, and a reason for that
        assignment.

        e.g. expected line would look like:
        1. Golden State Warriors | True | BASKETBALL_TEAM | is a basketball team in the NBA

        Handles an optional numbered list which we put in the prompt by default so the LLM
        can better structure the order of output for the spans. This number isn't crucial for
        the final parsing so we just strip it for now.

        line (str): Line of LLM output to parse
        sep (str): Optional separator to split on. Defaults to "|".

        RETURNS (Self)
        """
        clean_str = line.strip()
        if "." in clean_str:
            clean_str = clean_str.split(".", maxsplit=1)[1]
        components = [c.strip() for c in clean_str.split(sep)]
        if len(components) != 4:
            raise ValueError(
                "Unable to parse line of LLM output into a SpanReason. ",
                f"line: {line}",
            )

        return cls(
            text=components[0],
            is_entity=components[1].lower() == "true",
            label=components[2],
            reason=components[3],
        )

    def to_str(self, sep: str = "|") -> str:
        """Output as a single line of text representing the expected LLM COT output
        e.g.
        'Golden State Warriors | True | BASKETBALL_TEAM | is a basketball team in the NBA'
        """
        return (
            f"{self.text} {sep} {self.is_entity} {sep} {self.label} {sep} {self.reason}"
        )

    def __str__(self) -> str:
        return self.to_str()


class SpanCoTExample(FewshotExample[TaskContraT], abc.ABC, Generic[TaskContraT]):
    """Example for span tasks using CoT.
    Note: this should be SpanTaskContraT instead of TaskContraT, but this would entail a circular import.
    """

    text: str
    spans: List[SpanReason]

    class Config:
        arbitrary_types_allowed = True

    @staticmethod
    def _extract_span_reasons(spans: Iterable[Span]) -> List[SpanReason]:
        """Extracts SpanReasons from spans.
        spans (Iterable[Span]): Spans to extract reasons from.
        RETURNS (List[SpanReason]): SpanReason instances extracted from example.
        """
        span_reasons: List[SpanReason] = []
        for span in spans:
            span_reasons.append(
                SpanReason(
                    text=span.text,
                    is_entity=True,
                    label=span.label_,
                    reason=f"is a {span.label_}",
                )
            )

        return span_reasons



================================================
FILE: spacy_llm/tasks/span/parser.py
================================================
from typing import Callable, Dict, Iterable, List, Tuple

from spacy.tokens import Doc, Span

from ...tasks.util import find_substrings
from .examples import SpanReason
from .task import SpanTask


def _format_response(
    response: str, normalizer: Callable[[str], str], label_dict: Dict[str, str]
) -> Iterable[Tuple[str, Iterable[str]]]:
    """Parse raw string response into a structured format.
    response (str): LLM response.
    normalizer (Callable[[str], str]): normalizer function.
    label_dict (Dict[str, str]): Mapping of normalized to non-normalized labels.
    RETURNS (Iterable[Tuple[str, Iterable[str]]]): Formatted response.
    """
    output = []
    assert normalizer is not None

    for line in response.strip().split("\n"):
        # Check if the formatting we want exists
        # <entity label>: ent1, ent2
        if line and ":" in line:
            label, phrases = line.split(":", 1)
            norm_label = normalizer(label)
            if norm_label in label_dict:
                # Get the phrases / spans for each entity
                if phrases.strip():
                    _phrases = [p.strip() for p in phrases.strip().split(",")]
                    output.append((label_dict[norm_label], _phrases))

    return output


def parse_responses(
    task: SpanTask, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[str]]
) -> Iterable[Iterable[List[Span]]]:
    """Parses LLM responses for Span tasks.
    task (SpanTask): Task instance.
    shards (Iterable[Iterable[Doc]]): Doc shards.
    responses (Iterable[Iterable[str]]): LLM responses.
    RETURNS (Iterable[Iterable[List[Span]]]): Parsed spans per shard/response.
    """
    for responses_for_doc, shards_for_doc in zip(responses, shards):
        results_for_doc: List[List[Span]] = []

        for shard, response in zip(shards_for_doc, responses_for_doc):
            spans = []
            for label, phrases in _format_response(
                response, task._normalizer, task._label_dict
            ):
                # For each phrase, find the substrings in the text
                # and create a Span
                offsets = find_substrings(
                    shard.text,
                    phrases,
                    case_sensitive=task._case_sensitive_matching,
                    single_match=task._single_match,
                )
                for start, end in offsets:
                    span = shard.char_span(
                        start, end, alignment_mode=task._alignment_mode, label=label
                    )
                    if span is not None:
                        spans.append(span)

            results_for_doc.append(spans)

        yield results_for_doc


def _extract_span_reasons_cot(task: SpanTask, response: str) -> List[SpanReason]:
    """Parse raw string response into a list of SpanReasons.
    task (SpanTask): Task to extract span reasons for.
    response (str): Raw string response from the LLM.
    RETURNS (List[SpanReason]): List of SpanReasons parsed from the response.
    """
    span_reasons = []
    for line in response.strip().split("\n"):
        try:
            span_reason = SpanReason.from_str(line)
        except ValueError:
            continue
        if not span_reason.is_entity:
            continue
        norm_label = task.normalizer(span_reason.label)
        if norm_label not in task.label_dict:
            continue
        label = task.label_dict[norm_label]
        span_reason.label = label
        span_reasons.append(span_reason)
    return span_reasons


def _find_spans_cot(
    task: SpanTask, doc: Doc, span_reasons: List[SpanReason]
) -> List[Span]:
    """Find a list of spaCy Spans from a list of SpanReasons
    for a single spaCy Doc
    task (SpanTask): Task to extract span reasons for.
    doc (Doc): Input doc to parse spans for
    span_reasons (List[SpanReason]): List of SpanReasons to find in doc
    RETURNS (List[Span]): List of spaCy Spans found in doc
    """
    find_after = 0
    spans = []
    prev_span = None
    n_span_reasons = len(span_reasons)
    idx = 0
    while idx < n_span_reasons:
        span_reason = span_reasons[idx]

        # For each phrase, find the SpanReason substring in the text
        # and create a Span
        offsets = find_substrings(
            doc.text,
            [span_reason.text],
            case_sensitive=task.case_sensitive_matching,
            single_match=True,
            find_after=find_after,
        )
        if not offsets:
            idx += 1
            continue

        # Must have exactly one match because single_match=True
        assert len(offsets) == 1
        start, end = offsets[0]

        span = doc.char_span(
            start,
            end,
            alignment_mode=task.alignment_mode,
            label=span_reason.label,
        )

        if span is None:
            # If we couldn't resolve a span, just skip to the next
            # span_reason
            idx += 1
            continue

        if span == prev_span:
            # If the span is the same as the previous span,
            # re-run this span_reason but look farther into the text
            find_after = span.end_char
            continue

        spans.append(span)
        find_after = span.start_char if task.allow_overlap else span.end_char
        prev_span = span
        idx += 1

    return sorted(set(spans))


def parse_responses_cot(
    task: SpanTask, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[str]]
) -> Iterable[Iterable[List[Span]]]:
    """Since we provide entities in a numbered list, we expect the LLM to
    output entities in the order they occur in the text. This parse
    function now incrementally finds substrings in the text and tracks the
    last found span's start character to ensure we don't overwrite
    previously found spans.
    task (SpanTask): Task instance.
    shards (Iterable[Iterable[Doc]]): Doc shards.
    responses (Iterable[Iterable[str]]): LLM responses.
    RETURNS (Iterable[Iterable[List[Span]]]): Spans to assign per shard.
    """
    for responses_for_doc, shards_for_doc in zip(responses, shards):
        results_for_doc: List[List[Span]] = []

        for shard, response in zip(shards_for_doc, responses_for_doc):
            span_reasons = _extract_span_reasons_cot(task, response)
            results_for_doc.append(_find_spans_cot(task, shard, span_reasons))

        yield results_for_doc



================================================
FILE: spacy_llm/tasks/span/registry.py
================================================
from typing import Callable, Iterable

from ...registry import registry
from .examples import SpanCoTExample, SpanExample
from .task import SpanTask
from .util import check_label_consistency, check_label_consistency_cot


@registry.llm_misc("spacy.LabelCheck.v1")
def make_label_check() -> Callable[[SpanTask], Iterable[SpanExample]]:
    return check_label_consistency


@registry.llm_misc("spacy.LabelCheckCoT.v1")
def make_label_check_cot() -> Callable[[SpanTask], Iterable[SpanCoTExample]]:
    return check_label_consistency_cot



================================================
FILE: spacy_llm/tasks/span/task.py
================================================
import abc
from typing import Any, Callable, Dict, Iterable, List, Optional, Type, TypeVar, Union
from typing import cast

from spacy.tokens import Doc, Span

from ...compat import Literal, Protocol, Self
from ...ty import FewshotExample, ShardMapper, ShardReducer, TaskResponseParser
from ..builtin_task import BuiltinTaskWithLabels
from . import SpanExample
from .examples import SpanCoTExample

SpanTaskContraT = TypeVar("SpanTaskContraT", bound="SpanTask", contravariant=True)


class SpanTaskLabelCheck(Protocol[SpanTaskContraT]):
    """Generic protocol for checking label consistency of SpanTask."""

    def __call__(self, task: SpanTaskContraT) -> Iterable[FewshotExample]:
        ...


class SpanTask(BuiltinTaskWithLabels, abc.ABC):
    """Base class for Span-related tasks, eg NER and SpanCat."""

    def __init__(
        self,
        parse_responses: TaskResponseParser[Self],
        prompt_example_type: Type[Union[SpanExample[Self], SpanCoTExample[Self]]],
        labels: List[str],
        template: str,
        label_definitions: Optional[Dict[str, str]],
        prompt_examples: Optional[
            Union[List[SpanExample[Self]], List[SpanCoTExample[Self]]]
        ],
        shard_mapper: ShardMapper,
        shard_reducer: ShardReducer[Self],
        description: Optional[str],
        normalizer: Optional[Callable[[str], str]],
        alignment_mode: Literal["strict", "contract", "expand"],  # noqa: F821
        case_sensitive_matching: bool,
        allow_overlap: bool,
        single_match: bool,
        check_label_consistency: SpanTaskLabelCheck[Self],
    ):
        super().__init__(
            parse_responses=parse_responses,
            prompt_example_type=prompt_example_type,
            template=template,
            prompt_examples=prompt_examples,
            shard_mapper=shard_mapper,
            shard_reducer=shard_reducer,
            labels=labels,
            label_definitions=label_definitions,
            normalizer=normalizer,
        )

        self._prompt_example_type = cast(
            Type[Union[SpanExample[Self], SpanCoTExample[Self]]],
            self._prompt_example_type,
        )
        self._validate_alignment(alignment_mode)
        self._alignment_mode = alignment_mode
        self._case_sensitive_matching = case_sensitive_matching
        self._allow_overlap = allow_overlap
        self._single_match = single_match
        self._check_label_consistency = check_label_consistency
        self._description = description

        if self._prompt_examples:
            self._prompt_examples = list(self._check_label_consistency(self))

    def _get_prompt_data(
        self, shard: Doc, i_shard: int, i_doc: int, n_shards: int
    ) -> Dict[str, Any]:
        return {
            "description": self._description,
            "labels": list(self._label_dict.values()),
            "label_definitions": self._label_definitions,
            "examples": self._prompt_examples,
            "allow_overlap": self._allow_overlap,
        }

    @staticmethod
    def _validate_alignment(alignment_mode: str):
        """Raises error if specified alignment_mode is not supported.
        alignment_mode (str): Alignment mode to check.
        """
        # ideally, this list should be taken from spaCy, but it's not currently exposed from doc.pyx.
        alignment_modes = ("strict", "contract", "expand")
        if alignment_mode not in alignment_modes:
            raise ValueError(
                f"Unsupported alignment mode '{alignment_mode}'. Supported modes: {', '.join(alignment_modes)}"
            )

    def assign_spans(
        self,
        doc: Doc,
        spans: List[Span],
    ) -> None:
        """Assign spans to the document."""
        raise NotImplementedError()

    def parse_responses(
        self, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[str]]
    ) -> Iterable[Doc]:
        shards_teed = self._tee_2d_iterable(shards, 2)

        for shards_for_doc, spans_for_doc in zip(
            shards_teed[0], self._parse_responses(self, shards_teed[1], responses)
        ):
            shards_for_doc = list(shards_for_doc)
            for shard, spans in zip(shards_for_doc, spans_for_doc):
                self.assign_spans(shard, spans)

            yield self._shard_reducer(self, shards_for_doc)  # type: ignore[arg-type]

    @property
    def _cfg_keys(self) -> List[str]:
        return [
            "_label_dict",
            "_template",
            "_label_definitions",
            "_alignment_mode",
            "_case_sensitive_matching",
        ]

    @property
    def alignment_mode(self) -> Literal["strict", "contract", "expand"]:  # noqa: F821
        return self._alignment_mode

    @property
    def case_sensitive_matching(self) -> bool:
        return self._case_sensitive_matching

    @property
    def allow_overlap(self) -> bool:
        return self._allow_overlap

    @property
    def prompt_examples(self) -> Optional[Iterable[FewshotExample]]:
        return self._prompt_examples

    @property
    def prompt_example_type(
        self,
    ) -> Type[Union[SpanExample[Self], SpanCoTExample[Self]]]:
        return self._prompt_example_type

    @property
    def single_match(self) -> bool:
        return self._single_match



================================================
FILE: spacy_llm/tasks/span/util.py
================================================
import warnings
from typing import List

from .examples import SpanCoTExample, SpanExample
from .task import SpanTask


def check_label_consistency(task: SpanTask) -> List[SpanExample]:
    """Checks consistency of labels between examples and defined labels for non-CoT SpanTask. Emits warning on
    inconsistency.
    RETURNS (List[SpanExample]): List of SpanExamples with valid labels.
    """
    assert task.prompt_examples
    assert issubclass(task.prompt_example_type, SpanExample)

    example_labels = {
        task.normalizer(key): key
        for example in task.prompt_examples
        for key in example.entities
    }
    unspecified_labels = {
        example_labels[key]
        for key in (set(example_labels.keys()) - set(task.label_dict.keys()))
    }
    if not set(example_labels.keys()) <= set(task.label_dict.keys()):
        warnings.warn(
            f"Examples contain labels that are not specified in the task configuration. The latter contains the "
            f"following labels: {sorted(list(set(task.label_dict.values())))}. Labels in examples missing from "
            f"the task configuration: {sorted(list(unspecified_labels))}. Please ensure your label specification "
            f"and example labels are consistent."
        )

    # Return examples without non-declared labels. If an example only has undeclared labels, it is discarded.
    return [
        example
        for example in [
            task.prompt_example_type(
                text=example.text,
                entities={
                    label: entities
                    for label, entities in example.entities.items()
                    if task.normalizer(label) in task.label_dict
                },
            )
            for example in task.prompt_examples
        ]
        if len(example.entities)
    ]


def check_label_consistency_cot(task: SpanTask) -> List[SpanCoTExample]:
    """Checks consistency of labels between examples and defined labels for CoT version of SpanTask. Emits warning on
    inconsistency.
    RETURNS (List[SpanExampleCoT]): List of SpanExamples with valid labels.
    """
    assert task.prompt_examples
    assert issubclass(task.prompt_example_type, SpanCoTExample)

    null_labels = {
        task.normalizer(entity.label): entity.label
        for example in task.prompt_examples
        for entity in example.spans
        if not entity.is_entity
    }

    if len(null_labels) > 1:
        warnings.warn(
            f"Negative examples contain multiple negative labels: {', '.join(null_labels.keys())}."
        )

    example_labels = {
        task.normalizer(entity.label): entity.label
        for example in task.prompt_examples
        for entity in example.spans
        if entity.is_entity
    }

    unspecified_labels = {
        example_labels[key]
        for key in (set(example_labels.keys()) - set(task.label_dict.keys()))
    }
    if not set(example_labels.keys()) <= set(task.label_dict.keys()):
        warnings.warn(
            f"Examples contain labels that are not specified in the task configuration. The latter contains the "
            f"following labels: {sorted(list(set(task.label_dict.values())))}. Labels in examples missing from "
            f"the task configuration: {sorted(list(unspecified_labels))}. Please ensure your label specification "
            f"and example labels are consistent."
        )

    # Return examples without non-declared labels. If an example only has undeclared labels, it is discarded.
    include_labels = dict(task.label_dict)
    include_labels.update(null_labels)

    return [
        example
        for example in [
            task.prompt_example_type(
                text=example.text,
                spans=[
                    entity
                    for entity in example.spans
                    if task.normalizer(entity.label) in include_labels
                ],
            )
            for example in task.prompt_examples
        ]
        if len(example.spans)
    ]



================================================
FILE: spacy_llm/tasks/spancat/__init__.py
================================================
from .registry import make_spancat_task, make_spancat_task_v2, make_spancat_task_v3
from .task import SpanCatTask
from .util import SpanCatExample

__all__ = [
    "make_spancat_task",
    "make_spancat_task_v2",
    "make_spancat_task_v3",
    "SpanCatExample",
    "SpanCatTask",
]



================================================
FILE: spacy_llm/tasks/spancat/registry.py
================================================
from typing import Any, Callable, Dict, Iterable, List, Optional, Type, Union

from ...compat import Literal
from ...registry import registry
from ...ty import ExamplesConfigType, FewshotExample, Scorer, ShardMapper, ShardReducer
from ...ty import TaskResponseParser
from ...util import split_labels
from ..span import parse_responses as parse_span_responses
from ..span import parse_responses_cot as parse_span_responses_cot
from ..span.util import check_label_consistency as check_labels
from ..span.util import check_label_consistency_cot as check_labels_cot
from ..util.sharding import make_shard_mapper
from .task import DEFAULT_SPANCAT_TEMPLATE_V1, DEFAULT_SPANCAT_TEMPLATE_V2
from .task import DEFAULT_SPANCAT_TEMPLATE_V3, SpanCatTask
from .util import SpanCatCoTExample, SpanCatExample, reduce_shards_to_doc, score


@registry.llm_misc("spacy.SpanCatShardReducer.v1")
def make_shard_reducer() -> ShardReducer:
    return reduce_shards_to_doc


@registry.llm_tasks("spacy.SpanCat.v1")
def make_spancat_task(
    parse_responses: Optional[TaskResponseParser[SpanCatTask]] = None,
    prompt_example_type: Optional[Type[FewshotExample]] = None,
    labels: str = "",
    examples: Optional[Callable[[], Iterable[Any]]] = None,
    normalizer: Optional[Callable[[str], str]] = None,
    alignment_mode: Literal["strict", "contract", "expand"] = "contract",
    case_sensitive_matching: bool = False,
    single_match: bool = False,
    spans_key: str = "sc",
    scorer: Optional[Scorer] = None,
):
    """SpanCat.v1 task factory.

    parse_responses (Optional[TaskResponseParser[SpanCatTask]]): Callable for parsing LLM responses for this
        task.
    prompt_example_type (Optional[Type[FewshotExample]]): Type to use for fewshot examples.
    labels (str): Comma-separated list of labels to pass to the template.
        Leave empty to populate it at initialization time (only if examples are provided).
    examples (Optional[Callable[[], Iterable[Any]]]): Optional callable that reads a file containing task examples for
        few-shot learning. If None is passed, then zero-shot learning will be used.
    normalizer (Optional[Callable[[str], str]]): optional normalizer function.
    alignment_mode (str): "strict", "contract" or "expand".
    case_sensitive_matching (bool): Whether to search without case sensitivity.
    single_match (bool): If False, allow one substring to match multiple times in
        the text. If True, returns the first hit.
    spans_key (str): Key of the `Doc.spans` dict to save under.
    scorer (Optional[Scorer]): Scorer function.
    """
    labels_list = split_labels(labels)
    example_type = prompt_example_type or SpanCatExample
    span_examples = (
        [example_type(**eg) for eg in examples()] if callable(examples) else examples
    )

    return SpanCatTask(
        labels=labels_list,
        parse_responses=parse_responses or parse_span_responses,
        prompt_example_type=example_type,
        template=DEFAULT_SPANCAT_TEMPLATE_V1,
        prompt_examples=span_examples,
        shard_mapper=make_shard_mapper(),
        shard_reducer=make_shard_reducer(),
        normalizer=normalizer,
        alignment_mode=alignment_mode,
        case_sensitive_matching=case_sensitive_matching,
        single_match=single_match,
        label_definitions=None,
        spans_key=spans_key,
        scorer=scorer or score,
        description=None,
        check_label_consistency=check_labels,
    )


@registry.llm_tasks("spacy.SpanCat.v2")
def make_spancat_task_v2(
    parse_responses: Optional[TaskResponseParser[SpanCatTask]] = None,
    prompt_example_type: Optional[Type[FewshotExample]] = None,
    labels: Union[List[str], str] = [],
    template: str = DEFAULT_SPANCAT_TEMPLATE_V2,
    label_definitions: Optional[Dict[str, str]] = None,
    examples: ExamplesConfigType = None,
    normalizer: Optional[Callable[[str], str]] = None,
    alignment_mode: Literal["strict", "contract", "expand"] = "contract",
    case_sensitive_matching: bool = False,
    single_match: bool = False,
    spans_key: str = "sc",
    scorer: Optional[Scorer] = None,
):
    """SpanCat.v2 task factory.

    parse_responses (Optional[TaskResponseParser[SpanCatTask]]): Callable for parsing LLM responses for this
        task.
    prompt_example_type (Optional[Type[FewshotExample]]): Type to use for fewshot examples.
    labels (Union[str, List[str]]): List of labels to pass to the template,
        either an actual list or a comma-separated string.
        Leave empty to populate it at initialization time (only if examples are provided).
    template (str): Prompt template passed to the model.
    label_definitions (Optional[Dict[str, str]]): Map of label -> description
        of the label to help the language model output the entities wanted.
        It is usually easier to provide these definitions rather than
        full examples, although both can be provided.
    examples (ExamplesConfigType): Optional callable that reads a file containing task examples for
        few-shot learning. If None is passed, then zero-shot learning will be used.
    normalizer (Optional[Callable[[str], str]]): optional normalizer function.
    alignment_mode (str): "strict", "contract" or "expand".
    case_sensitive_matching (bool): Whether to search without case sensitivity.
    single_match (bool): If False, allow one substring to match multiple times in
        the text. If True, returns the first hit.
    spans_key (str): Key of the `Doc.spans` dict to save under.
    scorer (Optional[Scorer]): Scorer function.
    """
    labels_list = split_labels(labels)
    raw_examples = examples() if callable(examples) else examples
    example_type = prompt_example_type or SpanCatExample
    span_examples = (
        [example_type(**eg) for eg in raw_examples] if raw_examples else None
    )

    return SpanCatTask(
        parse_responses=parse_responses or parse_span_responses,
        prompt_example_type=example_type,
        labels=labels_list,
        template=template,
        label_definitions=label_definitions,
        prompt_examples=span_examples,
        shard_mapper=make_shard_mapper(),
        shard_reducer=make_shard_reducer(),
        normalizer=normalizer,
        alignment_mode=alignment_mode,
        case_sensitive_matching=case_sensitive_matching,
        single_match=single_match,
        spans_key=spans_key,
        scorer=scorer or score,
        description=None,
        check_label_consistency=check_labels,
    )


@registry.llm_tasks("spacy.SpanCat.v3")
def make_spancat_task_v3(
    parse_responses: Optional[TaskResponseParser[SpanCatTask]] = None,
    prompt_example_type: Optional[Type[FewshotExample]] = None,
    labels: Union[List[str], str] = [],
    template: str = DEFAULT_SPANCAT_TEMPLATE_V3,
    description: Optional[str] = None,
    label_definitions: Optional[Dict[str, str]] = None,
    examples: ExamplesConfigType = None,
    shard_mapper: Optional[ShardMapper] = None,
    shard_reducer: Optional[ShardReducer] = None,
    normalizer: Optional[Callable[[str], str]] = None,
    alignment_mode: Literal["strict", "contract", "expand"] = "contract",
    case_sensitive_matching: bool = False,
    spans_key: str = "sc",
    scorer: Optional[Scorer] = None,
):
    """SpanCat.v3 task factory for SpanCat with chain-of-thought prompting.

    parse_responses (Optional[TaskResponseParser[SpanCatTask]]): Callable for parsing LLM responses for this
        task.
    prompt_example_type (Optional[Type[FewshotExample]]): Type to use for fewshot examples.
    labels (Union[str, List[str]]): List of labels to pass to the template,
        either an actual list or a comma-separated string.
        Leave empty to populate it at initialization time (only if examples are provided).
    template (str): Prompt template passed to the model.
    description (str): A description of what to recognize or not recognize as entities.
    label_definitions (Optional[Dict[str, str]]): Map of label -> description
        of the label to help the language model output the entities wanted.
        It is usually easier to provide these definitions rather than
        full examples, although both can be provided.
    examples (Optional[Callable[[], Iterable[Any]]]): Optional callable that reads a file containing task examples for
        few-shot learning. If None is passed, then zero-shot learning will be used.
    shard_mapper (Optional[ShardMapper]): Maps docs to shards if they don't fit into the model context.
    shard_reducer (Optional[ShardReducer]): Reduces doc shards back into one doc instance.
    normalizer (Optional[Callable[[str], str]]): optional normalizer function.
    alignment_mode (str): "strict", "contract" or "expand".
    case_sensitive_matching (bool): Whether to search without case sensitivity.
    spans_key (str): Key of the `Doc.spans` dict to save under.
    scorer (Optional[Scorer]): Scorer function.
    """
    labels_list = split_labels(labels)
    raw_examples = examples() if callable(examples) else examples
    example_type = prompt_example_type or SpanCatCoTExample
    span_examples = (
        [example_type(**eg) for eg in raw_examples] if raw_examples else None
    )

    return SpanCatTask(
        parse_responses=parse_responses or parse_span_responses_cot,
        prompt_example_type=example_type,
        labels=labels_list,
        template=template,
        label_definitions=label_definitions,
        prompt_examples=span_examples,
        shard_mapper=shard_mapper or make_shard_mapper(),
        shard_reducer=shard_reducer or make_shard_reducer(),
        normalizer=normalizer,
        alignment_mode=alignment_mode,
        case_sensitive_matching=case_sensitive_matching,
        single_match=False,
        spans_key=spans_key,
        scorer=scorer or score,
        description=description,
        check_label_consistency=check_labels_cot,
    )



================================================
FILE: spacy_llm/tasks/spancat/task.py
================================================
from typing import Any, Callable, Dict, Iterable, List, Optional, Type

from spacy.language import Language
from spacy.tokens import Doc, Span
from spacy.training import Example

from ...compat import Literal, Self
from ...ty import FewshotExample, Scorer, ShardMapper, ShardReducer, TaskResponseParser
from ..span import SpanTask
from ..span.task import SpanTaskLabelCheck
from ..templates import read_template

DEFAULT_SPANCAT_TEMPLATE_V1 = read_template("spancat.v1")
DEFAULT_SPANCAT_TEMPLATE_V2 = read_template("spancat.v2")
DEFAULT_SPANCAT_TEMPLATE_V3 = read_template("spancat.v3")


class SpanCatTask(SpanTask):
    def __init__(
        self,
        parse_responses: TaskResponseParser[Self],
        prompt_example_type: Type[FewshotExample[Self]],
        labels: List[str],
        template: str,
        label_definitions: Optional[Dict[str, str]],
        spans_key: str,
        prompt_examples: Optional[List[FewshotExample[Self]]],
        shard_mapper: ShardMapper,
        shard_reducer: ShardReducer[Self],
        normalizer: Optional[Callable[[str], str]],
        alignment_mode: Literal["strict", "contract", "expand"],
        case_sensitive_matching: bool,
        single_match: bool,
        scorer: Scorer,
        description: Optional[str],
        check_label_consistency: SpanTaskLabelCheck[Self],
    ):
        """Default SpanCat task.

        parse_responses (TaskResponseParser[Self]): Callable for parsing LLM responses for this task.
        prompt_example_type (Type[FewshotExample[Self]): Type to use for fewshot examples.
        labels (List[str]): List of labels to pass to the template.
            Leave empty to populate it at initialization time (only if examples are provided).
        template (str): Prompt template passed to the model.
        label_definitions (Optional[Dict[str, str]]): Map of label -> description
            of the label to help the language model output the entities wanted.
            It is usually easier to provide these definitions rather than
            full examples, although both can be provided.
        spans_key (str): Key of the `Doc.spans` dict to save under.
        prompt_examples (Optional[List[FewshotExample[Self]]]): Optional list of few-shot examples to include in prompts.
        shard_mapper (ShardMapper): Maps docs to shards if they don't fit into the model context.
        shard_reducer (ShardReducer[Self]): Reduces doc shards back into one doc instance.
        normalizer (Optional[Callable[[str], str]]): optional normalizer function.
        alignment_mode (str): "strict", "contract" or "expand".
        case_sensitive_matching (bool): Whether to search without case sensitivity.
        single_match (bool): If False, allow one substring to match multiple times in
            the text. If True, returns the first hit.
        scorer (Scorer): Scorer function.
        description (str): A description of what to recognize or not recognize as entities.
        check_label_consistency (SpanTaskLabelCheck): Callable to check label consistency.
        """
        super(SpanCatTask, self).__init__(
            parse_responses=parse_responses,
            prompt_example_type=prompt_example_type,
            labels=labels,
            template=template,
            label_definitions=label_definitions,
            prompt_examples=prompt_examples,
            shard_mapper=shard_mapper,
            shard_reducer=shard_reducer,
            normalizer=normalizer,
            alignment_mode=alignment_mode,
            case_sensitive_matching=case_sensitive_matching,
            single_match=single_match,
            description=description,
            allow_overlap=True,
            check_label_consistency=check_label_consistency,
        )
        self._spans_key = spans_key
        self._scorer = scorer

    def assign_spans(
        self,
        doc: Doc,
        spans: List[Span],
    ) -> None:
        """Assign spans to the document."""
        doc.spans[self._spans_key] = sorted(spans)  # type: ignore [type-var]

    def scorer(self, examples: Iterable[Example]) -> Dict[str, Any]:
        return self._scorer(examples, spans_key=self._spans_key, allow_overlap=True)

    def initialize(
        self,
        get_examples: Callable[[], Iterable["Example"]],
        nlp: Language,
        labels: List[str] = [],
        n_prompt_examples: int = 0,
    ) -> None:
        super()._initialize(
            get_examples=get_examples,
            nlp=nlp,
            labels=labels,
            n_prompt_examples=n_prompt_examples,
            spans_key=self._spans_key,
        )

    @property
    def _cfg_keys(self) -> List[str]:
        return [*super()._cfg_keys, "_spans_key"]

    def _extract_labels_from_example(self, example: Example) -> List[str]:
        return [
            span.label_ for span in example.reference.spans.get(self._spans_key, [])
        ]

    @property
    def spans_key(self) -> str:
        return self._spans_key



================================================
FILE: spacy_llm/tasks/spancat/util.py
================================================
from collections import defaultdict
from typing import Any, Dict, Iterable, Optional

from spacy.pipeline.spancat import spancat_score
from spacy.tokens import Doc
from spacy.training import Example

from ...compat import Self
from ..span import SpanExample
from ..span.examples import SpanCoTExample
from .task import SpanCatTask


class SpanCatExample(SpanExample[SpanCatTask]):
    @classmethod
    def generate(cls, example: Example, task: SpanCatTask) -> Optional[Self]:
        entities = defaultdict(list)
        for span in example.reference.spans[task.spans_key]:
            entities[span.label_].append(span.text)

        return cls(text=example.reference.text, entities=entities)


class SpanCatCoTExample(SpanCoTExample[SpanCatTask]):
    @classmethod
    def generate(cls, example: Example, task: SpanCatTask) -> Self:
        return cls(
            text=example.reference.text,
            spans=SpanCoTExample._extract_span_reasons(
                example.reference.spans[task.spans_key]
            ),
        )


def score(examples: Iterable[Example], **kwargs) -> Dict[str, Any]:
    """Score spancat accuracy in examples.
    examples (Iterable[Example]): Examples to score.
    RETURNS (Dict[str, Any]): Dict with metric name -> score.
    """
    return spancat_score(
        examples,
        spans_key=kwargs["spans_key"],
        allow_overlap=True,
    )


def reduce_shards_to_doc(task: SpanCatTask, shards: Iterable[Doc]) -> Doc:
    """Reduces shards to docs for SpanCatTask.
    task (SpanCatTask): Task.
    shards (Iterable[Doc]): Shards to reduce to single doc instance.
    RETURNS (Doc): Fused doc instance.
    """
    # SpanCatTask only affects span-specific information, so we can just merge shards.
    return Doc.from_docs(list(shards), ensure_whitespace=True)



================================================
FILE: spacy_llm/tasks/summarization/__init__.py
================================================
from .registry import make_summarization_task
from .task import SummarizationTask
from .util import SummarizationExample

__all__ = ["make_summarization_task", "SummarizationExample", "SummarizationTask"]



================================================
FILE: spacy_llm/tasks/summarization/parser.py
================================================
from typing import Iterable, List

from spacy.tokens import Doc

from .task import SummarizationTask


def parse_responses_v1(
    task: SummarizationTask,
    shards: Iterable[Iterable[Doc]],
    responses: Iterable[Iterable[str]],
) -> Iterable[Iterable[str]]:
    """Parses LLM responses for spacy.Summarization.v1.
    task (SummarizationTask): Task instance.
    docs (Iterable[Iterable[Doc]]): Doc shards.
    responses (Iterable[Iterable[str]]): LLM responses.
    RETURNS (Iterable[Iterable[str]]): Summary per shard/response.
    """
    for responses_for_doc in responses:
        results_for_doc: List[str] = []
        for response in responses_for_doc:
            results_for_doc.append(response.replace("'''", "").strip())

        yield responses_for_doc



================================================
FILE: spacy_llm/tasks/summarization/registry.py
================================================
from typing import Optional, Type

from ...registry import registry
from ...ty import ExamplesConfigType, FewshotExample, ShardMapper, ShardReducer
from ...ty import TaskResponseParser
from ..util.sharding import make_shard_mapper
from .parser import parse_responses_v1
from .task import DEFAULT_SUMMARIZATION_TEMPLATE_V1, SummarizationTask
from .util import SummarizationExample, reduce_shards_to_doc


@registry.llm_misc("spacy.SummarizationShardReducer.v1")
def make_shard_reducer() -> ShardReducer:
    return reduce_shards_to_doc


@registry.llm_tasks("spacy.Summarization.v1")
def make_summarization_task(
    template: str = DEFAULT_SUMMARIZATION_TEMPLATE_V1,
    parse_responses: Optional[TaskResponseParser[SummarizationTask]] = None,
    prompt_example_type: Optional[Type[FewshotExample]] = None,
    examples: ExamplesConfigType = None,
    shard_mapper: Optional[ShardMapper] = None,
    shard_reducer: Optional[ShardReducer] = None,
    max_n_words: Optional[int] = None,
    field: str = "summary",
):
    """Summarization.v1 task factory.

    template (str): Prompt template passed to the model.
    parse_responses (Optional[TaskResponseParser[SummarizationTask]]): Callable for parsing LLM responses for
        this task.
    prompt_example_type (Optional[Type[FewshotExample]]): Type to use for fewshot examples.
    examples (ExamplesConfigType): Optional callable that reads a file containing task examples for
        few-shot learning. If None is passed, then zero-shot learning will be used.
    shard_mapper (Optional[ShardMapper]): Maps docs to shards if they don't fit into the model context.
    shard_reducer (Optional[ShardReducer]): Reduces doc shards back into one doc instance.
    max_n_words (int): Max. number of words to use in summary.
    field (str): The name of the doc extension in which to store the summary.
    """
    raw_examples = examples() if callable(examples) else examples
    example_type = prompt_example_type or SummarizationExample
    span_examples = (
        [example_type(**eg) for eg in raw_examples] if raw_examples else None
    )

    return SummarizationTask(
        template=template,
        parse_responses=parse_responses or parse_responses_v1,
        prompt_example_type=example_type,
        prompt_examples=span_examples,
        shard_mapper=shard_mapper or make_shard_mapper(),
        shard_reducer=shard_reducer or make_shard_reducer(),
        max_n_words=max_n_words,
        field=field,
    )



================================================
FILE: spacy_llm/tasks/summarization/task.py
================================================
import warnings
from typing import Any, Callable, Dict, Iterable, List, Optional, Type

from spacy.language import Language
from spacy.tokens import Doc
from spacy.training import Example

from ...compat import Self
from ...ty import FewshotExample, ShardMapper, ShardReducer, TaskResponseParser
from ..builtin_task import BuiltinTask
from ..templates import read_template

DEFAULT_SUMMARIZATION_TEMPLATE_V1 = read_template("summarization.v1")


class SummarizationTask(BuiltinTask):
    def __init__(
        self,
        parse_responses: TaskResponseParser[Self],
        prompt_example_type: Type[FewshotExample[Self]],
        template: str,
        shard_mapper: ShardMapper,
        shard_reducer: ShardReducer[Self],
        max_n_words: Optional[int],
        field: str,
        prompt_examples: Optional[List[FewshotExample[Self]]],
    ):
        """Default summarization task.

        template (str): Prompt template passed to the model.
        parse_responses (TaskResponseParser[Self]): Callable for parsing LLM responses for this task.
        prompt_example_type (Type[FewshotExample[Self]): Type to use for fewshot examples.
        shard_mapper (ShardMapper): Maps docs to shards if they don't fit into the model context.
        shard_reducer (ShardReducer[Self]): Reduces doc shards back into one doc instance.
        max_n_words (Optional[int]): Max. number of words to use in summary.
        field (str): The name of the doc extension in which to store the summary.
        prompt_examples (Optional[List[FewshotExample[Self]]]): Optional list of few-shot examples to include in prompts.
        """
        super().__init__(
            parse_responses=parse_responses,
            prompt_example_type=prompt_example_type,
            template=template,
            prompt_examples=prompt_examples,
            shard_mapper=shard_mapper,
            shard_reducer=shard_reducer,
        )
        self._max_n_words = max_n_words
        self._field = field
        self._check_example_summaries = True

        if not Doc.has_extension(field):
            Doc.set_extension(field, default=None)

    def initialize(
        self,
        get_examples: Callable[[], Iterable["Example"]],
        nlp: Language,
        n_prompt_examples: int = 0,
    ) -> None:
        super()._initialize(
            get_examples=get_examples,
            nlp=nlp,
            n_prompt_examples=n_prompt_examples,
        )

    def _check_prompt_example_summary_len(self) -> None:
        """Checks whether summaries of prompt examples are of expected lengths. Warns if they aren't."""
        if self._max_n_words is None:
            return

        for pr_ex in self._prompt_examples:
            len_summary = len(pr_ex.summary.split())
            len_text = len(pr_ex.text.split())
            if len_summary >= len_text * 1.2:
                warnings.warn(
                    f"The provided example '{pr_ex.text[:30]}...' has a summary of token length {len_summary} and a text "
                    f"of token length {len_text}. Ensure that your examples' summaries are shorter than their original "
                    f"texts."
                )
            if len_summary > self._max_n_words * 1.2:
                warnings.warn(
                    f"The provided example '{pr_ex.text[:20]}...' has a summary of length {len_summary}, but "
                    f"`max_n_words` == {self._max_n_words}. If your examples are longer than they should be, the "
                    f"LLM will likely produce responses that are too long."
                )

    def _get_prompt_data(
        self, shard: Doc, i_shard: int, i_doc: int, n_shards: int
    ) -> Dict[str, Any]:
        if self._check_example_summaries:
            self._check_prompt_example_summary_len()
            self._check_example_summaries = False

        return {
            "max_n_words": int(self._max_n_words / n_shards)
            if self._max_n_words is not None
            else None
        }

    def parse_responses(
        self, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[str]]
    ) -> Iterable[Doc]:
        shards_teed = self._tee_2d_iterable(shards, 2)

        for shards_for_doc, summaries_for_doc in zip(
            shards_teed[0], self._parse_responses(self, shards_teed[1], responses)
        ):
            shards_for_doc = list(shards_for_doc)
            for shard, summary in zip(shards_for_doc, summaries_for_doc):
                setattr(shard._, self._field, summary)

            yield self._shard_reducer(self, shards_for_doc)  # type: ignore[arg-type]

    @property
    def _cfg_keys(self) -> List[str]:
        return ["_template"]

    @property
    def field(self) -> str:
        return self._field

    @property
    def max_n_words(self) -> Optional[int]:
        return self._max_n_words



================================================
FILE: spacy_llm/tasks/summarization/util.py
================================================
import warnings
from typing import Iterable, Optional

from spacy.tokens import Doc
from spacy.training import Example

from ...compat import Self
from ...ty import FewshotExample
from .task import SummarizationTask


class SummarizationExample(FewshotExample[SummarizationTask]):
    text: str
    summary: str

    @classmethod
    def generate(cls, example: Example, task: SummarizationTask) -> Optional[Self]:
        return cls(
            text=example.reference.text,
            summary=getattr(example.reference._, task.field),
        )


def reduce_shards_to_doc(task: SummarizationTask, shards: Iterable[Doc]) -> Doc:
    """Reduces shards to docs for SummarizationTask.
    task (SummarizationTask): Task.
    shards (Iterable[Doc]): Shards to reduce to single doc instance.
    RETURNS (Doc): Fused doc instance.
    """
    shards = list(shards)

    with warnings.catch_warnings():
        warnings.filterwarnings(
            "ignore",
            category=UserWarning,
            message=".*Skipping .* while merging docs.",
        )
        doc = Doc.from_docs(list(shards), ensure_whitespace=True)

    # Summaries are per shard, so we can merge. Number of shards is considered in max. number of words. This means that
    # the resulting summaries will be per shard, which should be an approximately correct summary still.
    setattr(
        doc._, task.field, " ".join([getattr(shard._, task.field) for shard in shards])
    )

    return doc



================================================
FILE: spacy_llm/tasks/templates/__init__.py
================================================
from pathlib import Path

TEMPLATE_DIR = Path(__file__).parent


def read_template(name: str) -> str:
    """Read a template"""

    path = TEMPLATE_DIR / f"{name}.jinja"

    if not path.exists():
        raise ValueError(f"{name} is not a valid template.")

    return path.read_text()



================================================
FILE: spacy_llm/tasks/templates/entity_linker.v1.jinja
================================================
For each of the MENTIONS in the TEXT, resolve the MENTION to the correct entity listed in ENTITIES.
Each of the ENTITIES is prefixed by its ENTITY ID. Each of the MENTIONS in the TEXT is surrounded by *.
For each of the MENTIONS appearing in the text, output the ID of the description fitting them best.
This ID has to be surrounded by single <>, for example <1>. Make sure you make a choice for each MENTION. If no
candidate seems plausible, respond with <NIL> instead of an ENTITY ID.
Output "REASONING:". Describe, step by step, which MENTION should be linked to which ENTITY ID.
Output "SOLUTION:". After that, list the correct ENTITY ID (or NIL) per MENTION. Wrap the ENTITY ID in <>. Each ENTITY ID
should be in a new line, prefixed by the corresponding MENTION and " ::: ".

{# whitespace #}
{%- if prompt_examples -%}
Below are some examples (only use these as a guide):
{# whitespace #}
{%- for example in prompt_examples -%}
{# whitespace #}
TEXT:
'''
{{ example.text }}
'''
MENTIONS: {{ example.mention_str }}
ENTITIES:
{%- for ent_descs in example.entity_descriptions -%}
{% set mention_i = loop.index0 %}
- For *{{ example.mentions[loop.index0] }}*:
{%- for ent_desc in ent_descs -%}
    {# whitespace #}
    {{ example.entity_ids[mention_i][loop.index0] }}. {{ ent_desc }}
{%- endfor -%}
{%- endfor -%}
{# whitespace #}
REASONING:
{%- if example.reasons|length -%}
{# whitespace #}
{# whitespace #}
    {%- for reason in example.reasons -%}
        {%- if reason|length -%}
- {{ reason }}
        {%- else -%}
- The description of the chosen entity {{ example.solutions[loop.index0] }} fits the presented mention *{{ example.mentions[loop.index0] }}* best.
        {%- endif -%}
{# whitespace #}
{# whitespace #}
    {%- endfor -%}
{% else %}
    {%- for reason in example.mentions -%}
        {# whitespace #}
- The description of the chosen entity {{ example.solutions[loop.index0] }} fits the presented mention *{{ example.mentions[loop.index0] }}* best.
    {%- endfor -%}
{%- endif -%}
{# whitespace #}
SOLUTION:
{%- for solution in example.solutions -%}
{# whitespace #}
*{{ example.mentions[loop.index0] }}* ::: <{{ solution }}>
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
End of examples.
{%- endif -%}

TEXT: 
'''
{{ text }}
'''
MENTIONS: {{ mentions_str }}
ENTITIES:
{%- for ent_descs in entity_descriptions -%}
{% set mention_i = loop.index0 %}
- For *{{ mentions[loop.index0] }}*:
{%- for ent_desc in ent_descs -%}
    {# whitespace #}
    {{ entity_ids[mention_i][loop.index0] }}. {{ ent_desc }}
{%- endfor -%}
{%- endfor -%}
{# whitespace #}



================================================
FILE: spacy_llm/tasks/templates/lemma.v1.jinja
================================================
You are an expert lemmatization system. Your task is to accept Text as input and identify the lemma for every token in the Text.
Consider that contractions represent multiple words. Each word in a contraction should be annotated with its lemma separately.
Output each original word on a new line, followed by a colon and the word's lemma - like this:
'''
Word1: Lemma of Word1
Word2: Lemma of Word2
'''
Include the final punctuation token in this list.
Prefix with your output with "Lemmatized text".

{# whitespace #}
{%- if prompt_examples -%}
Below are some examples (only use these as a guide):
{# whitespace #}
{%- for example in prompt_examples -%}
{# whitespace #}
Text:
'''
{{ example.text }}
'''
Lemmas:
'''
{%- for lemma in example.lemmas -%}
{%- for k, v in lemma.items() -%}
{# whitespace #}
{{ k }}: {{ v }}
{%- endfor -%}
{%- endfor -%}
{# whitespace #}
'''
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{%- endif -%}
{# whitespace #}
Here is the text that needs to be lemmatized:
'''
{{ text }}
'''



================================================
FILE: spacy_llm/tasks/templates/ner.v1.jinja
================================================
From the text below, extract the following entities in the following format:
{# whitespace #}
{%- for label in labels -%}
{{ label }}: <comma delimited list of strings>
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{%- if prompt_examples -%}
{# whitespace #}
Below are some examples (only use these as a guide):
{# whitespace #}
{# whitespace #}
{%- for example in prompt_examples -%}
{# whitespace #}
Text:
'''
{{ example.text }}
'''
{# whitespace #}
{%- for label, substrings in example.entities.items() -%}
{{ label }}: {{ ', '.join(substrings) }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- endfor -%}
{%- endif -%}
{# whitespace #}
Here is the text that needs labeling:
{# whitespace #}
Text:
'''
{{ text }}
'''



================================================
FILE: spacy_llm/tasks/templates/ner.v2.jinja
================================================
You are an expert Named Entity Recognition (NER) system. Your task is to accept Text as input and extract named entities for the set of predefined entity labels.
From the Text input provided, extract named entities for each label in the following format:
{# whitespace #}
{# whitespace #}
{%- for label in labels -%}
{{ label }}: <comma delimited list of strings>
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- if label_definitions -%}
Below are definitions of each label to help aid you in what kinds of named entities to extract for each label.
Assume these definitions are written by an expert and follow them closely.
{# whitespace #}
{# whitespace #}
{%- for label, definition in label_definitions.items() -%}
{{ label }}: {{ definition }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- endif -%}
{# whitespace #}
{# whitespace #}
{%- if prompt_examples -%}
Below are some examples (only use these as a guide):
{# whitespace #}
{# whitespace #}
{%- for example in prompt_examples -%}
Text:
'''
{{ example.text }}
'''
{# whitespace #}
{%- for label, substrings in example.entities.items() -%}
{# whitespace #}
{{ label }}: {{ ', '.join(substrings) }}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- endif -%}
Here is the text that needs labeling:
{# whitespace #}
Text:
'''
{{ text }}
'''



================================================
FILE: spacy_llm/tasks/templates/ner.v3.jinja
================================================
You are an expert Named Entity Recognition (NER) system.
Your task is to accept Text as input and extract named entities.
Entities must have one of the following labels: {{ ', '.join(labels) }}.
If a span is not an entity label it: `==NONE==`.
{# whitespace #}
{# whitespace #}
{%- if description -%}
{# whitespace #}
{{ description }}
{# whitespace #}
{%- endif -%}
{%- if label_definitions -%}
Below are definitions of each label to help aid you in what kinds of named entities to extract for each label.
Assume these definitions are written by an expert and follow them closely.
{# whitespace #}
{%- for label, definition in label_definitions.items() -%}
{{ label }}: {{ definition }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- endif -%}
{%- if prompt_examples -%}
Q: Given the paragraph below, identify a list of entities, and for each entry explain why it is or is not an entity:
{# whitespace #}
{# whitespace #}
{%- for example in prompt_examples -%}
Paragraph: {{ example.text }}
Answer:
{# whitespace #}
{%- for span in example.spans -%}
{{ loop.index }}. {{ span.to_str() }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- endfor -%}
{%- else -%}
{# whitespace #}
Here is an example of the output format for a paragraph using different labels than this task requires.
Only use this output format but use the labels provided
above instead of the ones defined in the example below.
Do not output anything besides entities in this output format.
Output entities in the order they occur in the input paragraph regardless of label.

Q: Given the paragraph below, identify a list of entities, and for each entry explain why it is or is not an entity:

Paragraph: Sriracha sauce goes really well with hoisin stir fry, but you should add it after you use the wok.
Answer:
1. Sriracha sauce | True | INGREDIENT | is an ingredient to add to a stir fry
2. really well | False | ==NONE== | is a description of how well sriracha sauce goes with hoisin stir fry
3. hoisin stir fry | True | DISH | is a dish with stir fry vegetables and hoisin sauce
4. wok | True | EQUIPMENT | is a piece of cooking equipment used to stir fry ingredients
{# whitespace #}
{# whitespace #}
{%- endif -%}
Paragraph: {{ text }}
Answer:



================================================
FILE: spacy_llm/tasks/templates/raw.v1.jinja
================================================
{%- if prompt_examples -%}
Below are some examples (only use these as a guide):
{# whitespace #}
{%- for example in prompt_examples -%}
{# whitespace #}
Text:
{{ example.text }}
Reply:
{{ example.reply }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{%- endif -%}
{# whitespace #}
Text:
{{ text }}
Reply:



================================================
FILE: spacy_llm/tasks/templates/rel.v1.jinja
================================================
The text below contains pre-extracted entities, denoted in the following format within the text:
{# whitespace #}
<entity text>[ENT<entity id>:<entity label>]
{# whitespace #}
From the text below, extract the following relations between entities:
{# whitespace #}
{# whitespace #}
{%- for label in labels -%}
{{ label }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
The extraction has to use the following format, with one line for each detected relation:
{# whitespace #}
{"dep": <entity id>, "dest": <entity id>, "relation": <relation label>}
{# whitespace #}
Make sure that only relevant relations are listed, and that each line is a valid JSON object.
{# whitespace #}
{%- if label_definitions -%}
Below are definitions of each label to help aid you in what kinds of relationship to extract for each label.
Assume these definitions are written by an expert and follow them closely.
{# whitespace #}
{# whitespace #}
{%- for label, definition in label_definitions.items() -%}
{{ label }}: {{ definition }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- endif -%}
{%- if prompt_examples -%}
Below are some examples (only use these as a guide):
{# whitespace #}
{# whitespace #}
{%- for example in prompt_examples -%}
Text:
'''
{{ preannotate(example) }}
'''
{# whitespace #}
{%- for item in example.relations -%}
{# whitespace #}
{{ item.json() }}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- endif -%}
Here is the text that needs labeling:
{# whitespace #}
Text:
'''
{{ text }}
'''


================================================
FILE: spacy_llm/tasks/templates/sentiment.v1.jinja
================================================
Analyse whether the text surrounded by ''' is positive or negative. Respond with a float value between 0 and 1. 1 represents an exclusively positive sentiment, 0 an exclusively negative sentiment.
{# whitespace #}
{%- if prompt_examples -%}
Below are some examples (only use these as a guide):
{# whitespace #}
{%- for example in prompt_examples -%}
{# whitespace #}
Text:
'''
{{ example.text }}
'''
Answer: {{ example.score }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{%- endif -%}
{# whitespace #}
Text:
'''
{{ text }}
'''
Answer:


================================================
FILE: spacy_llm/tasks/templates/spancat.v1.jinja
================================================
From the text below, extract the following (possibly overlapping) entities in the following format:
{# whitespace #}
{%- for label in labels -%}
{{ label }}: <comma delimited list of strings>
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{%- if prompt_examples -%}
{# whitespace #}
Below are some examples (only use these as a guide):
{# whitespace #}
{# whitespace #}
{%- for example in prompt_examples -%}
{# whitespace #}
Text:
'''
{{ example.text }}
'''
{# whitespace #}
{%- for label, substrings in example.entities.items() -%}
{{ label }}: {{ ', '.join(substrings) }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- endfor -%}
{%- endif -%}
{# whitespace #}
Here is the text that needs labeling:
{# whitespace #}
Text:
'''
{{ text }}
'''



================================================
FILE: spacy_llm/tasks/templates/spancat.v2.jinja
================================================
You are an expert Named Entity Recognition (NER) system. Your task is to accept Text as input and extract named entities for the set of predefined entity labels.
The entities you extract for each label can overlap with each other.
From the Text input provided, extract named entities for each label in the following format:
{# whitespace #}
{# whitespace #}
{%- for label in labels -%}
{{ label }}: <comma delimited list of strings>
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- if label_definitions -%}
Below are definitions of each label to help aid you in what kinds of named entities to extract for each label.
Assume these definitions are written by an expert and follow them closely.
{# whitespace #}
{# whitespace #}
{%- for label, definition in label_definitions.items() -%}
{{ label }}: {{ definition }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- endif -%}
{# whitespace #}
{# whitespace #}
{%- if prompt_examples -%}
Below are some examples (only use these as a guide):
{# whitespace #}
{# whitespace #}
{%- for example in prompt_examples -%}
Text:
'''
{{ example.text }}
'''
{# whitespace #}
{%- for label, substrings in example.entities.items() -%}
{# whitespace #}
{{ label }}: {{ ', '.join(substrings) }}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- endif -%}
Here is the text that needs labeling:
{# whitespace #}
Text:
'''
{{ text }}
'''



================================================
FILE: spacy_llm/tasks/templates/spancat.v3.jinja
================================================
You are an expert Entity Recognition system.
Your task is to accept Text as input and extract named entities.
The entities you extract can overlap with each other.
{# whitespace #}
Entities must have one of the following labels: {{ ', '.join(labels) }}.
If a span is not an entity label it: `==NONE==`.
{# whitespace #}
{# whitespace #}
{%- if description -%}
{# whitespace #}
{{ description }}
{# whitespace #}
{%- endif -%}
{%- if label_definitions -%}
Below are definitions of each label to help aid you in what kinds of named entities to extract for each label.
Assume these definitions are written by an expert and follow them closely.
{# whitespace #}
{%- for label, definition in label_definitions.items() -%}
{{ label }}: {{ definition }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- endif -%}
{%- if prompt_examples %}
Q: Given the paragraph below, identify a list of entities, and for each entry explain why it is or is not an entity:
{# whitespace #}
{# whitespace #}
{%- for example in prompt_examples -%}
Paragraph: {{ example.text }}
Answer:
{# whitespace #}
{%- for span in example.spans -%}
{{ loop.index }}. {{ span.to_str() }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- endfor -%}
{%- else %}
{# whitespace #}
Here is an example of the output format for a paragraph using different labels than this task requires.
Only use this output format but use the labels provided
above instead of the ones defined in the example below.
Do not output anything besides entities in this output format.
Output entities in the order they occur in the input paragraph regardless of label.

Q: Given the paragraph below, identify a list of entities, and for each entry explain why it is or is not an entity:

Paragraph: Sriracha sauce goes really well with hoisin stir fry, but you should add it after you use the wok.
Answer:
1. Sriracha sauce | True | INGREDIENT | is an ingredient to add to a stir fry
2. really well | False | ==NONE== | is a description of how well sriracha sauce goes with hoisin stir fry
3. hoisin stir fry | True | DISH | is a dish with stir fry vegetables and hoisin sauce
4. wok | True | EQUIPMENT | is a piece of cooking equipment used to stir fry ingredients
{# whitespace #}
{# whitespace #}
{%- endif -%}
Paragraph: {{ text }}
Answer:



================================================
FILE: spacy_llm/tasks/templates/summarization.v1.jinja
================================================
You are an expert summarization system. Your task is to accept Text as input and summarize the Text in a concise way.
{%- if max_n_words -%}
{# whitespace #}
The summary must not, under any circumstances, contain more than {{ max_n_words }} words.
{%- endif -%}
{# whitespace #}
{%- if prompt_examples -%}
{# whitespace #}
Below are some examples (only use these as a guide):
{# whitespace #}
{%- for example in prompt_examples -%}
{# whitespace #}
Text:
'''
{{ example.text }}
'''
Summary:
'''
{{ example.summary }}
'''
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{%- endif -%}
{# whitespace #}
Here is the Text that needs to be summarized:
'''
{{ text }}
'''
Summary:



================================================
FILE: spacy_llm/tasks/templates/textcat.v1.jinja
================================================
{%- if labels|length == 1 -%}
{%- set label = labels[0] -%}
Classify whether the text below belongs to the {{ label }} category or not.
If it is a {{ label }}, answer `POS`. If it is not a {{ label }}, answer `NEG`.
{%- else -%}
Classify the text below to any of the following labels: {{ labels|join(", ") }}
{# whitespace #}
{%- if exclusive_classes -%}
{# whitespace #}
The task is exclusive, so only choose one label from what I provided.
{%- else -%}
{# whitespace #}
The task is non-exclusive, so you can provide more than one label as long as
they're comma-delimited. For example: Label1, Label2, Label3.
{%- if allow_none -%}
{# whitespace #}
If the text cannot be classified into any of the provided labels, answer `==NONE==`.
{%- endif -%}
{%- endif -%}
{# whitespace #}
{%- endif -%}
{# whitespace #}
{%- if prompt_examples -%}
{# whitespace #}
Below are some examples (only use these as a guide):
{# whitespace #}
{# whitespace #}
{%- for example in prompt_examples -%}
{# whitespace #}
Text:
'''
{{ example.text }}
'''
{# whitespace #}
{{ example.answer }}
{# whitespace #}
{%- endfor -%}
{%- endif -%}
{# whitespace #}
{# whitespace #}
Here is the text that needs classification
{# whitespace #}
{# whitespace #}
Text:
'''
{{ text }}
'''



================================================
FILE: spacy_llm/tasks/templates/textcat.v2.jinja
================================================
You are an expert Text Classification system. Your task is to accept Text as input
and provide a category for the text based on the predefined labels.
{# whitespace #}
{# whitespace #}
{%- if labels|length == 1 -%}
{%- set label = labels[0] -%}
Classify whether the text below belongs to the {{ label }} category or not.
If it is a {{ label }}, answer `POS`. If it is not a {{ label }}, answer `NEG`.
Do not put any other text in your answer, only one of 'POS' or 'NEG' with nothing before or after.
{%- else -%}
Classify the text below to any of the following labels: {{ labels|join(", ") }}
{# whitespace #}
{%- if exclusive_classes -%}
The task is exclusive, so only choose one label from what I provided.
Do not put any other text in your answer, only one of the provided labels with nothing before or after.
{%- else -%}
The task is non-exclusive, so you can provide more than one label as long as
they're comma-delimited. For example: Label1, Label2, Label3.
Do not put any other text in your answer, only one or more of the provided labels with nothing before or after.
{%- if allow_none -%}
{# whitespace #}
If the text cannot be classified into any of the provided labels, answer `==NONE==`.
{%- endif -%}
{%- endif -%}
{# whitespace #}
{%- endif -%}
{# whitespace #}
{%- if prompt_examples -%}
{# whitespace #}
Below are some examples (only use these as a guide):
{# whitespace #}
{# whitespace #}
{%- for example in prompt_examples -%}
{# whitespace #}
Text:
'''
{{ example.text }}
'''
{# whitespace #}
{{ example.answer }}
{# whitespace #}
{%- endfor -%}
{%- endif -%}
{# whitespace #}
{# whitespace #}
Here is the text that needs classification
{# whitespace #}
{# whitespace #}
Text:
'''
{{ text }}
'''



================================================
FILE: spacy_llm/tasks/templates/textcat.v3.jinja
================================================
You are an expert Text Classification system. Your task is to accept Text as input
and provide a category for the text based on the predefined labels.
{# whitespace #}
{# whitespace #}
{%- if labels|length == 1 -%}
{%- set label = labels[0] -%}
Classify whether the text below belongs to the {{ label }} category or not.
If it is a {{ label }}, answer `POS`. If it is not a {{ label }}, answer `NEG`.
Do not put any other text in your answer, only one of 'POS' or 'NEG' with nothing before or after.
{%- else -%}
Classify the text below to any of the following labels: {{ labels|join(", ") }}
{# whitespace #}
{# whitespace #}
{%- if exclusive_classes -%}
The task is exclusive, so only choose one label from what I provided.
Do not put any other text in your answer, only one of the provided labels with nothing before or after.
{%- else -%}
The task is non-exclusive, so you can provide more than one label as long as
they're comma-delimited. For example: Label1, Label2, Label3.
Do not put any other text in your answer, only one or more of the provided labels with nothing before or after.
{%- if allow_none -%}
{# whitespace #}
If the text cannot be classified into any of the provided labels, answer `==NONE==`.
{%- endif -%}
{%- endif -%}
{# whitespace #}
{%- endif -%}
{# whitespace #}
{%- if label_definitions -%}
{# whitespace #}
{# whitespace #}
Below are definitions of each label to help aid you in correctly classifying the text.
Assume these definitions are written by an expert and follow them closely.
{# whitespace #}
{# whitespace #}
{%- for label, definition in label_definitions.items() -%}
{{ label }}: {{ definition }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{%- endif -%}
{# whitespace #}
{%- if prompt_examples -%}
{# whitespace #}
Below are some examples (only use these as a guide):
{# whitespace #}
{# whitespace #}
{%- for example in prompt_examples -%}
{# whitespace #}
Text:
'''
{{ example.text }}
'''
{# whitespace #}
{{ example.answer }}
{# whitespace #}
{%- endfor -%}
{%- endif -%}
{# whitespace #}
{# whitespace #}
Here is the text that needs classification
{# whitespace #}
{# whitespace #}
Text:
'''
{{ text }}
'''



================================================
FILE: spacy_llm/tasks/templates/translation.v1.jinja
================================================
{%- if source_lang -%}
Translate the text after "Text:" from {{ source_lang }} to {{ target_lang }}.
{% else %}
Translate the text after "Text:" to {{ target_lang }}.
{% endif %}
Respond after "Translation:" with nothing but the translated text.
{# whitespace #}
{%- if prompt_examples -%}
Below are some examples (only use these as a guide):
{# whitespace #}
{%- for example in prompt_examples -%}
{# whitespace #}
Text:
{{ example.text }}
Translation:
{{ example.translation }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{%- endif -%}
{# whitespace #}
Text:
{{ text }}
Translation:


================================================
FILE: spacy_llm/tasks/textcat/__init__.py
================================================
from .registry import make_textcat_task, make_textcat_task_v2, make_textcat_task_v3
from .task import TextCatTask
from .util import TextCatExample

__all__ = [
    "make_textcat_task",
    "make_textcat_task_v2",
    "make_textcat_task_v3",
    "TextCatExample",
    "TextCatTask",
]



================================================
FILE: spacy_llm/tasks/textcat/parser.py
================================================
from typing import Dict, Iterable, List

from spacy.tokens import Doc
from wasabi import msg

from .task import TextCatTask


def parse_responses_v1_v2_v3(
    task: TextCatTask,
    shards: Iterable[Iterable[Doc]],
    responses: Iterable[Iterable[str]],
) -> Iterable[Iterable[Dict[str, float]]]:
    """Parses LLM responses for spacy.TextCat.v1, v2 and v3
    task (LemmaTask): Task instance.
    shards (Iterable[Iterable[Doc]]): Doc shards.
    responses (Iterable[Iterable[str]]): LLM responses.
    RETURNS (Iterable[Iterable[Dict[str, float]]]): TextCat scores per shard and class.
    """
    for response_for_doc in responses:
        results_for_doc: List[Dict[str, float]] = []

        for response in response_for_doc:
            categories: Dict[str, float]
            response = response.strip()
            if task.use_binary:
                # Binary classification: We only have one label
                label: str = list(task.label_dict.values())[0]
                score = 1.0 if response.upper() == "POS" else 0.0
                categories = {label: score}
            else:
                # Multilabel classification
                categories = {label: 0.0 for label in task.label_dict.values()}

                pred_labels = response.split(",")
                if task.exclusive_classes and len(pred_labels) > 1:
                    # Don't use anything but raise a debug message
                    # Don't raise an error. Let user abort if they want to.
                    msg.text(
                        f"LLM returned multiple labels for this exclusive task: {pred_labels}.",
                        " Will store an empty label instead.",
                        show=task.verbose,
                    )
                    pred_labels = []

                for pred in pred_labels:
                    if task.normalizer(pred.strip()) in task.label_dict:
                        category = task.label_dict[task.normalizer(pred.strip())]
                        categories[category] = 1.0

            results_for_doc.append(categories)

        yield results_for_doc



================================================
FILE: spacy_llm/tasks/textcat/registry.py
================================================
from typing import Callable, Dict, List, Optional, Type, Union

from ...registry import registry
from ...ty import ExamplesConfigType, FewshotExample, Scorer, ShardMapper, ShardReducer
from ...ty import TaskResponseParser
from ...util import split_labels
from ..util.sharding import make_shard_mapper
from .parser import parse_responses_v1_v2_v3
from .task import DEFAULT_TEXTCAT_TEMPLATE_V1, DEFAULT_TEXTCAT_TEMPLATE_V2
from .task import DEFAULT_TEXTCAT_TEMPLATE_V3, TextCatTask
from .util import TextCatExample, reduce_shards_to_doc, score


@registry.llm_misc("spacy.TextCatShardReducer.v1")
def make_shard_reducer() -> ShardReducer:
    return reduce_shards_to_doc


@registry.llm_tasks("spacy.TextCat.v1")
def make_textcat_task(
    parse_responses: Optional[TaskResponseParser[TextCatTask]] = None,
    prompt_example_type: Optional[Type[FewshotExample]] = None,
    labels: str = "",
    examples: ExamplesConfigType = None,
    normalizer: Optional[Callable[[str], str]] = None,
    exclusive_classes: bool = False,
    allow_none: bool = True,
    verbose: bool = False,
    scorer: Optional[Scorer] = None,
) -> "TextCatTask":
    """TextCat.v1 task factory.

    You can use either binary or multilabel text classification based on the
    labels you provide.

    If a single label is provided, binary classification
    will be used. The label will get a score of `0` or `1` in `doc.cats`.

    Otherwise, multilabel classification will be used. The document labels
    in `doc.cats` will be a dictionary of strings and their score.

    Lastly, you can toggle between exclusive or no-exclusive text
    categorization by passing a flag to the `exclusive_classes` parameter.

    parse_responses (Optional[TaskResponseParser[TextCatTask]]): Callable for parsing LLM responses for this
        task.
    prompt_example_type (Optional[Type[FewshotExample]]): Type to use for fewshot examples.
    labels (str): Comma-separated list of labels to pass to the template.
        This task assumes binary classification if a single label is provided.
        Leave empty to populate it at initialization time (only if examples are provided).
    examples (ExamplesConfigType): Optional callable that reads a file containing task examples for
        few-shot learning. If None is passed, then zero-shot learning will be used.
    normalizer (Optional[Callable[[str], str]]): Optional normalizer function.
    exclusive_classes (bool): If True, require the language model to suggest only one
        label per class. This is automatically set when using binary classification.
    allow_none (bool): if True, there might be cases where no label is applicable.
    verbose (bool): If True, show extra information.
    scorer (Optional[Scorer]): Scorer function.
    """
    labels_list = split_labels(labels)
    raw_examples = examples() if callable(examples) else examples
    example_type = prompt_example_type or TextCatExample
    textcat_examples = (
        [example_type(**eg) for eg in raw_examples] if raw_examples else None
    )
    return TextCatTask(
        parse_responses=parse_responses or parse_responses_v1_v2_v3,
        prompt_example_type=example_type,
        labels=labels_list,
        template=DEFAULT_TEXTCAT_TEMPLATE_V1,
        prompt_examples=textcat_examples,
        shard_mapper=make_shard_mapper(),
        shard_reducer=make_shard_reducer(),
        normalizer=normalizer,
        exclusive_classes=exclusive_classes,
        allow_none=allow_none,
        verbose=verbose,
        label_definitions=None,
        scorer=scorer or score,
    )


@registry.llm_tasks("spacy.TextCat.v2")
def make_textcat_task_v2(
    parse_responses: Optional[TaskResponseParser[TextCatTask]] = None,
    prompt_example_type: Optional[Type[FewshotExample]] = None,
    labels: Union[List[str], str] = [],
    template: str = DEFAULT_TEXTCAT_TEMPLATE_V2,
    examples: ExamplesConfigType = None,
    normalizer: Optional[Callable[[str], str]] = None,
    exclusive_classes: bool = False,
    allow_none: bool = True,
    verbose: bool = False,
    scorer: Optional[Scorer] = None,
) -> "TextCatTask":
    """TextCat.v2 task factory.

    You can use either binary or multilabel text classification based on the
    labels you provide.

    If a single label is provided, binary classification
    will be used. The label will get a score of `0` or `1` in `doc.cats`.

    Otherwise, multilabel classification will be used. The document labels
    in `doc.cats` will be a dictionary of strings and their score.

    Lastly, you can toggle between exclusive or no-exclusive text
    categorization by passing a flag to the `exclusive_classes` parameter.

    parse_responses (Optional[TaskResponseParser[TextCatTask]]): Callable for parsing LLM responses for this
        task.
    prompt_example_type (Optional[Type[FewshotExample]]): Type to use for fewshot examples.
    labels (Union[List[str], str]): List of labels to pass to the template,
        either an actual list or a comma-separated string.
        This task assumes binary classification if a single label is provided.
        Leave empty to populate it at initialization time (only if examples are provided).
    template (str): Prompt template passed to the model.
    examples (ExamplesConfigType): Optional callable that reads a file containing task examples for
        few-shot learning. If None is passed, then zero-shot learning will be used.
    normalizer (Optional[Callable[[str], str]]): Optional normalizer function.
    exclusive_classes (bool): If True, require the language model to suggest only one
        label per class. This is automatically set when using binary classification.
    allow_none (bool): if True, there might be cases where no label is applicable.
    verbose (bool): If True, show extra information.
    scorer (Optional[Scorer]): Scorer function.
    """
    labels_list = split_labels(labels)
    raw_examples = examples() if callable(examples) else examples
    example_type = prompt_example_type or TextCatExample
    textcat_examples = (
        [example_type(**eg) for eg in raw_examples] if raw_examples else None
    )

    return TextCatTask(
        parse_responses=parse_responses or parse_responses_v1_v2_v3,
        prompt_example_type=example_type,
        labels=labels_list,
        template=template,
        prompt_examples=textcat_examples,
        shard_mapper=make_shard_mapper(),
        shard_reducer=make_shard_reducer(),
        normalizer=normalizer,
        exclusive_classes=exclusive_classes,
        allow_none=allow_none,
        verbose=verbose,
        label_definitions=None,
        scorer=scorer or score,
    )


@registry.llm_tasks("spacy.TextCat.v3")
def make_textcat_task_v3(
    parse_responses: Optional[TaskResponseParser[TextCatTask]] = None,
    prompt_example_type: Optional[Type[FewshotExample]] = None,
    labels: Union[List[str], str] = [],
    template: str = DEFAULT_TEXTCAT_TEMPLATE_V3,
    label_definitions: Optional[Dict[str, str]] = None,
    examples: ExamplesConfigType = None,
    shard_mapper: Optional[ShardMapper] = None,
    shard_reducer: Optional[ShardReducer] = None,
    normalizer: Optional[Callable[[str], str]] = None,
    exclusive_classes: bool = False,
    allow_none: bool = True,
    verbose: bool = False,
    scorer: Optional[Scorer] = None,
) -> "TextCatTask":
    """TextCat.v3 task factory.

    You can use either binary or multilabel text classification based on the
    labels you provide.

    If a single label is provided, binary classification
    will be used. The label will get a score of `0` or `1` in `doc.cats`.

    Otherwise, multilabel classification will be used. The document labels
    in `doc.cats` will be a dictionary of strings and their score.

    Lastly, you can toggle between exclusive or no-exclusive text
    categorization by passing a flag to the `exclusive_classes` parameter.

    parse_responses (Optional[TaskResponseParser[TextCatTask]]): Callable for parsing LLM responses for this
        task.
    prompt_example_type (Optional[Type[FewshotExample]]): Type to use for fewshot examples.
    labels (Union[List[str], str]): List of labels to pass to the template,
        either an actual list or a comma-separated string.
        This task assumes binary classification if a single label is provided.
        Leave empty to populate it at initialization time (only if examples are provided).
    template (str): Prompt template passed to the model.
    label_definitions (Optional[Dict[str, str]]): Optional dict mapping a label to a description of that label.
        These descriptions are added to the prompt to help instruct the LLM on what to extract.
    examples (ExamplesConfigType): Optional callable that reads a file containing task examples for
        few-shot learning. If None is passed, then zero-shot learning will be used.
    shard_mapper (Optional[ShardMapper]): Maps docs to shards if they don't fit into the model context.
    shard_reducer (Optional[ShardReducer]): Reduces doc shards back into one doc instance.
    normalizer (Optional[Callable[[str], str]]): Optional normalizer function.
    exclusive_classes (bool): If True, require the language model to suggest only one
        label per class. This is automatically set when using binary classification.
    allow_none (bool): if True, there might be cases where no label is applicable.
    verbose (bool): If True, show extra information.
    scorer (Optional[Scorer]): Scorer function.
    """

    labels_list = split_labels(labels)
    raw_examples = examples() if callable(examples) else examples
    example_type = prompt_example_type or TextCatExample
    textcat_examples = (
        [example_type(**eg) for eg in raw_examples] if raw_examples else None
    )

    return TextCatTask(
        parse_responses=parse_responses or parse_responses_v1_v2_v3,
        prompt_example_type=example_type,
        labels=labels_list,
        template=template,
        label_definitions=label_definitions,
        prompt_examples=textcat_examples,
        shard_mapper=shard_mapper or make_shard_mapper(),
        shard_reducer=shard_reducer or make_shard_reducer(),
        normalizer=normalizer,
        exclusive_classes=exclusive_classes,
        allow_none=allow_none,
        verbose=verbose,
        scorer=scorer or score,
    )



================================================
FILE: spacy_llm/tasks/textcat/task.py
================================================
from typing import Any, Callable, Dict, Iterable, List, Optional, Type

from spacy.language import Language
from spacy.tokens import Doc
from spacy.training import Example
from wasabi import msg

from ...compat import Self
from ...ty import FewshotExample, Scorer, ShardMapper, ShardReducer, TaskResponseParser
from ..builtin_task import BuiltinTaskWithLabels
from ..templates import read_template

DEFAULT_TEXTCAT_TEMPLATE_V1 = read_template("textcat.v1")
DEFAULT_TEXTCAT_TEMPLATE_V2 = read_template("textcat.v2")
DEFAULT_TEXTCAT_TEMPLATE_V3 = read_template("textcat.v3")


class TextCatTask(BuiltinTaskWithLabels):
    def __init__(
        self,
        parse_responses: TaskResponseParser[Self],
        prompt_example_type: Type[FewshotExample[Self]],
        labels: List[str],
        template: str,
        label_definitions: Optional[Dict[str, str]],
        prompt_examples: Optional[List[FewshotExample[Self]]],
        shard_mapper: ShardMapper,
        shard_reducer: ShardReducer[Self],
        normalizer: Optional[Callable[[str], str]],
        exclusive_classes: bool,
        allow_none: bool,
        verbose: bool,
        scorer: Scorer,
    ):
        """Default TextCat task.

        You can use either binary or multilabel text classification based on the
        labels you provide.

        If a single label is provided, binary classification
        will be used. The label will get a score of `0` or `1` in `doc.cats`.

        Otherwise, multilabel classification will be used. The document labels
        in `doc.cats` will be a dictionary of strings and their score.

        Lastly, you can toggle between exclusive or no-exclusive text
        categorization by passing a flag to the `exclusive_classes` parameter.

        parse_responses (TaskResponseParser[Self]): Callable for parsing LLM responses for this task.
        prompt_example_type (Type[FewshotExample[Self]): Type to use for fewshot examples.
        labels (List[str]): List of labels to pass to the template. This task
            assumes binary classification if a single label is provided.
            Leave empty to populate it at initialization time (only if examples are provided).
        template (str): Prompt template passed to the model.
        label_definitions (Optional[Dict[str, str]]): Optional dict mapping a label to a description of that label.
            These descriptions are added to the prompt to help instruct the LLM on what to extract.
        prompt_examples (Optional[List[FewshotExample[Self]]]): Optional list of few-shot examples to include in prompts.
        shard_mapper (ShardMapper): Maps docs to shards if they don't fit into the model context.
        shard_reducer (ShardReducer[Self]): Reduces doc shards back into one doc instance.
        normalizer (Optional[Callable[[str], str]]): Optional normalizer function.
        exclusive_classes (bool): If True, require the language model to suggest only one
            label per class. This is automatically set when using binary classification.
        allow_none (bool): if True, there might be cases where no label is applicable.
        verbose (bool): If True, show extra information.
        scorer (Scorer): Scorer function.
        """
        super().__init__(
            parse_responses=parse_responses,
            prompt_example_type=prompt_example_type,
            template=template,
            prompt_examples=prompt_examples,
            shard_mapper=shard_mapper,
            shard_reducer=shard_reducer,
            labels=labels,
            label_definitions=label_definitions,
            normalizer=normalizer,
        )
        # Textcat configuration
        self._use_binary = True if len(self._label_dict) == 1 else False
        self._exclusive_classes = exclusive_classes
        self._allow_none = allow_none
        self._verbose = verbose
        self._scorer = scorer

        if self._use_binary and not self._exclusive_classes:
            msg.info(
                "Detected binary classification: setting "
                "the `exclusive_classes` parameter to True."
            )
            self._exclusive_classes = True

    def _get_prompt_data(
        self, shard: Doc, i_shard: int, i_doc: int, n_shards: int
    ) -> Dict[str, Any]:
        return {
            "labels": list(self._label_dict.values()),
            "label_definitions": self._label_definitions,
            "exclusive_classes": self._exclusive_classes,
            "allow_none": self._allow_none,
        }

    def parse_responses(
        self, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[str]]
    ) -> Iterable[Doc]:
        shards_teed = self._tee_2d_iterable(shards, 2)
        for shards_for_doc, cats_for_doc in zip(
            shards_teed[0], self._parse_responses(self, shards_teed[1], responses)
        ):
            updated_shards_for_doc: List[Doc] = []

            for shard, cats in zip(shards_for_doc, cats_for_doc):
                shard.cats = cats
                updated_shards_for_doc.append(shard)

            yield self._shard_reducer(self, updated_shards_for_doc)  # type: ignore[arg-type]

    def scorer(
        self,
        examples: Iterable[Example],
    ) -> Dict[str, Any]:
        return self._scorer(
            examples,
            attr="cats",
            labels=self._label_dict.values(),
            multi_label=not self._exclusive_classes,
        )

    def initialize(
        self,
        get_examples: Callable[[], Iterable["Example"]],
        nlp: Language,
        labels: List[str] = [],
        n_prompt_examples: int = 0,
    ) -> None:
        super()._initialize(
            get_examples=get_examples,
            nlp=nlp,
            labels=labels,
            n_prompt_examples=n_prompt_examples,
            use_binary=self._use_binary,
            label_dict=self._label_dict,
        )

    @property
    def _cfg_keys(self) -> List[str]:
        return [
            "_template",
            "_label_dict",
            "_label_definitions",
            "_use_binary",
            "_exclusive_classes",
            "_allow_none",
            "_verbose",
        ]

    def _extract_labels_from_example(self, example: Example) -> List[str]:
        return list(example.reference.cats.keys())

    @property
    def use_binary(self) -> bool:
        return self._use_binary

    @property
    def exclusive_classes(self) -> bool:
        return self._exclusive_classes

    @property
    def allow_none(self) -> bool:
        return self._allow_none

    @property
    def verbose(self) -> bool:
        return self._verbose



================================================
FILE: spacy_llm/tasks/textcat/util.py
================================================
import warnings
from collections import defaultdict
from typing import Any, DefaultDict, Dict, Iterable, Optional

from spacy.scorer import Scorer
from spacy.tokens import Doc
from spacy.training import Example

from ...compat import Self
from ...ty import FewshotExample
from .task import TextCatTask


class TextCatExample(FewshotExample[TextCatTask]):
    text: str
    answer: str

    @classmethod
    def generate(cls, example: Example, task: TextCatTask) -> Optional[Self]:
        if task.use_binary:
            answer = (
                "POS"
                if example.reference.cats[list(task.label_dict.values())[0]] == 1.0
                else "NEG"
            )
        else:
            answer = ",".join(
                [
                    label
                    for label, score in example.reference.cats.items()
                    if score == 1.0
                ]
            )

        return cls(
            text=example.reference.text,
            answer=answer,
        )


def score(examples: Iterable[Example], **kwargs) -> Dict[str, Any]:
    """Score textcat accuracy in examples.
    examples (Iterable[Example]): Examples to score.
    RETURNS (Dict[str, Any]): Dict with metric name -> score.
    """
    return Scorer.score_cats(
        examples,
        attr=kwargs["attr"],
        labels=kwargs["labels"],
        multi_label=kwargs["multi_label"],
    )


def reduce_shards_to_doc(task: TextCatTask, shards: Iterable[Doc]) -> Doc:
    """Reduces shards to docs for TextCatTask.
    task (TextCatTask): Task.
    shards (Iterable[Doc]): Shards to reduce to single doc instance.
    RETURNS (Doc): Fused doc instance.
    """
    shards = list(shards)

    # Compute average sum per category weighted by shard length.
    weights = [len(shard) for shard in shards]
    weights = [n_tokens / sum(weights) for n_tokens in weights]
    all_cats: DefaultDict[str, float] = defaultdict(lambda: 0)
    for weight, shard in zip(weights, shards):
        for cat, cat_score in shard.cats.items():
            all_cats[cat] += cat_score * weight

    with warnings.catch_warnings():
        warnings.filterwarnings(
            "ignore",
            category=UserWarning,
            message=".*Skipping .* while merging docs.",
        )
        doc = Doc.from_docs(shards, ensure_whitespace=True)
    doc.cats = all_cats

    return doc



================================================
FILE: spacy_llm/tasks/translation/__init__.py
================================================
from .registry import make_translation_task
from .task import TranslationTask
from .util import TranslationExample

__all__ = ["make_translation_task", "TranslationExample", "TranslationTask"]



================================================
FILE: spacy_llm/tasks/translation/parser.py
================================================
from typing import Iterable

from spacy.tokens import Doc

from .task import TranslationTask


def parse_responses_v1(
    task: TranslationTask,
    shards: Iterable[Iterable[Doc]],
    responses: Iterable[Iterable[str]],
) -> Iterable[Iterable[str]]:
    """Parses LLM responses for spacy.Translation.v1.
    task (TranslationTask): Task instance.
    docs (Iterable[Iterable[Doc]]): Doc shards.
    responses (Iterable[Iterable[str]]): LLM responses.
    RETURNS (Iterable[Iterable[str]]): Summary per shard/response.
    """
    for responses_for_doc in responses:
        yield list(responses_for_doc)



================================================
FILE: spacy_llm/tasks/translation/registry.py
================================================
from typing import Optional, Type

from ...registry import registry
from ...ty import ExamplesConfigType, FewshotExample, ShardMapper, ShardReducer
from ...ty import TaskResponseParser
from ..util.sharding import make_shard_mapper
from .parser import parse_responses_v1
from .task import DEFAULT_TRANSLATION_TEMPLATE_V1, TranslationTask
from .util import TranslationExample, reduce_shards_to_doc


@registry.llm_misc("spacy.TranslationShardReducer.v1")
def make_shard_reducer() -> ShardReducer:
    return reduce_shards_to_doc


@registry.llm_tasks("spacy.Translation.v1")
def make_translation_task(
    target_lang: str,
    source_lang: Optional[str] = None,
    template: str = DEFAULT_TRANSLATION_TEMPLATE_V1,
    parse_responses: Optional[TaskResponseParser[TranslationTask]] = None,
    prompt_example_type: Optional[Type[FewshotExample]] = None,
    examples: ExamplesConfigType = None,
    shard_mapper: Optional[ShardMapper] = None,
    shard_reducer: Optional[ShardReducer] = None,
    field: str = "translation",
):
    """Translation.v1 task factory.

    target_lang (str): Language to translate the text to.
    source_lang (Optional[str]): Language the text is in.
    template (str): Prompt template passed to the model.
    parse_responses (Optional[TaskResponseParser[SummarizationTask]]): Callable for parsing LLM responses for
        this task.
    prompt_example_type (Optional[Type[FewshotExample]]): Type to use for fewshot examples.
    examples (ExamplesConfigType): Optional callable that reads a file containing task examples for
        few-shot learning. If None is passed, then zero-shot learning will be used.
    shard_mapper (Optional[ShardMapper]): Maps docs to shards if they don't fit into the model context.
    shard_reducer (Optional[ShardReducer]): Reduces doc shards back into one doc instance.
    field (str): The name of the doc extension in which to store the summary.
    """
    raw_examples = examples() if callable(examples) else examples
    example_type = prompt_example_type or TranslationExample
    span_examples = (
        [example_type(**eg) for eg in raw_examples] if raw_examples else None
    )

    return TranslationTask(
        template=template,
        parse_responses=parse_responses or parse_responses_v1,
        prompt_example_type=example_type,
        prompt_examples=span_examples,
        shard_mapper=shard_mapper or make_shard_mapper(),
        shard_reducer=shard_reducer or make_shard_reducer(),
        field=field,
        source_lang=source_lang,
        target_lang=target_lang,
    )



================================================
FILE: spacy_llm/tasks/translation/task.py
================================================
from typing import Any, Callable, Dict, Iterable, List, Optional, Type

from spacy.language import Language
from spacy.tokens import Doc
from spacy.training import Example

from ...compat import Self
from ...ty import FewshotExample, ShardMapper, ShardReducer, TaskResponseParser
from ..builtin_task import BuiltinTask
from ..templates import read_template

DEFAULT_TRANSLATION_TEMPLATE_V1 = read_template("translation.v1")


class TranslationTask(BuiltinTask):
    def __init__(
        self,
        parse_responses: TaskResponseParser[Self],
        prompt_example_type: Type[FewshotExample[Self]],
        prompt_examples: Optional[List[FewshotExample[Self]]],
        template: str,
        shard_mapper: ShardMapper,
        shard_reducer: ShardReducer[Self],
        field: str,
        source_lang: Optional[str],
        target_lang: str,
    ):
        """Default summarization task.

        template (str): Prompt template passed to the model.
        parse_responses (TaskResponseParser[Self]): Callable for parsing LLM responses for this task.
        prompt_example_type (Type[FewshotExample[Self]): Type to use for fewshot examples.
        prompt_examples (Optional[List[FewshotExample[Self]]]): Optional list of few-shot examples to include in prompts.
        shard_mapper (ShardMapper): Maps docs to shards if they don't fit into the model context.
        shard_reducer (ShardReducer[Self]): Reduces doc shards back into one doc instance.
        field (str): The name of the doc extension in which to store the summary.
        source_lang (Optional[str]): Language the text is in.
        target_lang (str): Language to translate the text to.
        """
        super().__init__(
            parse_responses=parse_responses,
            prompt_example_type=prompt_example_type,
            template=template,
            prompt_examples=prompt_examples,
            shard_mapper=shard_mapper,
            shard_reducer=shard_reducer,
        )
        self._field = field
        self._source_lang = source_lang
        self._target_lang = target_lang

        if not Doc.has_extension(field):
            Doc.set_extension(field, default=None)

    def initialize(
        self,
        get_examples: Callable[[], Iterable["Example"]],
        nlp: Language,
        n_prompt_examples: int = 0,
    ) -> None:
        super()._initialize(
            get_examples=get_examples,
            nlp=nlp,
            n_prompt_examples=n_prompt_examples,
        )

    def _get_prompt_data(
        self, shard: Doc, i_shard: int, i_doc: int, n_shards: int
    ) -> Dict[str, Any]:
        return {"source_lang": self._source_lang, "target_lang": self._target_lang}

    def parse_responses(
        self, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[str]]
    ) -> Iterable[Doc]:
        shards_teed = self._tee_2d_iterable(shards, 2)

        for shards_for_doc, translations_for_doc in zip(
            shards_teed[0], self._parse_responses(self, shards_teed[1], responses)
        ):
            shards_for_doc = list(shards_for_doc)
            for shard, translation in zip(shards_for_doc, translations_for_doc):
                setattr(shard._, self._field, translation)

            yield self._shard_reducer(self, shards_for_doc)  # type: ignore[arg-type]

    @property
    def _cfg_keys(self) -> List[str]:
        return ["_template"]

    @property
    def field(self) -> str:
        return self._field



================================================
FILE: spacy_llm/tasks/translation/util.py
================================================
import warnings
from typing import Iterable, Optional

from spacy.tokens import Doc
from spacy.training import Example

from ...compat import Self
from ...ty import FewshotExample
from .task import TranslationTask


class TranslationExample(FewshotExample[TranslationTask]):
    text: str
    translation: str

    @classmethod
    def generate(cls, example: Example, task: TranslationTask) -> Optional[Self]:
        return cls(
            text=example.reference.text,
            summary=getattr(example.reference._, task.field),
        )


def reduce_shards_to_doc(task: TranslationTask, shards: Iterable[Doc]) -> Doc:
    """Reduces shards to docs for TranslationTask.
    task (TranslationTask): Task.
    shards (Iterable[Doc]): Shards to reduce to single doc instance.
    RETURNS (Doc): Fused doc instance.
    """
    shards = list(shards)

    with warnings.catch_warnings():
        warnings.filterwarnings(
            "ignore",
            category=UserWarning,
            message=".*Skipping .* while merging docs.",
        )
        doc = Doc.from_docs(list(shards), ensure_whitespace=True)

    setattr(
        doc._, task.field, " ".join([getattr(shard._, task.field) for shard in shards])
    )

    return doc



================================================
FILE: spacy_llm/tasks/util/__init__.py
================================================
from .parsing import find_substrings

__all__ = ["find_substrings"]



================================================
FILE: spacy_llm/tasks/util/parsing.py
================================================
from typing import Iterable, List, Tuple


def _unique(items: Iterable[str]) -> Iterable[str]:
    """Remove duplicates without changing order"""
    seen = set()
    output = []
    for item in items:
        if item not in seen:
            output.append(item)
            seen.add(item)
    return output


def find_substrings(
    text: str,
    substrings: Iterable[str],
    *,
    case_sensitive: bool = False,
    single_match: bool = False,
    find_after: int = 0,
) -> List[Tuple[int, int]]:
    """Given a list of substrings, find their character start and end positions
    in a text"""

    # Remove empty and duplicate strings, and lowercase everything if need be
    substrings = [s for s in substrings if s and len(s) > 0]
    if not case_sensitive:
        text = text.lower()
        substrings = [s.lower() for s in substrings]
    substrings = _unique(substrings)
    offsets = []
    for substring in substrings:
        search_from = find_after
        # Search until one hit is found. Continue only if single_match is False.
        while True:
            start = text.find(substring, search_from)
            if start == -1:
                break
            end = start + len(substring)
            offsets.append((start, end))
            if single_match:
                break
            search_from = end
    return offsets



================================================
FILE: spacy_llm/tasks/util/sharding.py
================================================
from typing import Callable, Iterable, List, Optional, Union

from spacy.tokens import Doc

from ...registry import registry
from ...ty import NTokenEstimator, ShardMapper


@registry.llm_misc("spacy.NTokenEstimator.v1")
def make_n_token_estimator() -> NTokenEstimator:
    """Generates Callable estimating the number of tokens in a given string.
    # todo improve default tokenization (allow language code to do tokenization with pretrained spacy model)
    RETURNS (NTokenEstimator): Callable estimating the number of tokens in a given string.
    """

    def count_tokens_by_spaces(value: str) -> int:
        return len(value.split())

    return count_tokens_by_spaces


@registry.llm_misc("spacy.ShardMapper.v1")
def make_shard_mapper(
    n_token_estimator: Optional[NTokenEstimator] = None,
    buffer_frac: float = 1.1,
) -> ShardMapper:
    """Generates Callable mapping doc to doc shards fitting within context length.
    n_token_estimator (NTokenEstimator): Estimates number of tokens in a string.
    buffer_frac (float): Buffer to consider in assessment of whether prompt fits into context. E. g. if value is 1.1,
        prompt length * 1.1 will be compared with the context length.
    todo sharding would be better with sentences instead of tokens, but this requires some form of sentence
     splitting we can't rely one...maybe checking for sentences and/or as optional arg?
    RETURNS (ShardMapper): Callable mapping doc to doc shards fitting within context length.
    """
    n_tok_est: NTokenEstimator = n_token_estimator or make_n_token_estimator()

    def map_doc_to_shards(
        doc: Doc,
        i_doc: int,
        context_length: int,
        render_template: Callable[[Doc, int, int, int], str],
    ) -> Union[Iterable[Doc], Doc]:
        prompt = render_template(doc, 0, i_doc, 1)

        # If prompt with complete doc too long: split in shards.
        if n_tok_est(prompt) * buffer_frac > context_length:
            shards: List[Doc] = []
            # Prompt length unfortunately can't be exacted computed prior to rendering the prompt, as external
            # information not present in the doc (e. g. entity description for EL prompts) may be injected.
            # For this reason we follow a greedy binary search heuristic, if the fully rendered prompt is too long:
            #   1. Get total number of tokens/sentences (depending on the reducer's configuration)
            #   2. Splice off doc up to the first half of tokens/sentences
            #   3. Render prompt and check whether it fits into context
            #   4. If yes: repeat with second doc half.
            #   5. If not: repeat from 2., but with split off shard instead of doc.
            remaining_doc: Optional[Doc] = doc.copy()
            fraction = 0.5
            start_idx = 0
            n_shards = 1

            while remaining_doc is not None:
                fits_in_context = False
                shard: Optional[Doc] = None
                end_idx = -1
                n_tries = 0

                while fits_in_context is False:
                    end_idx = start_idx + int(len(remaining_doc) * fraction)
                    shard = doc[start_idx:end_idx].as_doc(copy_user_data=True)
                    fits_in_context = (
                        n_tok_est(render_template(shard, len(shards), i_doc, n_shards))
                        * buffer_frac
                        <= context_length
                    )
                    fraction /= 2
                    n_tries += 1

                    # If prompt is too large even with shard of a single token, raise error - we can't shard any more
                    # than this. This is an edge case and will most likely never occur.
                    if len(shard) == 1 and not fits_in_context:
                        raise ValueError(
                            "Prompt size doesn't allow for the inclusion for shard of length 1. Please "
                            "review your prompt and reduce its size."
                        )

                assert shard is not None
                shards.append(shard)
                fraction = 1
                n_shards = max(len(shards) + round(1 / fraction), 1)
                start_idx = end_idx
                # Set remaining_doc to None if shard contains all of it, i. e. entire original doc has been processed.
                remaining_doc = (
                    doc[end_idx:].as_doc(copy_user_data=True)
                    if shard.text != remaining_doc.text
                    else None
                )

            return shards

        else:
            return [doc]

    return map_doc_to_shards



================================================
FILE: spacy_llm/tests/__init__.py
================================================
[Empty file]


================================================
FILE: spacy_llm/tests/compat.py
================================================
import os

has_openai_key = os.getenv("OPENAI_API_KEY") is not None
has_anthropic_key = os.getenv("ANTHROPIC_API_KEY") is not None
has_cohere_key = os.getenv("CO_API_KEY") is not None
has_azure_openai_key = os.getenv("AZURE_OPENAI_KEY") is not None
has_palm_key = os.getenv("PALM_API_KEY") is not None



================================================
FILE: spacy_llm/tests/conftest.py
================================================
import os
from typing import Iterable

import pytest

from spacy_llm.registry.util import registry


def pytest_addoption(parser):
    parser.addoption(
        "--external",
        action="store_true",
        default=bool(int(os.environ.get("TEST_EXTERNAL", 0))),
        help="include tests that connects to third-party API",
    )
    parser.addoption(
        "--gpu",
        action="store_true",
        default=bool(int(os.environ.get("TEST_GPU", 0))),
        help="include tests that use a GPU",
    )


def pytest_runtest_setup(item):
    def getopt(opt):
        return item.config.getoption(f"--{opt}", False)

    # Integration of boolean flags
    for opt in ["external", "gpu"]:
        if opt in item.keywords and not getopt(opt):
            pytest.skip(f"need --{opt} option to run")


def pytest_collection_modifyitems(config, items):
    types = ("external", "gpu")
    skip_marks = [pytest.mark.skip(reason=f"need --{t} option to run") for t in types]
    for item in items:
        for t, sm in zip(types, skip_marks):
            if (not config.getoption(f"--{t}")) and (t in item.keywords):
                item.add_marker(sm)


@registry.llm_models("test.NoOpModel.v1")
def noop_factory(output: str = ""):
    def noop(prompts: Iterable[Iterable[str]]) -> Iterable[Iterable[str]]:
        return [[output]] * len(list(prompts))

    return noop



================================================
FILE: spacy_llm/tests/test_cache.py
================================================
import copy
import os
import re
import time
from pathlib import Path
from typing import Dict, Iterable, Optional, Tuple

import pytest
import spacy
import srsly  # type: ignore[import]
from spacy.language import Language
from spacy.tokens import Doc, DocBin

from ..cache import BatchCache
from ..registry import registry

_DEFAULT_CFG = {
    "model": {"@llm_models": "spacy.NoOp.v1"},
    "task": {"@llm_tasks": "spacy.NoOp.v1"},
    "cache": {
        "batch_size": 2,
        "max_batches_in_mem": 3,
    },
}


def _init_nlp(tmp_dir: Path) -> Language:
    nlp = spacy.blank("en")
    config = copy.deepcopy(_DEFAULT_CFG)
    config["cache"]["path"] = str(tmp_dir)  # type: ignore
    nlp.add_pipe("llm", config=config)
    return nlp


@pytest.mark.parametrize("use_pipe", (False, True))
def test_caching(use_pipe: bool) -> None:
    """Test pipeline with caching.
    use_pipe (bool): Whether to use .pipe().
    """
    n = 10

    with spacy.util.make_tempdir() as tmpdir:
        nlp = _init_nlp(tmpdir)
        texts = [f"Test {i}" for i in range(n)]
        # Test writing to cache dir.
        docs = list(nlp.pipe(texts)) if use_pipe else [nlp(text) for text in texts]

        #######################################################
        # Test cache writing
        #######################################################

        index = list(srsly.read_jsonl(tmpdir / "index.jsonl"))
        index_dict: Dict[int, int] = {}
        for rec in index:
            index_dict = {**index_dict, **{int(k): int(v) for k, v in rec.items()}}
        assert len(index) == len(index_dict) == n
        cache = nlp.get_pipe("llm")._cache  # type: ignore
        assert cache._stats["hit"] == 0
        assert cache._stats["hit_contains"] == 0
        assert cache._stats["missed"] == 0
        assert cache._stats["missed_contains"] == n
        assert cache._stats["added"] == n
        assert cache._stats["persisted"] == n
        # Check whether docs are in the batch files they are supposed to be in.
        for doc in docs:
            doc_id = BatchCache._doc_id(doc)
            batch_id = index_dict[doc_id]
            batch_path = cache._batch_path(batch_id)
            batch_docs = DocBin().from_disk(batch_path).get_docs(nlp.vocab)
            doc_ids = [BatchCache._doc_id(d) for d in batch_docs]
            assert BatchCache._batch_id(doc_ids) == batch_id
            assert doc_id in doc_ids

        #######################################################
        # Test cache reading
        #######################################################

        nlp_2 = _init_nlp(tmpdir)
        [nlp_2(text) for text in texts]
        cache = nlp_2.get_pipe("llm")._cache  # type: ignore
        assert cache._stats["hit"] == n
        assert cache._stats["hit_contains"] == n
        assert cache._stats["missed"] == 0
        assert cache._stats["missed_contains"] == 0
        assert cache._stats["added"] == 0
        assert cache._stats["persisted"] == 0


@pytest.mark.skip(reason="Flaky test - needs to be updated")
def test_caching_interrupted() -> None:
    """Test pipeline with caching with simulated interruption (i. e. pipeline stops writing before entire batch is
    done).
    """
    n = 100
    texts = [f"Test {i}" for i in range(n)]

    # Collect stats for complete run with caching.
    with spacy.util.make_tempdir() as tmpdir:
        nlp = _init_nlp(tmpdir)
        start = time.time()
        [nlp(text) for text in texts]
        ref_duration = time.time() - start

    with spacy.util.make_tempdir() as tmpdir:
        nlp2 = _init_nlp(tmpdir)
        # Write half of all docs.
        start = time.time()
        for i in range(int(n / 2)):
            nlp2(texts[i])
        pass1_duration = time.time() - start
        pass1_cache = nlp2.get_pipe("llm")._cache  # type: ignore
        # Arbitrary time check to ensure that first pass through half of the doc batch takes up roughly half of the time
        # of a full pass.
        assert abs(ref_duration / 2 - pass1_duration) < ref_duration / 2 * 0.3
        assert pass1_cache._stats["hit"] == 0
        assert pass1_cache._stats["hit"] == 0
        assert pass1_cache._stats["missed"] == n / 2
        assert pass1_cache._stats["missed_contains"] == n / 2
        assert pass1_cache._stats["added"] == n / 2
        assert pass1_cache._stats["persisted"] == n / 2

        nlp3 = _init_nlp(tmpdir)
        start = time.time()
        for i in range(n):
            nlp3(texts[i])
        pass2_duration = time.time() - start
        cache = nlp3.get_pipe("llm")._cache  # type: ignore
        # Arbitrary time check to ensure second pass (leveraging caching) is at least 30% faster (re-utilizing 50% of
        # the entire doc batch, so max. theoretical speed-up is 50%).
        assert ref_duration - pass2_duration >= ref_duration * 0.3
        assert cache._stats["hit"] == n / 2
        assert cache._stats["hit_contains"] == n / 2
        assert cache._stats["missed"] == n / 2
        assert cache._stats["missed_contains"] == n / 2
        assert cache._stats["added"] == n / 2
        assert cache._stats["persisted"] == n / 2


def test_path_file_invalid():
    with spacy.util.make_tempdir() as tmpdir:
        # File path instead of directory path.
        open(tmpdir / "empty_file", "a").close()
        with pytest.raises(
            ValueError, match="Cache directory exists and is not a directory."
        ):
            config = copy.deepcopy(_DEFAULT_CFG)
            config["cache"]["path"] = str(tmpdir / "empty_file")
            spacy.blank("en").add_pipe("llm", config=config)


def test_path_dir_created():
    with spacy.util.make_tempdir() as tmpdir:
        # Non-existing cache directory should be created.
        config = copy.deepcopy(_DEFAULT_CFG)
        assert not (tmpdir / "new_dir").exists()
        config["cache"]["path"] = str(tmpdir / "new_dir")
        spacy.blank("en").add_pipe("llm", config=config)
        assert (tmpdir / "new_dir").exists()


def test_caching_llm_io() -> None:
    """Test availability of LLM IO after caching."""
    with spacy.util.make_tempdir() as tmpdir:
        config = copy.deepcopy(_DEFAULT_CFG)
        config["cache"]["path"] = str(tmpdir)  # type: ignore[index]
        config["cache"]["batch_size"] = 3  # type: ignore[index]
        config["save_io"] = True
        nlp = spacy.blank("en")
        nlp.add_pipe("llm", config=config)
        docs = [nlp(txt) for txt in ("What's 1+1?", "What's 2+2?", "What's 3+3?")]

        assert all([doc.user_data["llm_io"]["llm"]] for doc in docs)
        cached_file_names = [
            f
            for f in os.listdir(tmpdir)
            if os.path.isfile(tmpdir / f) and f.endswith(".spacy")
        ]
        assert len(cached_file_names) == 1
        cached_docs = list(
            DocBin().from_disk(tmpdir / cached_file_names[0]).get_docs(nlp.vocab)
        )
        assert len(cached_docs) == 3
        assert all([doc.user_data["llm_io"]["llm"]] for doc in cached_docs)


def test_prompt_template_handling():
    """Tests that prompt template comparison is done properly."""
    with spacy.util.make_tempdir() as tmpdir:
        # Check if prompt template is written to file properly.
        config = copy.deepcopy(_DEFAULT_CFG)
        config["cache"]["path"] = str(tmpdir)
        nlp = spacy.blank("en")
        nlp.add_pipe("llm", config=config)
        llm = nlp.get_pipe("llm")
        docs = [nlp(text) for text in ("Test 1", "Test 2", "Test 3")]

        prompt_template_filepath = tmpdir / "prompt_template.txt"
        assert prompt_template_filepath.exists() and prompt_template_filepath.is_file()
        with open(prompt_template_filepath, "r") as file:
            assert hash("".join(file.readlines())) == hash(llm._task.prompt_template)

        # This should fail, as the prompt template diverges from the persisted one.
        with pytest.raises(ValueError, match="Prompt template in cache directory"):
            llm._cache.prompt_template = llm._cache.prompt_template + " something else"

        with pytest.warns(UserWarning, match="No prompt template set for Cache object"):
            BatchCache(path=tmpdir, batch_size=3, max_batches_in_mem=4).add(docs[0])

    # Check with task not providing a prompt template.
    with spacy.util.make_tempdir() as tmpdir:

        @registry.llm_tasks("NoPromptTemplate.v1")
        class NoopTask_NoPromptTemplate:
            def generate_prompts(
                self, docs: Iterable[Doc], context_length: Optional[int] = None
            ) -> Iterable[Tuple[Iterable[str], Iterable[Doc]]]:
                for doc in docs:
                    yield [""], [doc]

            def parse_responses(
                self, docs: Iterable[Doc], responses: Iterable[Iterable[str]]
            ) -> Iterable[Doc]:
                return docs

        # Check if prompt template is written to file properly.
        config = copy.deepcopy(_DEFAULT_CFG)
        config["cache"]["path"] = str(tmpdir)
        config["task"]["@llm_tasks"] = "NoPromptTemplate.v1"
        nlp = spacy.blank("en")

        with pytest.warns(
            UserWarning,
            match=re.escape(
                "The specified task does not provide its prompt template via `prompt_template()`."
            ),
        ):
            nlp.add_pipe("llm", config=config)



================================================
FILE: spacy_llm/tests/test_combinations.py
================================================
from typing import Any, Dict

import pytest
import spacy
from thinc.api import NumpyOps, get_current_ops

from spacy_llm.compat import has_langchain
from spacy_llm.pipeline import LLMWrapper


@pytest.mark.external
@pytest.mark.skipif(has_langchain is False, reason="LangChain is not installed")
@pytest.mark.parametrize(
    "model",
    ["langchain.OpenAIChat.v1", "spacy.GPT-3-5.v3", "spacy.GPT-4.v3"],
    ids=["langchain", "rest-openai", "rest-openai"],
)
@pytest.mark.parametrize(
    "task",
    ["spacy.NER.v1", "spacy.NER.v3", "spacy.TextCat.v1"],
    ids=["ner.v1", "ner.v3", "textcat"],
)
@pytest.mark.parametrize("n_process", [1, 2])
def test_combinations(model: str, task: str, n_process: int):
    """Randomly test combinations of models and tasks."""
    ops = get_current_ops()
    if not isinstance(ops, NumpyOps) and n_process != 1:
        pytest.skip("Only test multiple processes on CPU")

    config: Dict[str, Any] = {
        "model": {
            "@llm_models": model,
            "config": {},
        },
        "task": {"@llm_tasks": task},
    }
    if model.startswith("langchain"):
        config["model"]["name"] = "gpt-3.5-turbo"
    # Configure task-specific settings.
    if task.startswith("spacy.NER"):
        config["task"]["labels"] = "PER,ORG,LOC"
    elif task.startswith("spacy.TextCat"):
        config["task"]["labels"] = "Recipe"
        config["task"]["exclusive_classes"] = True

    nlp = spacy.blank("en")
    if model.startswith("langchain"):
        with pytest.warns(UserWarning, match="Task supports sharding"):
            nlp.add_pipe("llm", config=config)
    else:
        nlp.add_pipe("llm", config=config)

    name, component = nlp.pipeline[0]
    assert name == "llm"
    assert isinstance(component, LLMWrapper)

    nlp("This is a test.")
    list(
        nlp.pipe(
            ["This is a second test", "This is a third test"],
            n_process=n_process,
        )
    )



================================================
FILE: spacy_llm/tests/test_registry.py
================================================
import pytest
from spacy import registry

FUNCTIONS = [
    ("misc", "spacy.FewShotReader.v1"),
    ("misc", "spacy.FileReader.v1"),
]


@pytest.mark.parametrize("reg_name,func_name", FUNCTIONS)
def test_registry(reg_name, func_name):
    assert registry.has(reg_name, func_name)
    assert registry.get(reg_name, func_name)



================================================
FILE: spacy_llm/tests/models/__init__.py
================================================
[Empty file]


================================================
FILE: spacy_llm/tests/models/test_anthropic.py
================================================
# mypy: ignore-errors
import re

import pytest

from spacy_llm.models.rest.anthropic import Anthropic, Endpoints

from ..compat import has_anthropic_key


@pytest.mark.external
@pytest.mark.skipif(has_anthropic_key is False, reason="Anthropic API key unavailable")
def test_anthropic_api_response_is_correct():
    """Check if we're getting the expected response and we're parsing it properly"""
    anthropic = Anthropic(
        name="claude-2.1",
        endpoint=Endpoints.COMPLETIONS.value,
        config={"max_tokens_to_sample": 10},
        strict=False,
        max_tries=10,
        interval=5.0,
        max_request_time=20,
        context_length=None,
    )

    prompt = "Count the number of characters in this string: hello"
    num_prompts = 3
    responses = anthropic(prompts=[[prompt]] * num_prompts)
    for response in responses:
        assert isinstance(response, list)
        assert len(response) == 1
        assert isinstance(response[0], str)


@pytest.mark.external
@pytest.mark.skipif(has_anthropic_key is False, reason="Anthropic API key unavailable")
def test_anthropic_api_response_when_error():
    """Check if error message shows up properly given incorrect config"""
    # Incorrect config c.f. https://console.anthropic.com/docs/api/reference
    incorrect_temperature = "one"  # should be an int
    with pytest.raises(ValueError, match="Request to Anthropic API failed:"):
        Anthropic(
            name="claude-instant-1",
            endpoint=Endpoints.COMPLETIONS.value,
            config={
                "max_tokens_to_sample": 10,
                "temperature": incorrect_temperature,
            },
            strict=False,
            max_tries=10,
            interval=5.0,
            max_request_time=20,
            context_length=None,
        )


@pytest.mark.external
@pytest.mark.skipif(has_anthropic_key is False, reason="Anthropic API key unavailable")
def test_anthropic_error_unsupported_model():
    """Ensure graceful handling of error when model is not supported"""
    incorrect_model = "x-gpt-3.5-turbo"
    with pytest.raises(
        ValueError,
        match=re.escape(
            "Ensure that the selected model (x-gpt-3.5-turbo) is supported by the API"
        ),
    ):
        Anthropic(
            name=incorrect_model,
            endpoint=Endpoints.COMPLETIONS.value,
            config={"max_tokens_to_sample": 10},
            strict=False,
            max_tries=10,
            interval=5.0,
            max_request_time=20,
            context_length=None,
        )



================================================
FILE: spacy_llm/tests/models/test_cohere.py
================================================
# mypy: ignore-errors
import pytest

from spacy_llm.models.rest.cohere import Cohere, Endpoints

from ..compat import has_cohere_key


@pytest.mark.external
@pytest.mark.skipif(has_cohere_key is False, reason="Cohere API key not available")
def test_cohere_api_response_is_correct():
    """Check if we're getting the response from the correct structure"""
    cohere = Cohere(
        name="command",
        endpoint=Endpoints.COMPLETION.value,
        config={},
        strict=False,
        max_tries=10,
        interval=5.0,
        max_request_time=20,
        context_length=None,
    )
    prompt = "Count the number of characters in this string: hello"
    num_prompts = 3  # arbitrary number to check multiple inputs
    responses = cohere(prompts=[[prompt]] * num_prompts)
    for response in responses:
        assert isinstance(response, list)
        assert len(response) == 1
        assert isinstance(response[0], str)


@pytest.mark.external
@pytest.mark.skipif(has_cohere_key is False, reason="Cohere API key not available")
def test_cohere_api_response_n_generations():
    """Test how the model handles more than 1 generation of output

    Users can configure Cohere to return more than 1 output for a single prompt
    The current model doesn't support that and the implementation only returns
    the very first output.
    """
    num_generations = 3
    cohere = Cohere(
        name="command",
        endpoint=Endpoints.COMPLETION.value,
        config={"num_generations": num_generations},
        strict=False,
        max_tries=10,
        interval=5.0,
        max_request_time=20,
        context_length=None,
    )

    prompt = "Count the number of characters in this string: hello"
    num_prompts = 3
    responses = cohere(prompts=[[prompt]] * num_prompts)
    for response in responses:
        assert isinstance(response, list)
        assert len(response) == 1
        assert isinstance(response[0], str)


@pytest.mark.external
@pytest.mark.skipif(has_cohere_key is False, reason="Cohere API key not available")
def test_cohere_api_response_when_error():
    """Ensure graceful handling of error in the Cohere model"""
    # Incorrect config because temperature is in incorrect range [0, 5]
    # c.f. https://docs.cohere.com/reference/generate
    incorrect_temperature = 1000  # must be between 0 and 5
    with pytest.raises(ValueError, match="Request to Cohere API failed:"):
        Cohere(
            name="command",
            endpoint=Endpoints.COMPLETION.value,
            config={"temperature": incorrect_temperature},
            strict=False,
            max_tries=10,
            interval=5.0,
            max_request_time=20,
            context_length=None,
        )


@pytest.mark.external
@pytest.mark.skipif(has_cohere_key is False, reason="Cohere API key not available")
def test_cohere_error_unsupported_model():
    """Ensure graceful handling of error when model is not supported"""
    incorrect_model = "x-gpt-3.5-turbo"
    with pytest.raises(ValueError, match="model not found"):
        Cohere(
            name=incorrect_model,
            config={},
            endpoint=Endpoints.COMPLETION.value,
            strict=False,
            max_tries=10,
            interval=5.0,
            max_request_time=20,
            context_length=None,
        )



================================================
FILE: spacy_llm/tests/models/test_dolly.py
================================================
import copy

import pytest
import spacy
from confection import Config  # type: ignore[import]
from thinc.compat import has_torch_cuda_gpu

from ...compat import torch

_PIPE_CFG = {
    "model": {
        "@llm_models": "spacy.Dolly.v1",
        "name": "dolly-v2-3b",
    },
    "task": {"@llm_tasks": "spacy.NoOp.v1"},
    "save_io": True,
}

_NLP_CONFIG = """

[nlp]
lang = "en"
pipeline = ["llm"]
batch_size = 128

[components]

[components.llm]
factory = "llm"

[components.llm.task]
@llm_tasks = "spacy.NoOp.v1"

[components.llm.model]
@llm_models = "spacy.Dolly.v1"
name = "dolly-v2-3b"
"""


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_init():
    """Test initialization and simple run."""
    nlp = spacy.blank("en")
    nlp.add_pipe("llm", config=_PIPE_CFG)
    doc = nlp("This is a test.")
    nlp.get_pipe("llm")._model.get_model_names()
    torch.cuda.empty_cache()
    assert not doc.user_data["llm_io"]["llm"]["response"][0].startswith(
        doc.user_data["llm_io"]["llm"]["prompt"][0]
    )


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_init_from_config():
    orig_config = Config().from_str(_NLP_CONFIG)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]
    torch.cuda.empty_cache()


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_invalid_model():
    orig_config = Config().from_str(_NLP_CONFIG)
    config = copy.deepcopy(orig_config)
    config["components"]["llm"]["model"]["name"] = "dolly-the-sheep"
    with pytest.raises(ValueError, match="unexpected value; permitted"):
        spacy.util.load_model_from_config(config, auto_fill=True)
    torch.cuda.empty_cache()



================================================
FILE: spacy_llm/tests/models/test_falcon.py
================================================
import copy

import pytest
import spacy
from confection import Config  # type: ignore[import]
from thinc.compat import has_torch_cuda_gpu

from ...compat import torch

_PIPE_CFG = {
    "model": {
        "@llm_models": "spacy.Falcon.v1",
        "name": "falcon-rw-1b",
    },
    "task": {"@llm_tasks": "spacy.NoOp.v1"},
    "save_io": True,
}

_NLP_CONFIG = """

[nlp]
lang = "en"
pipeline = ["llm"]
batch_size = 128

[components]

[components.llm]
factory = "llm"

[components.llm.task]
@llm_tasks = "spacy.NoOp.v1"

[components.llm.model]
@llm_models = "spacy.Falcon.v1"
name = "falcon-rw-1b"
"""


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_init():
    """Test initialization and simple run."""
    nlp = spacy.blank("en")
    cfg = copy.deepcopy(_PIPE_CFG)
    nlp.add_pipe("llm", config=cfg)
    doc = nlp("This is a test.")
    torch.cuda.empty_cache()
    assert not doc.user_data["llm_io"]["llm"]["response"][0].startswith(
        doc.user_data["llm_io"]["llm"]["prompt"][0]
    )


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_init_from_config():
    orig_config = Config().from_str(_NLP_CONFIG)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]
    torch.cuda.empty_cache()


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_invalid_model():
    orig_config = Config().from_str(_NLP_CONFIG)
    config = copy.deepcopy(orig_config)
    config["components"]["llm"]["model"]["name"] = "x"
    with pytest.raises(ValueError, match="unexpected value; permitted"):
        spacy.util.load_model_from_config(config, auto_fill=True)
    torch.cuda.empty_cache()



================================================
FILE: spacy_llm/tests/models/test_hf.py
================================================
from typing import Tuple

import pytest
import spacy
from thinc.compat import has_torch_cuda_gpu

from spacy_llm.compat import has_accelerate, torch

_PIPE_CFG = {
    "model": {
        "@llm_models": "",
        "name": "",
    },
    "task": {"@llm_tasks": "spacy.NoOp.v1"},
    "save_io": True,
}


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
@pytest.mark.parametrize(
    "model", (("spacy.Dolly.v1", "dolly-v2-3b"), ("spacy.Llama2.v1", "Llama-2-7b-hf"))
)
def test_device_config_conflict(model: Tuple[str, str]):
    """Test device configuration."""
    nlp = spacy.blank("en")
    model, name = model
    cfg = {**_PIPE_CFG, **{"model": {"@llm_models": model, "name": name}}}

    # Set device only.
    cfg["model"]["config_init"] = {"device": "cpu"}  # type: ignore[index]
    nlp.add_pipe("llm", name="llm1", config=cfg)

    # Set device_map only.
    cfg["model"]["config_init"] = {"device_map": "auto"}  # type: ignore[index]
    if has_accelerate:
        nlp.add_pipe("llm", name="llm2", config=cfg)
    else:
        with pytest.raises(ImportError, match="requires Accelerate"):
            nlp.add_pipe("llm", name="llm2", config=cfg)

    # Set device_map and device.
    cfg["model"]["config_init"] = {"device_map": "auto", "device": "cpu"}  # type: ignore[index]
    with pytest.warns(UserWarning, match="conflicting arguments"):
        if has_accelerate:
            nlp.add_pipe("llm", name="llm3", config=cfg)
        else:
            with pytest.raises(ImportError, match="requires Accelerate"):
                nlp.add_pipe("llm", name="llm3", config=cfg)

    torch.cuda.empty_cache()


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_torch_dtype():
    """Test torch_dtype setting."""
    nlp = spacy.blank("en")
    cfg = {
        **_PIPE_CFG,
        **{"model": {"@llm_models": "spacy.Dolly.v1", "name": "dolly-v2-3b"}},
    }

    # Should be converted to torch.float16.
    cfg["model"]["config_init"] = {"torch_dtype": "float16"}  # type: ignore[index]
    llm = nlp.add_pipe("llm", name="llm1", config=cfg)
    assert llm._model._config_init["torch_dtype"] == torch.float16

    # Should remain "auto".
    cfg["model"]["config_init"] = {"torch_dtype": "auto"}  # type: ignore[index]
    nlp.add_pipe("llm", name="llm2", config=cfg)

    # Should fail - nonexistent dtype.
    cfg["model"]["config_init"] = {"torch_dtype": "float999"}  # type: ignore[index]
    with pytest.raises(ValueError, match="Invalid value float999"):
        nlp.add_pipe("llm", name="llm3", config=cfg)

    torch.cuda.empty_cache()



================================================
FILE: spacy_llm/tests/models/test_langchain.py
================================================
import os
from typing import List

import pytest
import spacy

from spacy_llm.compat import has_langchain
from spacy_llm.models.langchain import LangChain
from spacy_llm.tests.compat import has_azure_openai_key

PIPE_CFG = {
    "model": {
        "@llm_models": "langchain.OpenAIChat.v1",
        "name": "gpt-3.5-turbo",
        "config": {"temperature": 0.3},
    },
    "task": {"@llm_tasks": "spacy.NoOp.v1"},
}


def langchain_model_reg_handles() -> List[str]:
    """Returns a list of all LangChain model reg handles."""
    return [
        f"langchain.{cls.__name__}.v1"
        for class_id, cls in LangChain.get_type_to_cls_dict().items()
    ]


@pytest.mark.external
@pytest.mark.skipif(has_langchain is False, reason="LangChain is not installed")
def test_initialization():
    """Test initialization and simple run"""
    nlp = spacy.blank("en")
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp.add_pipe("llm", config=PIPE_CFG)
    nlp("This is a test.")


@pytest.mark.external
@pytest.mark.skipif(has_langchain is False, reason="LangChain is not installed")
@pytest.mark.skipif(
    has_azure_openai_key is False, reason="Azure OpenAI key not available"
)
def test_initialization_azure_openai():
    """Test initialization and simple run with Azure OpenAI."""
    os.environ["OPENAI_API_KEY"] = os.environ["AZURE_OPENAI_KEY"]
    _pipe_cfg = {
        "model": {
            "@llm_models": "langchain.AzureOpenAI.v1",
            "name": "gpt-35-turbo",
            "config": {
                "deployment_name": "gpt-35-turbo",
                "openai_api_version": "2023-05-15",
                "openai_api_base": "https://explosion.openai.azure.com/",
            },
        },
        "task": {"@llm_tasks": "spacy.NoOp.v1"},
    }

    nlp = spacy.blank("en")
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp.add_pipe("llm", config=_pipe_cfg)
    nlp("This is a test.")



================================================
FILE: spacy_llm/tests/models/test_llama2.py
================================================
import copy

import pytest
import spacy
from confection import Config  # type: ignore[import]
from thinc.compat import has_torch_cuda_gpu

from ...compat import torch

_PIPE_CFG = {
    "model": {
        "@llm_models": "spacy.Llama2.v1",
        "name": "Llama-2-7b-hf",
    },
    "task": {"@llm_tasks": "spacy.NoOp.v1"},
    "save_io": True,
}

_NLP_CONFIG = """

[nlp]
lang = "en"
pipeline = ["llm"]
batch_size = 128

[components]

[components.llm]
factory = "llm"

[components.llm.task]
@llm_tasks = "spacy.NoOp.v1"

[components.llm.model]
@llm_models = "spacy.Llama2.v1"
name = "Llama-2-7b-hf"
"""


@pytest.mark.skip(reason="CI runner needs more GPU memory")
@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_init():
    """Test initialization and simple run."""
    nlp = spacy.blank("en")
    cfg = copy.deepcopy(_PIPE_CFG)
    nlp.add_pipe("llm", config=cfg)
    doc = nlp("This is a test.")
    torch.cuda.empty_cache()
    assert not doc.user_data["llm_io"]["llm"]["response"].startswith(
        doc.user_data["llm_io"]["llm"]["prompt"]
    )


@pytest.mark.skip(reason="CI runner needs more GPU memory")
@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_init_from_config():
    orig_config = Config().from_str(_NLP_CONFIG)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]
    torch.cuda.empty_cache()


@pytest.mark.skip(reason="CI runner needs more GPU memory")
@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_invalid_model():
    orig_config = Config().from_str(_NLP_CONFIG)
    config = copy.deepcopy(orig_config)
    config["components"]["llm"]["model"]["name"] = "x"
    with pytest.raises(ValueError, match="unexpected value; permitted"):
        spacy.util.load_model_from_config(config, auto_fill=True)
    torch.cuda.empty_cache()



================================================
FILE: spacy_llm/tests/models/test_mistral.py
================================================
import copy

import pytest
import spacy
from confection import Config  # type: ignore[import]
from thinc.compat import has_torch_cuda_gpu

from ...compat import torch

_PIPE_CFG = {
    "model": {
        "@llm_models": "spacy.Mistral.v1",
        "name": "Mistral-7B-v0.1",
    },
    "task": {"@llm_tasks": "spacy.NoOp.v1"},
}

_NLP_CONFIG = """

[nlp]
lang = "en"
pipeline = ["llm"]
batch_size = 128

[components]

[components.llm]
factory = "llm"

[components.llm.task]
@llm_tasks = "spacy.NoOp.v1"

[components.llm.model]
@llm_models = "spacy.Mistral.v1"
name = "Mistral-7B-v0.1"
"""


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_init():
    """Test initialization and simple run."""
    nlp = spacy.blank("en")
    cfg = copy.deepcopy(_PIPE_CFG)
    nlp.add_pipe("llm", config=cfg)
    nlp("This is a test.")
    torch.cuda.empty_cache()


@pytest.mark.gpu
@pytest.mark.skip(reason="CI runner needs more GPU memory")
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_init_from_config():
    orig_config = Config().from_str(_NLP_CONFIG)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]
    torch.cuda.empty_cache()


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_invalid_model():
    orig_config = Config().from_str(_NLP_CONFIG)
    config = copy.deepcopy(orig_config)
    config["components"]["llm"]["model"]["name"] = "x"
    with pytest.raises(ValueError, match="unexpected value; permitted"):
        spacy.util.load_model_from_config(config, auto_fill=True)
    torch.cuda.empty_cache()



================================================
FILE: spacy_llm/tests/models/test_openllama.py
================================================
import copy

import pytest
import spacy
from confection import Config  # type: ignore[import]
from thinc.compat import has_torch_cuda_gpu

from ...compat import torch

_PIPE_CFG = {
    "model": {
        "@llm_models": "spacy.OpenLLaMA.v1",
        "name": "open_llama_3b",
    },
    "task": {"@llm_tasks": "spacy.NoOp.v1"},
    "save_io": True,
}

_NLP_CONFIG = """
[nlp]
lang = "en"
pipeline = ["llm"]
batch_size = 128

[components]

[components.llm]
factory = "llm"
save_io = True

[components.llm.task]
@llm_tasks = "spacy.NoOp.v1"

[components.llm.model]
@llm_models = spacy.OpenLLaMA.v1
name = open_llama_3b
"""


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_init():
    """Test initialization and simple run."""
    nlp = spacy.blank("en")
    nlp.add_pipe("llm", config=_PIPE_CFG)
    doc = nlp("This is a test.")
    torch.cuda.empty_cache()
    assert not doc.user_data["llm_io"]["llm"]["response"][0].startswith(
        doc.user_data["llm_io"]["llm"]["prompt"][0]
    )


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_init_with_set_config():
    """Test initialization and simple run with changed config."""
    nlp = spacy.blank("en")
    cfg = copy.deepcopy(_PIPE_CFG)
    cfg["model"]["config_run"] = {"max_new_tokens": 32}
    nlp.add_pipe("llm", config=cfg)
    doc = nlp("This is a test.")
    torch.cuda.empty_cache()
    assert not doc.user_data["llm_io"]["llm"]["response"][0].startswith(
        doc.user_data["llm_io"]["llm"]["prompt"][0]
    )


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_init_from_config():
    orig_config = Config().from_str(_NLP_CONFIG)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]
    torch.cuda.empty_cache()


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_invalid_model():
    orig_config = Config().from_str(_NLP_CONFIG)
    config = copy.deepcopy(orig_config)
    config["components"]["llm"]["model"]["name"] = "anything-else"
    with pytest.raises(ValueError, match="unexpected value; permitted"):
        spacy.util.load_model_from_config(config, auto_fill=True)
    torch.cuda.empty_cache()



================================================
FILE: spacy_llm/tests/models/test_palm.py
================================================
# mypy: ignore-errors
import pytest

from spacy_llm.models.rest.palm import palm_bison

from ..compat import has_palm_key


@pytest.mark.external
@pytest.mark.skipif(has_palm_key is False, reason="PaLM API key not available")
@pytest.mark.parametrize("name", ("text-bison-001", "chat-bison-001"))
def test_palm_api_response_is_correct(name: str):
    """Check if we're getting the response from the correct structure"""
    model = palm_bison(name=name)
    prompt = "The number of stars in the universe is"
    num_prompts = 3  # arbitrary number to check multiple inputs
    responses = list(model([prompt] * num_prompts))
    for response in responses:
        assert isinstance(response, str)
    assert len(responses) == 3


@pytest.mark.external
@pytest.mark.skipif(has_palm_key is False, reason="PaLM API key not available")
def test_palm_api_response_n_generations():
    """Test how the model handles more than 1 generation of output

    Users can configure PaLM to return more than one candidate for a single prompt.
    The current model doesn't support that and the implementation only returns
    the very first output.
    """
    candidate_count = 3
    model = palm_bison(config={"candidate_count": candidate_count})

    prompt = "The number of stars in the universe is"
    num_prompts = 3
    responses = list(model([prompt] * num_prompts))
    assert len(responses) == 3
    for response in responses:
        assert isinstance(response, str)


@pytest.mark.external
@pytest.mark.skipif(has_palm_key is False, reason="PaLM API key not available")
def test_palm_api_response_when_error():
    """Ensure graceful handling of error in the PaLM model."""
    # Incorrect config because temperature is in incorrect range [0, 5]
    # c.f. https://developers.generativeai.google/api/rest/generativelanguage/models/generateText
    incorrect_temperature = 1000  # must be between 0 and 1.0
    with pytest.raises(ValueError, match="Request to PaLM API failed:"):
        palm_bison(config={"temperature": incorrect_temperature})


@pytest.mark.external
@pytest.mark.skipif(has_palm_key is False, reason="PaLM API key not available")
def test_palm_error_unsupported_model():
    """Ensure graceful handling of error when model is not supported"""
    incorrect_model = "x-gpt-3.5-turbo"
    with pytest.raises(ValueError, match="Model 'x-gpt-3.5-turbo' is not supported"):
        palm_bison(name=incorrect_model)



================================================
FILE: spacy_llm/tests/models/test_rest.py
================================================
# mypy: ignore-errors
import copy
import re
from typing import Iterable, Optional, Tuple

import pytest
import spacy
from spacy.tokens import Doc

from ...registry import registry
from ..compat import has_azure_openai_key, has_openai_key

PIPE_CFG = {
    "model": {
        "@llm_models": "spacy.GPT-3-5.v2",
    },
    "task": {"@llm_tasks": "spacy.TextCat.v1", "labels": "POSITIVE,NEGATIVE"},
}


@registry.llm_tasks("spacy.Count.v1")
class _CountTask:
    _PROMPT_TEMPLATE = "Count the number of characters in this string: '{text}'."

    def generate_prompts(
        self, docs: Iterable[Doc], context_length: Optional[int] = None
    ) -> Iterable[Tuple[Iterable[str], Iterable[Doc]]]:
        for doc in docs:
            yield [_CountTask._PROMPT_TEMPLATE.format(text=doc.text)], [doc]

    def parse_responses(
        self, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[str]]
    ) -> Iterable[Doc]:
        # Grab the first shard per doc
        return [list(shards_for_doc)[0] for shards_for_doc in shards]

    @property
    def prompt_template(self) -> str:
        return _CountTask._PROMPT_TEMPLATE


def test_initialization():
    """Test initialization and simple run"""
    nlp = spacy.blank("en")
    cfg = copy.deepcopy(PIPE_CFG)
    cfg["model"] = {"@llm_models": "spacy.NoOp.v1"}
    nlp.add_pipe("llm", config=cfg)
    nlp("This is a test.")


@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.external
def test_model_error_handling():
    """Test error handling for wrong model."""
    nlp = spacy.blank("en")
    with pytest.raises(ValueError, match="Could not find function 'spacy.gpt-3.5x.v1'"):
        nlp.add_pipe(
            "llm",
            config={
                "task": {"@llm_tasks": "spacy.NoOp.v1"},
                "model": {"@llm_models": "spacy.gpt-3.5x.v1"},
            },
        )


@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.external
def test_doc_length_error_handling():
    """Test error handling for excessive doc length."""
    nlp = spacy.blank("en")
    nlp.add_pipe(
        "llm",
        config={
            # Not using the NoOp task is necessary here, as the NoOp task sends a fixed-size prompt.
            "task": {"@llm_tasks": "spacy.Count.v1"},
            "model": {"config": {"model": "ada"}},
        },
    )
    # Call with an overly long document to elicit error.
    with pytest.raises(
        ValueError,
        match=re.escape(
            "Request to OpenAI API failed: This model's maximum context length is 4097 tokens. However, your messages "
            "resulted in 5018 tokens. Please reduce the length of the messages."
        ),
    ):
        nlp("n" * 10000)


@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.external
def test_max_time_error_handling():
    """Test error handling for exceeding max. time."""
    nlp = spacy.blank("en")
    with pytest.raises(
        TimeoutError,
        match="Request time out. Check your network connection and the API's availability.",
    ):
        nlp.add_pipe(
            "llm",
            config={
                "task": {"@llm_tasks": "spacy.Count.v1"},
                "model": {
                    "config": {"model": "ada"},
                    "max_request_time": 0.001,
                    "max_tries": 1,
                    "interval": 0.001,
                },
            },
        )


@pytest.mark.skipif(
    has_azure_openai_key is False, reason="Azure OpenAI API key not available"
)
@pytest.mark.external
@pytest.mark.parametrize("deployment_name", ("gpt-35-turbo", "gpt-35-turbo-instruct"))
def test_azure_openai(deployment_name: str):
    """Test initialization and simple run for Azure OpenAI."""
    nlp = spacy.blank("en")
    _pipe_cfg = {
        "model": {
            "@llm_models": "spacy.Azure.v1",
            "base_url": "https://explosion.openai.azure.com/",
            "model_type": "completions",
            "deployment_name": deployment_name,
            "name": deployment_name.replace("35", "3.5"),
        },
        "task": {"@llm_tasks": "spacy.NoOp.v1"},
        "save_io": True,
    }

    cfg = copy.deepcopy(_pipe_cfg)
    nlp.add_pipe("llm", config=cfg)
    nlp("This is a test.")



================================================
FILE: spacy_llm/tests/models/test_stablelm.py
================================================
import copy

import pytest
import spacy
from confection import Config  # type: ignore[import]
from thinc.compat import has_torch_cuda_gpu

from ...compat import torch

_PIPE_CFG = {
    "model": {
        "@llm_models": "spacy.StableLM.v1",
        "name": "stablelm-base-alpha-3b",
    },
    "task": {"@llm_tasks": "spacy.NoOp.v1"},
    "save_io": True,
}

_NLP_CONFIG = """
[nlp]
lang = "en"
pipeline = ["llm"]
batch_size = 128

[components]

[components.llm]
factory = "llm"

[components.llm.task]
@llm_tasks = "spacy.NoOp.v1"

[components.llm.model]
@llm_models = "spacy.StableLM.v1"
name = "stablelm-base-alpha-3b"
"""


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
@pytest.mark.parametrize("name", ("stablelm-base-alpha-3b", "stablelm-tuned-alpha-3b"))
def test_init(name: str):
    """Test initialization and simple run.
    name (str): Name of model to run.
    """
    nlp = spacy.blank("en")
    cfg = copy.deepcopy(_PIPE_CFG)
    cfg["model"]["name"] = name  # type: ignore[index]
    nlp.add_pipe("llm", config=cfg)
    doc = nlp("This is a test.")
    torch.cuda.empty_cache()
    assert not doc.user_data["llm_io"]["llm"]["response"][0].startswith(
        doc.user_data["llm_io"]["llm"]["prompt"][0]
    )


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_init_from_config():
    orig_config = Config().from_str(_NLP_CONFIG)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]
    torch.cuda.empty_cache()


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_init_with_set_config():
    """Test initialization and simple run with changed config."""
    nlp = spacy.blank("en")
    cfg = copy.deepcopy(_PIPE_CFG)
    cfg["model"]["config_run"] = {"temperature": 0.3}
    nlp.add_pipe("llm", config=cfg)
    nlp("This is a test.")
    torch.cuda.empty_cache()


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_invalid_model():
    orig_config = Config().from_str(_NLP_CONFIG)
    config = copy.deepcopy(orig_config)
    config["components"]["llm"]["model"]["name"] = "anything-else"
    with pytest.raises(ValueError, match="unexpected value; permitted:"):
        spacy.util.load_model_from_config(config, auto_fill=True)



================================================
FILE: spacy_llm/tests/pipeline/__init__.py
================================================
[Empty file]


================================================
FILE: spacy_llm/tests/pipeline/test_llm.py
================================================
import logging
import sys
import warnings
from pathlib import Path
from typing import Any, Dict, Iterable, Optional, Tuple

import pytest
import spacy
import srsly
from confection import Config
from spacy.language import Language
from spacy.tokens import Doc
from spacy.util import make_tempdir
from thinc.api import NumpyOps, get_current_ops

import spacy_llm
from spacy_llm.models.rest.noop import _NOOP_RESPONSE
from spacy_llm.pipeline import LLMWrapper
from spacy_llm.registry import registry
from spacy_llm.tasks import _LATEST_TASKS, make_noop_task
from spacy_llm.tasks.noop import _NOOP_PROMPT, ShardingNoopTask

from ...cache import BatchCache
from ...registry.reader import fewshot_reader
from ...util import assemble_from_config
from ..compat import has_openai_key
from ..tasks.test_entity_linker import build_el_pipeline


@pytest.fixture
def noop_config() -> Dict[str, Any]:
    """Returns NoOp config.
    RETURNS (Dict[str, Any]): NoOp config.
    """
    return {
        "save_io": True,
        "task": {"@llm_tasks": "spacy.NoOp.v1"},
        "model": {"@llm_models": "spacy.NoOp.v1"},
    }


@pytest.fixture
def nlp(noop_config) -> Language:
    nlp = spacy.blank("en")
    nlp.add_pipe("llm", config=noop_config)
    return nlp


def test_llm_init(nlp):
    """Test pipeline intialization."""
    assert ["llm"] == nlp.pipe_names


@pytest.mark.parametrize("n_process", [1, 2])
@pytest.mark.parametrize("shard", [True, False])
def test_llm_pipe(noop_config: Dict[str, Any], n_process: int, shard: bool):
    """Test call .pipe()."""
    nlp = spacy.blank("en")
    nlp.add_pipe(
        "llm",
        config={**noop_config, **{"task": {"@llm_tasks": "spacy.NoOpNoShards.v1"}}}
        if not shard
        else noop_config,
    )
    ops = get_current_ops()
    if not isinstance(ops, NumpyOps) and n_process != 1:
        pytest.skip("Only test multiple processes on CPU")

    docs = list(
        nlp.pipe(texts=["This is a test", "This is another test"], n_process=n_process)
    )
    assert len(docs) == 2

    for doc in docs:
        llm_io = doc.user_data["llm_io"]
        assert llm_io["llm"]["prompt"] == ([_NOOP_PROMPT] if shard else _NOOP_PROMPT)
        assert llm_io["llm"]["response"] == (
            [_NOOP_RESPONSE] if shard else _NOOP_RESPONSE
        )


@pytest.mark.parametrize("n_process", [2])
def test_llm_pipe_with_cache(tmp_path: Path, n_process: int):
    """Test call .pipe() with pre-cached docs"""
    ops = get_current_ops()
    if not isinstance(ops, NumpyOps) and n_process != 1:
        pytest.skip("Only test multiple processes on CPU")

    path = tmp_path / "cache"

    config = {
        "task": {"@llm_tasks": "spacy.NoOp.v1"},
        "model": {"@llm_models": "spacy.NoOp.v1"},
        "cache": {
            "path": str(path),
            "batch_size": 1,  # Eager caching
            "max_batches_in_mem": 10,
        },
    }

    nlp = spacy.blank("en")
    nlp.add_pipe("llm", config=config)

    cached_text = "This is a cached test"

    # Run the text through, caching it.
    nlp(cached_text)

    texts = [cached_text, "This is a test", "This is another test"]

    # Run it again, along with other documents
    docs = list(nlp.pipe(texts=texts, n_process=n_process))
    assert [doc.text for doc in docs] == texts

    egs = [(text, i) for i, text in enumerate(texts)]
    egs_processed = list(nlp.pipe(egs, as_tuples=True, n_process=n_process))
    assert [doc.text for doc, _ in egs_processed] == texts
    assert [eg for _, eg in egs_processed] == list(range(len(texts)))


def test_llm_pipe_empty(nlp):
    """Test call .pipe() with empty batch."""
    assert list(nlp.pipe(texts=[])) == []


def test_llm_serialize_bytes():
    with pytest.warns(UserWarning, match="Task supports sharding"):
        llm = LLMWrapper(
            task=make_noop_task(),
            save_io=False,
            model=None,  # type: ignore
            cache=BatchCache(path=None, batch_size=0, max_batches_in_mem=0),
            vocab=None,  # type: ignore
        )
    llm.from_bytes(llm.to_bytes())


def test_llm_serialize_disk():
    with pytest.warns(UserWarning, match="Task supports sharding"):
        llm = LLMWrapper(
            task=make_noop_task(),
            save_io=False,
            model=None,  # type: ignore
            cache=BatchCache(path=None, batch_size=0, max_batches_in_mem=0),
            vocab=None,  # type: ignore
        )

    with spacy.util.make_tempdir() as tmp_dir:
        llm.to_disk(tmp_dir / "llm")
        llm.from_disk(tmp_dir / "llm")


@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.external
def test_type_checking_valid(noop_config) -> None:
    """Test type checking for consistency between functions."""
    # Ensure default config doesn't raise warnings.
    nlp = spacy.blank("en")
    with warnings.catch_warnings():
        warnings.simplefilter("error")
        nlp.add_pipe("llm", config={"task": {"@llm_tasks": "spacy.NoOp.v1"}})


def test_type_checking_invalid(noop_config) -> None:
    """Test type checking for consistency between functions."""

    @registry.llm_tasks("IncorrectTypes.v1")
    class NoopTask_Incorrect:
        def __init__(self):
            pass

        def generate_prompts(
            self, docs: Iterable[Doc], context_length: Optional[int] = None
        ) -> Iterable[Tuple[Iterable[int], Iterable[Doc]]]:
            for doc in docs:
                yield [0], [doc]

        def parse_responses(
            self, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[float]]
        ) -> Iterable[Doc]:
            return list(shards)[0]

    nlp = spacy.blank("en")
    with pytest.warns(UserWarning) as record:
        noop_config["task"] = {"@llm_tasks": "IncorrectTypes.v1"}
        nlp.add_pipe("llm", config=noop_config)
    assert len(record) == 2
    assert (
        str(record[0].message)
        == "First type in value returned from `task.generate_prompts()` (`typing.Iterable[int]`) doesn't match type "
        "expected by `model` (`typing.Iterable[str]`)."
    )
    assert (
        str(record[1].message)
        == "Type returned from `model` (`typing.Iterable[typing.Iterable[str]]`) doesn't match type expected by "
        "`task.parse_responses()` (`typing.Iterable[typing.Iterable[float]]`)."
    )

    # Run with disabled type consistency validation.
    nlp = spacy.blank("en")
    noop_config["validate_types"] = False
    nlp.add_pipe("llm", config=noop_config)


@pytest.mark.parametrize("use_pipe", [True, False])
def test_llm_logs_at_debug_level(
    nlp: Language, use_pipe: bool, caplog: pytest.LogCaptureFixture
):
    with caplog.at_level(logging.INFO):
        if use_pipe:
            doc = next(nlp.pipe(["This is a test"]))
        else:
            doc = nlp("This is a test")

    assert "spacy_llm" not in caplog.text
    assert doc.text not in caplog.text

    with caplog.at_level(logging.DEBUG):
        if use_pipe:
            doc = next(nlp.pipe(["This is a test"]))
        else:
            doc = nlp("This is a test")

    assert "spacy_llm" in caplog.text
    assert doc.text in caplog.text

    assert f"Generated prompt for doc: {doc.text}" in caplog.text
    assert "Don't do anything" in caplog.text
    assert f"LLM response for doc: {doc.text}" in caplog.text


def test_llm_logs_default_null_handler(nlp: Language, capsys: pytest.CaptureFixture):
    nlp("This is a test")

    captured = capsys.readouterr()
    assert captured.out == ""
    assert captured.err == ""

    # Add a basic Stream Handler
    stream_handler = logging.StreamHandler(sys.stdout)
    spacy_llm.logger.addHandler(stream_handler)
    spacy_llm.logger.setLevel(logging.DEBUG)

    doc = nlp("This is a test")
    captured = capsys.readouterr()
    assert f"Generated prompt for doc: {doc.text}" in captured.out
    assert "Don't do anything" in captured.out
    assert f"LLM response for doc: {doc.text}" in captured.out

    # Remove the Stream Handler from the spacy_llm logger
    spacy_llm.logger.removeHandler(stream_handler)

    doc = nlp("This is a test with no handler")
    captured = capsys.readouterr()
    assert f"Generated prompt for doc: {doc.text}" not in captured.out
    assert "Don't do anything" not in captured.out
    assert f"LLM response for doc: {doc.text}" not in captured.out


def test_fewshot_reader_file_format_handling():
    """Test if fewshot reader copes with file formats as expected."""
    example = [
        {
            "text": "Circe lived on Aeaea.",
            "entities": {"PER": ["Circe"], "LOC": ["Aeaea"]},
        }
    ]
    with make_tempdir() as tmpdir:
        srsly.write_yaml(tmpdir / "example.yml", example)
        srsly.write_yaml(tmpdir / "example.json", example)
        srsly.write_yaml(tmpdir / "example.foo", example)

        fewshot_reader(tmpdir / "example.yml")
        fewshot_reader(tmpdir / "example.json")
        fewshot_reader(tmpdir / "example.foo")


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_pipe_labels():
    """Test pipe labels with serde."""

    cfg_string = """
    [nlp]
    lang = "en"
    pipeline = ["llm"]

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.TextCat.v2"
    labels = ["COMPLIMENT", "INSULT"]

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """

    config = Config().from_str(cfg_string)
    nlp = assemble_from_config(config)

    with spacy.util.make_tempdir() as tmpdir:
        nlp.to_disk(tmpdir / "tst.nlp")
        nlp = spacy.load(tmpdir / "tst.nlp")
        assert nlp.pipe_labels["llm"] == ["COMPLIMENT", "INSULT"]


def test_llm_task_factories():
    """Test whether llm_TASK factories run successfully."""
    for task_handle in _LATEST_TASKS:
        # Separate test for EntityLinker in test_llm_task_factories_el().
        if "EntityLinker" in task_handle:
            continue

        cfg_string = f"""
        [nlp]
        lang = "en"
        pipeline = ["llm"]

        [components]

        [components.llm]
        factory = "llm_{task_handle.split('.')[1].lower()}"

        [components.llm.model]
        @llm_models = "test.NoOpModel.v1"
        """
        config = Config().from_str(cfg_string)

        # Translation task is expected to require a target language.
        if "Translation" in task_handle:
            config["components"]["llm"]["task"] = {"target_lang": "Spanish"}

        with pytest.warns(UserWarning, match="Task supports sharding"):
            assemble_from_config(config)


def test_llm_task_factories_el(tmp_path):
    """Test whether llm_entity_linking factory runs successfully. It's necessary to do this separately, as the EL task
    requires a non-defaultable extra config setup and knowledge base."""
    cfg = """
    [paths]
    el_nlp = null
    el_kb = null
    el_desc = null

    [nlp]
    lang = "en"
    pipeline = ["llm"]

    [components]

    [components.llm]
    factory = "llm_entitylinker"

    [components.llm.model]
    @llm_models = "test.NoOpModel.v1"

    [components.llm.task]
    @llm_tasks = "spacy.EntityLinker.v1"

    [initialize]
    [initialize.components]
    [initialize.components.llm]

    [initialize.components.llm.candidate_selector]
    @llm_misc = "spacy.CandidateSelector.v1"

    [initialize.components.llm.candidate_selector.kb_loader]
    @llm_misc = "spacy.KBObjectLoader.v1"
    path = ${paths.el_kb}
    nlp_path = ${paths.el_nlp}
    desc_path = ${paths.el_desc}
    """
    config = Config().from_str(
        cfg,
        overrides={
            "paths.el_nlp": str(tmp_path),
            "paths.el_kb": str(tmp_path / "entity_linker" / "kb"),
            "paths.el_desc": str(tmp_path / "desc.csv"),
        },
    )
    build_el_pipeline(nlp_path=tmp_path, desc_path=tmp_path / "desc.csv")
    with pytest.warns(UserWarning, match="Task supports sharding"):
        assemble_from_config(config)


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_llm_task_factories_ner():
    """Test whether llm_ner behaves as expected."""
    cfg_string = """
    [nlp]
    lang = "en"
    pipeline = ["llm"]

    [components]

    [components.llm]
    factory = "llm_ner"

    [components.llm.task]
    labels = PER,ORG,LOC

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v1"
    """
    config = Config().from_str(cfg_string)
    nlp = assemble_from_config(config)
    text = "Marc and Bob both live in Ireland."
    doc = nlp(text)

    assert len(doc.ents) > 0
    for ent in doc.ents:
        assert ent.label_ in ["PER", "ORG", "LOC"]


@pytest.mark.parametrize("shard", [True, False])
def test_llm_custom_data(noop_config: Dict[str, Any], shard: bool):
    """Test whether custom doc data is preserved."""
    nlp = spacy.blank("en")
    nlp.add_pipe(
        "llm",
        config={**noop_config, **{"task": {"@llm_tasks": "spacy.NoOpNoShards.v1"}}}
        if not shard
        else noop_config,
    )

    doc = nlp.make_doc("This is a test")
    if not Doc.has_extension("test"):
        Doc.set_extension("test", default=None)
    doc._.test = "Test"
    doc.user_data["test"] = "Test"

    doc = nlp(doc)
    assert doc._.test == "Test"
    assert doc.user_data["test"] == "Test"


def test_llm_custom_data_overwrite(noop_config: Dict[str, Any]):
    """Test whether custom doc data is overwritten as expected."""

    class NoopTaskWithCustomData(ShardingNoopTask):
        def parse_responses(
            self, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[str]]
        ) -> Iterable[Doc]:
            docs = super().parse_responses(shards, responses)
            for doc in docs:
                doc._.test = "Test 2"
                doc.user_data["test"] = "Test 2"
            return docs

    @registry.llm_tasks("spacy.NoOpCustomData.v1")
    def make_noopnoshards_task():
        return NoopTaskWithCustomData()

    nlp = spacy.blank("en")
    nlp.add_pipe(
        "llm",
        config={**noop_config, **{"task": {"@llm_tasks": "spacy.NoOpCustomData.v1"}}},
    )
    doc = nlp.make_doc("This is a test")
    for extension in ("test", "test_nooverwrite"):
        if not Doc.has_extension(extension):
            Doc.set_extension(extension, default=None)
    doc._.test = "Test"
    doc._.test_nooverwrite = "Test"
    doc.user_data["test"] = "Test"
    doc.user_data["test_nooverwrite"] = "Test"

    doc = nlp(doc)
    assert doc._.test == "Test 2"
    assert doc.user_data["test"] == "Test 2"
    assert doc._.test_nooverwrite == "Test"
    assert doc.user_data["test_nooverwrite"] == "Test"



================================================
FILE: spacy_llm/tests/sharding/__init__.py
================================================
[Empty file]


================================================
FILE: spacy_llm/tests/sharding/test_sharding.py
================================================
import numbers
from pathlib import Path

import pytest
from confection import Config
from spacy.pipeline import EntityLinker
from spacy.tokens import Span

from spacy_llm.tests.compat import has_openai_key
from spacy_llm.util import assemble_from_config

from .util import ShardingCountTask  # noqa: F401

_CONTEXT_LENGTH = 20
_TEXT = "Do one thing every day that scares you. The only thing we have to fear is fear itself."


@pytest.fixture
def config():
    return Config().from_str(
        f"""
            [nlp]
            lang = "en"
            pipeline = ["llm"]

            [components]

            [components.llm]
            factory = "llm"
            save_io = True

            [components.llm.task]
            @llm_tasks = "spacy.CountWithSharding.v1"

            [components.llm.model]
            @llm_models = "spacy.GPT-3-5.v3"
            context_length = {_CONTEXT_LENGTH}
        """
    )


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_sharding_count(config):
    """Tests whether task shards data as expected."""
    nlp = assemble_from_config(config)

    doc = nlp(_TEXT)
    marker = "(and nothing else): '"
    prompts = [
        pr[pr.index(marker) + len(marker) : -1]
        for pr in doc.user_data["llm_io"]["llm"]["prompt"]
    ]
    responses = [int(r) for r in doc.user_data["llm_io"]["llm"]["response"]]
    assert prompts == [
        "Do one thing every day ",
        "that scares you",
        ". The only ",
        "thing we have to ",
        "fear is fear itself.",
    ]
    assert all(
        [response == len(pr.split()) for response, pr in zip(responses, prompts)]
    )
    assert sum(responses) == doc.user_data["count"]


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_sharding_lemma(config):
    context_length = 120
    config["components"]["llm"]["model"]["context_length"] = context_length
    config["components"]["llm"]["task"] = {"@llm_tasks": "spacy.Lemma.v1"}
    nlp = assemble_from_config(config)

    doc = nlp(_TEXT)
    marker = "to be lemmatized:\n'''\n"
    prompts = [
        pr[pr.index(marker) + len(marker) : -4]
        for pr in doc.user_data["llm_io"]["llm"]["prompt"]
    ]
    # Make sure lemmas are set (somme might not be because the LLM didn't return parsable a response).
    assert any([t.lemma != 0 for t in doc])
    assert prompts == [
        "Do one thing every day that scares you. The ",
        "only thing we have to fear is fear itself.",
    ]
    assert len(doc.user_data["llm_io"]["llm"]["response"]) == 2


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_sharding_ner(config):
    context_length = 265
    config["components"]["llm"]["model"]["context_length"] = context_length
    config["components"]["llm"]["task"] = {
        "@llm_tasks": "spacy.NER.v3",
        "labels": ["LOCATION"],
    }
    nlp = assemble_from_config(config)

    doc = nlp(_TEXT + " Paris is a city.")
    marker = "Paragraph: "
    prompts = [
        pr[pr.rindex(marker) + len(marker) : pr.rindex("\nAnswer:")]
        for pr in doc.user_data["llm_io"]["llm"]["prompt"]
    ]
    assert len(doc.ents)
    assert prompts == [
        "Do one thing every day that scares you. The only thing ",
        "we have to fear is fear itself. Paris is a city.",
    ]
    assert len(doc.user_data["llm_io"]["llm"]["response"]) == 2


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_sharding_rel(config):
    context_length = 100
    config["nlp"]["pipeline"] = ["ner", "llm"]
    config["components"]["ner"] = {"source": "en_core_web_md"}
    config["components"]["llm"]["model"]["context_length"] = context_length
    config["components"]["llm"]["task"] = {
        "@llm_tasks": "spacy.REL.v1",
        "labels": "LivesIn,Visits",
    }
    config["initialize"] = {"vectors": "en_core_web_md"}
    nlp = assemble_from_config(config)

    doc = nlp("Joey rents a place in New York City, which is in North America.")
    marker = "Text:\n'''\n"
    prompts = [
        pr[pr.rindex(marker) + len(marker) : -4]
        for pr in doc.user_data["llm_io"]["llm"]["prompt"]
    ]
    assert len(doc.ents)
    assert hasattr(doc._, "rel") and len(doc._.rel)
    assert prompts == [
        "Joey[ENT0:PERSON] rents a place in New York City",
        "[ENT1:GPE], which is in North America[ENT2:LOC].",
    ]
    assert len(doc.user_data["llm_io"]["llm"]["response"]) == 2


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_sharding_sentiment(config):
    context_length = 50
    config["components"]["llm"]["model"]["context_length"] = context_length
    config["components"]["llm"]["task"] = {"@llm_tasks": "spacy.Sentiment.v1"}
    nlp = assemble_from_config(config)

    doc = nlp(_TEXT)
    marker = "Text:\n'''\n"
    prompts = [
        pr[pr.index(marker) + len(marker) : pr.rindex("\n'''\nAnswer:")]
        for pr in doc.user_data["llm_io"]["llm"]["prompt"]
    ]
    assert hasattr(doc._, "sentiment") and isinstance(doc._.sentiment, numbers.Number)
    assert prompts == [
        "Do one thing every day that scares you. The ",
        "only thing we have to fear is fear itself.",
    ]
    assert len(doc.user_data["llm_io"]["llm"]["response"]) == 2


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_sharding_spancat(config):
    context_length = 265
    config["components"]["llm"]["model"]["context_length"] = context_length
    config["components"]["llm"]["task"] = {
        "@llm_tasks": "spacy.SpanCat.v3",
        "labels": ["LOCATION"],
    }
    nlp = assemble_from_config(config)

    doc = nlp(_TEXT + " Paris is a city.")
    marker = "Paragraph: "
    prompts = [
        pr[pr.rindex(marker) + len(marker) : pr.rindex("\nAnswer:")]
        for pr in doc.user_data["llm_io"]["llm"]["prompt"]
    ]
    assert len(doc.spans.data["sc"])
    assert prompts == [
        "Do one thing every day that ",
        "scares you. The only thing we have to ",
        "fear is fear itself. Paris is a city.",
    ]
    assert len(doc.user_data["llm_io"]["llm"]["response"]) == 3


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_sharding_summary(config):
    context_length = 50
    config["components"]["llm"]["model"]["context_length"] = context_length
    config["components"]["llm"]["task"] = {"@llm_tasks": "spacy.Summarization.v1"}
    nlp = assemble_from_config(config)

    doc = nlp(_TEXT)
    marker = "needs to be summarized:\n'''\n"
    prompts = [
        pr[pr.rindex(marker) + len(marker) : pr.rindex("\n'''\nSummary:")]
        for pr in doc.user_data["llm_io"]["llm"]["prompt"]
    ]
    assert hasattr(doc._, "summary") and doc._.summary
    assert prompts == [
        "Do one thing every day that scares you. The ",
        "only thing we have to fear is fear itself.",
    ]
    assert len(doc.user_data["llm_io"]["llm"]["response"]) == 2


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_sharding_textcat(config):
    context_length = 100
    config["components"]["llm"]["model"]["context_length"] = context_length
    config["components"]["llm"]["task"] = {
        "@llm_tasks": "spacy.TextCat.v3",
        "labels": "RECIPE",
        "exclusive_classes": True,
    }
    nlp = assemble_from_config(config)

    doc = nlp(
        "Fry an egg in a pan. Scramble it. Add some salt, pepper and truffle oil."
    )
    marker = "Text:\n'''\n"
    prompts = [
        pr[pr.rindex(marker) + len(marker) : -4]
        for pr in doc.user_data["llm_io"]["llm"]["prompt"]
    ]
    assert len(doc.cats) == 1 and "RECIPE" in doc.cats
    assert prompts == [
        "Fry an egg in ",
        "a pan. Scramble it. Add ",
        "some salt, pepper and truffle oil.",
    ]
    assert len(doc.user_data["llm_io"]["llm"]["response"]) == 3


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_sharding_entity_linker(config):
    context_length = 290
    config["components"]["llm"]["model"]["context_length"] = context_length
    config["components"]["llm"]["task"] = {"@llm_tasks": "spacy.EntityLinker.v1"}
    config["initialize"] = {
        "components": {
            "llm": {
                "candidate_selector": {
                    "@llm_misc": "spacy.CandidateSelector.v1",
                    "kb_loader": {
                        "@llm_misc": "spacy.KBFileLoader.v1",
                        "path": "${paths.el_kb}",
                    },
                }
            }
        }
    }
    config["paths"] = {
        "el_kb": str(
            Path(__file__).resolve().parent.parent / "tasks" / "misc" / "el_kb_data.yml"
        )
    }
    nlp = assemble_from_config(config)

    doc = nlp.make_doc("Alice goes to Boston to see the Boston Celtics game.")
    doc.ents = [
        Span(doc=doc, start=3, end=4, label="LOC"),  # Q100
        Span(doc=doc, start=7, end=9, label="ORG"),  # Q131371
    ]
    doc = nlp(doc)
    marker = "TEXT: \n'''\n"
    prompts = [
        pr[pr.rindex(marker) + len(marker) : pr.rindex("\n'''")]
        for pr in doc.user_data["llm_io"]["llm"]["prompt"]
    ]
    assert len(doc.ents) == 2
    assert all([ent.kb_id_ != EntityLinker.NIL for ent in doc.ents])
    assert prompts == ["Alice goes to *Boston* to ", "see the *Boston Celtics* game."]
    assert len(doc.user_data["llm_io"]["llm"]["response"]) == 2


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_sharding_raw(config):
    context_length = 20
    config["components"]["llm"]["model"]["context_length"] = context_length
    config["components"]["llm"]["task"] = {"@llm_tasks": "spacy.Raw.v1"}
    nlp = assemble_from_config(config)

    doc = nlp(_TEXT)
    marker = "Text:\n"
    prompts = [
        pr[pr.rindex(marker) + len(marker) : pr.rindex("\nReply:")]
        for pr in doc.user_data["llm_io"]["llm"]["prompt"]
    ]
    assert hasattr(doc._, "llm_reply") and doc._.llm_reply
    assert prompts == [
        "Do one thing every day that scares you. The ",
        "only thing we have to fear is fear itself.",
    ]
    assert len(doc.user_data["llm_io"]["llm"]["response"]) == 2


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_sharding_translation(config):
    context_length = 30
    config["components"]["llm"]["model"]["context_length"] = context_length
    config["components"]["llm"]["task"] = {
        "@llm_tasks": "spacy.Translation.v1",
        "target_lang": "Spanish",
    }
    nlp = assemble_from_config(config)

    doc = nlp(_TEXT)
    marker = "Text:\n"
    prompts = [
        pr[pr.rindex(marker) + len(marker) : pr.rindex("Translation:")].strip()
        for pr in doc.user_data["llm_io"]["llm"]["prompt"]
    ]
    assert hasattr(doc._, "translation") and doc._.translation
    assert prompts == [
        "Do one thing every day that scares you. The",
        "only thing we have to fear is fear itself.",
    ]
    assert len(doc.user_data["llm_io"]["llm"]["response"]) == 2



================================================
FILE: spacy_llm/tests/sharding/util.py
================================================
import warnings
from typing import Iterable, List, Optional

from spacy.tokens import Doc
from spacy.training import Example

from spacy_llm.compat import Self
from spacy_llm.registry import registry
from spacy_llm.tasks import BuiltinTask
from spacy_llm.tasks.util.sharding import make_shard_mapper
from spacy_llm.ty import FewshotExample, ShardReducer


def parse_responses(
    task: "ShardingCountTask",
    shards: Iterable[Iterable[Doc]],
    responses: Iterable[Iterable[str]],
) -> Iterable[Iterable[int]]:
    for responses_for_doc, shards_for_doc in zip(responses, shards):
        results_for_doc: List[int] = []
        for response, shard in zip(responses_for_doc, shards_for_doc):
            results_for_doc.append(int(response))

        yield results_for_doc


def reduce_shards_to_doc(task: "ShardingCountExample", shards: Iterable[Doc]) -> Doc:
    shards = list(shards)
    with warnings.catch_warnings():
        warnings.filterwarnings(
            "ignore",
            category=UserWarning,
            message=".*Skipping unsupported user data",
        )
        doc = Doc.from_docs(shards, ensure_whitespace=True)
    doc.user_data["count"] = sum([shard.user_data["count"] for shard in shards])
    return doc


class ShardingCountExample(FewshotExample):
    @classmethod
    def generate(cls, example: Example, task: "ShardingCountTask") -> Optional[Self]:
        return None


@registry.llm_tasks("spacy.CountWithSharding.v1")
class ShardingCountTask(BuiltinTask):
    _PROMPT_TEMPLATE = (
        "Reply with the number of words in this string (and nothing else): '{{ text }}'"
    )

    def __init__(self):
        assert isinstance(reduce_shards_to_doc, ShardReducer)
        super().__init__(
            parse_responses=parse_responses,
            prompt_example_type=ShardingCountExample,
            template=self._PROMPT_TEMPLATE,
            prompt_examples=[],
            shard_mapper=make_shard_mapper(),
            shard_reducer=reduce_shards_to_doc,
        )

    def parse_responses(
        self, shards: Iterable[Iterable[Doc]], responses: Iterable[Iterable[str]]
    ) -> Iterable[Doc]:
        shards_teed = self._tee_2d_iterable(shards, 2)

        for shards_for_doc, counts_for_doc in zip(
            shards_teed[0], self._parse_responses(self, shards_teed[1], responses)
        ):
            shards_for_doc = list(shards_for_doc)
            for shard, count in zip(shards_for_doc, counts_for_doc):
                shard.user_data["count"] = count

            yield self._shard_reducer(self, shards_for_doc)  # type: ignore[arg-type]

    @property
    def prompt_template(self) -> str:
        return self._PROMPT_TEMPLATE

    @property
    def _cfg_keys(self) -> List[str]:
        return []



================================================
FILE: spacy_llm/tests/tasks/__init__.py
================================================
[Empty file]


================================================
FILE: spacy_llm/tests/tasks/custom.cfg
================================================
[nlp]
lang = "en"
pipeline = ["llm"]

[components]

[components.llm]
factory = "llm"
save_io = True

[components.llm.task]
@llm_tasks = "my_namespace.MyTask.v1"

[components.llm.model]
@llm_models = "spacy.NoOp.v1"


================================================
FILE: spacy_llm/tests/tasks/test_custom.py
================================================
from typing import Iterable

from confection import Config
from spacy.tokens import Doc

from spacy_llm.registry import registry
from spacy_llm.util import assemble_from_config

config = """[nlp]
lang = "en"
pipeline = ["llm"]

[components]

[components.llm]
factory = "llm"
save_io = True

[components.llm.task]
@llm_tasks = "my_namespace.MyTask.v1"

[components.llm.model]
@llm_models = "spacy.NoOp.v1"
"""


@registry.llm_tasks("my_namespace.MyTask.v1")
def make_my_task() -> "MyTask":
    return MyTask()


class MyTask:
    def __init__(self):
        self._template = "Do a sumersault"

    def generate_prompts(self, docs: Iterable[Doc]) -> Iterable[str]:
        for doc in docs:
            yield self._template

    def parse_responses(
        self, docs: Iterable[Doc], responses: Iterable[str]
    ) -> Iterable[Doc]:
        return docs


def test_custom_task():
    nlp = assemble_from_config(Config().from_str(config))
    nlp("This is a test.")



================================================
FILE: spacy_llm/tests/tasks/test_entity_linker.py
================================================
# ruff: noqa: W291
import csv
import functools
from pathlib import Path

import numpy
import pytest
import spacy
import srsly
from confection import Config
from spacy import Vocab
from spacy.kb import InMemoryLookupKB
from spacy.pipeline import EntityLinker
from spacy.tokens import Doc, Span
from spacy.training import Example
from spacy.util import make_tempdir

from spacy_llm.registry import fewshot_reader, file_reader
from spacy_llm.tasks.entity_linker import EntityLinkerTask, make_entitylinker_task
from spacy_llm.util import assemble_from_config

from ...tasks.entity_linker.candidate_selector import KBCandidateSelector
from ...tasks.entity_linker.registry import make_candidate_selector_pipeline
from ...tasks.entity_linker.registry import make_kb_object_loader
from ...tasks.entity_linker.util import UNAVAILABLE_ENTITY_DESC, KBFileLoader
from ..compat import has_openai_key

EXAMPLES_DIR = Path(__file__).parent / "examples"
TEMPLATES_DIR = Path(__file__).parent / "templates"


@functools.lru_cache()
def build_el_pipeline(nlp_path: Path, desc_path: Path) -> None:
    """Builds and persists pipeline with untrained EL component and initialized toy knowledge base.
    nlp_path (Path): Path to store pipeline under.
    desc_path (Path): Path to store descriptions file under.
    """
    nlp = spacy.load("en_core_web_md")
    nlp.add_pipe("entity_linker")
    kb = InMemoryLookupKB(
        vocab=nlp.vocab, entity_vector_length=nlp.vocab.vectors_length
    )

    # Define entities.
    kb_data = srsly.read_yaml(
        Path(__file__).resolve().parent / "misc" / "el_kb_data.yml"
    )
    entities = kb_data["entities"]
    qids = list(entities.keys())

    # Set entities with dummy values for embeddings and frequencies.
    vec_shape = spacy.load("en_core_web_md")(" ").vector.shape
    kb.set_entities(
        entity_list=qids,
        vector_list=[numpy.zeros(vec_shape)] * len(qids),
        freq_list=[1] * len(qids),
    )

    # Add aliases and dummy prior probabilities.
    for alias_data in kb_data["aliases"]:
        kb.add_alias(**alias_data)

    # Set KB in pipeline, persist.
    def load_kb(vocab: Vocab) -> InMemoryLookupKB:
        return kb

    nlp.get_pipe("entity_linker").set_kb(load_kb)
    nlp.to_disk(nlp_path)

    # Write descriptions to file.
    with open(desc_path, "w") as csvfile:
        csv_writer = csv.writer(csvfile, quoting=csv.QUOTE_ALL, delimiter=";")
        for qid, ent_desc in entities.items():
            csv_writer.writerow([qid, ent_desc["desc"]])


@pytest.fixture
def noop_config():
    return """
    [paths]
    el_nlp = null
    el_kb = null
    el_desc = null

    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.EntityLinker.v1"

    [components.llm.model]
    @llm_models = "test.NoOpModel.v1"

    [initialize]
    [initialize.components]
    [initialize.components.llm]

    [initialize.components.llm.candidate_selector]
    @llm_misc = "spacy.CandidateSelector.v1"

    [initialize.components.llm.candidate_selector.kb_loader]
    @llm_misc = "spacy.KBObjectLoader.v1"
    path = ${paths.el_kb}
    nlp_path = ${paths.el_nlp}
    desc_path = ${paths.el_desc}
    """


@pytest.fixture
def zeroshot_cfg_string():
    return """
    [paths]
    el_nlp = null
    el_kb = null
    el_desc = null

    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.EntityLinker.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v1"
    config = {"temperature": 0}

    [initialize]
    [initialize.components]
    [initialize.components.llm]

    [initialize.components.llm.candidate_selector]
    @llm_misc = "spacy.CandidateSelector.v1"

    [initialize.components.llm.candidate_selector.kb_loader]
    @llm_misc = "spacy.KBObjectLoader.v1"
    path = ${paths.el_kb}
    nlp_path = ${paths.el_nlp}
    desc_path = ${paths.el_desc}
    """


@pytest.fixture
def fewshot_cfg_string():
    return f"""
    [paths]
    el_nlp = null
    el_kb = null
    el_desc = null

    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.EntityLinker.v1"

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str((Path(__file__).parent / "examples" / "entity_linker.yml"))}

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v1"
    config = {{"temperature": 0}}

    [initialize]
    [initialize.components]
    [initialize.components.llm]

    [initialize.components.llm.candidate_selector]
    @llm_misc = "spacy.CandidateSelector.v1"

    [initialize.components.llm.candidate_selector.kb_loader]
    @llm_misc = "spacy.KBObjectLoader.v1"
    path = ${{paths.el_kb}}
    nlp_path = ${{paths.el_nlp}}
    desc_path = ${{paths.el_desc}}
    """


@pytest.fixture
def ext_template_cfg_string():
    """Simple zero-shot config with an external template"""

    return f"""
    [paths]
    el_nlp = null
    el_kb = null
    el_desc = null

    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]
    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.EntityLinker.v1"

    [components.llm.task.template]
    @misc = "spacy.FileReader.v1"
    path = {str((Path(__file__).parent / "templates" / "entity_linker.jinja2"))}

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v1"
    config = {{"temperature": 0}}

    [initialize]
    [initialize.components]
    [initialize.components.llm]

    [initialize.components.llm.candidate_selector]
    @llm_misc = "spacy.CandidateSelector.v1"

    [initialize.components.llm.candidate_selector.kb_loader]
    @llm_misc = "spacy.KBObjectLoader.v1"
    path = ${{paths.el_kb}}
    nlp_path = ${{paths.el_nlp}}
    desc_path = ${{paths.el_desc}}
    """


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "fewshot_cfg_string",
        "ext_template_cfg_string",
    ],
)
def test_entity_linker_config(cfg_string, request, tmp_path):
    cfg_string = request.getfixturevalue(cfg_string)
    config = Config().from_str(
        cfg_string,
        overrides={
            "paths.el_nlp": str(tmp_path),
            "paths.el_kb": str(tmp_path / "entity_linker" / "kb"),
            "paths.el_desc": str(tmp_path / "desc.csv"),
        },
    )
    build_el_pipeline(nlp_path=tmp_path, desc_path=tmp_path / "desc.csv")
    nlp = spacy.util.load_model_from_config(config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]

    # also test nlp config from a dict in add_pipe
    component_cfg = dict(config["components"]["llm"])
    component_cfg.pop("factory")

    nlp2 = spacy.blank("en")
    nlp2.add_pipe("llm", config=component_cfg)
    assert nlp2.pipe_names == ["llm"]


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "fewshot_cfg_string",
        "ext_template_cfg_string",
    ],
)
def test_entity_linker_predict(cfg_string, request, tmp_path):
    """Use OpenAI to get EL results."""
    cfg = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(
        cfg,
        overrides={
            "paths.el_nlp": str(tmp_path),
            "paths.el_kb": str(tmp_path / "entity_linker" / "kb"),
            "paths.el_desc": str(tmp_path / "desc.csv"),
        },
    )
    build_el_pipeline(nlp_path=tmp_path, desc_path=tmp_path / "desc.csv")
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    nlp.initialize(lambda: [])

    text = "Alice goes to Boston to see the Boston Celtics game."
    doc = nlp.make_doc(text)
    doc.ents = [
        Span(doc=doc, start=3, end=4, label="LOC"),  # Q100
        Span(doc=doc, start=7, end=9, label="ORG"),  # Q131371
    ]
    doc = nlp(doc)
    if cfg_string != "ext_template_cfg_string":
        assert len(doc.ents) == 2
        assert doc.ents[0].kb_id_ == "Q100"
        assert doc.ents[1].kb_id_ == "Q131371"


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_entity_linker_predict_no_candidates(request, tmp_path):
    """Test behavior if no candidates are available for certain mention."""
    cfg = request.getfixturevalue("zeroshot_cfg_string")
    orig_config = Config().from_str(
        cfg,
        overrides={
            "paths.el_nlp": str(tmp_path),
            "paths.el_kb": str(tmp_path / "entity_linker" / "kb"),
            "paths.el_desc": str(tmp_path / "desc.csv"),
            "components.llm.save_io": True,
        },
    )
    build_el_pipeline(nlp_path=tmp_path, desc_path=tmp_path / "desc.csv")
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    nlp.initialize(lambda: [])

    def make_doc() -> Doc:
        text = "Alice goes to Foo to see the Boston Celtics game."
        doc = nlp.make_doc(text)
        doc.ents = [
            Span(doc=doc, start=3, end=4, label="LOC"),  # NIL
            Span(doc=doc, start=7, end=9, label="ORG"),  # Q131371
        ]
        return doc

    # Run with auto-nil: Foo shouldn't appear in prompt. It should be set to NIL due to not having candidates in the KB.
    assert nlp.components[0][1]._task._auto_nil is True
    doc = nlp(make_doc())
    assert len(doc.ents) == 2
    assert "*Foo*" not in doc.user_data["llm_io"]["llm"]["prompt"]
    assert doc.ents[0].kb_id_ == EntityLinker.NIL
    assert doc.ents[1].kb_id_ == "Q131371"

    # Run without auto-nil: Foo should appear in prompt with no candidates and default "non-available" description.
    nlp.components[0][1]._task._auto_nil = False
    doc = nlp(make_doc())
    assert (
        f"- For *Foo*:\n    {EntityLinker.NIL}. {UNAVAILABLE_ENTITY_DESC}"
        in doc.user_data["llm_io"]["llm"]["prompt"][0]
    )
    assert doc.ents[0].kb_id_ == EntityLinker.NIL
    # Sometimes GPT-3.5 doesn't manage to include the NIL prediction, in which case all entities are set to NIL.
    assert doc.ents[1].kb_id_ in ("Q131371", EntityLinker.NIL)


@pytest.mark.external
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "fewshot_cfg_string",
    ],
)
def test_el_io(cfg_string, request, tmp_path):
    cfg = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(
        cfg,
        overrides={
            "paths.el_nlp": str(tmp_path),
            "paths.el_kb": str(tmp_path / "entity_linker" / "kb"),
            "paths.el_desc": str(tmp_path / "desc.csv"),
        },
    )
    build_el_pipeline(nlp_path=tmp_path, desc_path=tmp_path / "desc.csv")
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    nlp.initialize(lambda: [])

    assert nlp.pipe_names == ["llm"]
    # ensure you can save a pipeline to disk and run it after loading
    with make_tempdir() as tmpdir:
        nlp.to_disk(tmpdir)
        nlp2 = spacy.load(tmpdir)
    assert nlp2.pipe_names == ["llm"]
    nlp2.initialize(lambda: [])

    text = "Alice goes to Boston to see the Boston Celtics game."
    doc = nlp.make_doc(text)
    doc.ents = [
        Span(doc=doc, start=3, end=4, label="LOC"),  # Q100
        Span(doc=doc, start=7, end=9, label="ORG"),  # Q131371
    ]
    doc = nlp2(doc)
    if cfg_string != "ext_template_cfg_string":
        assert len(doc.ents) == 2
        assert doc.ents[0].kb_id_ == "Q100"
        assert doc.ents[1].kb_id_ == "Q131371"


def test_jinja_template_rendering_without_examples(tmp_path):
    """Test if jinja template renders as we expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    nlp = spacy.blank("en")
    text = "Alice goes to Boston to see the Boston Celtics game."
    doc = nlp.make_doc(text)
    doc.ents = [
        Span(doc=doc, start=3, end=4, label="LOC"),
        Span(doc=doc, start=7, end=9, label="ORG"),
    ]

    build_el_pipeline(nlp_path=tmp_path, desc_path=tmp_path / "desc.csv")
    el_task = make_entitylinker_task(examples=None)
    el_task._candidate_selector = make_candidate_selector_pipeline(
        kb_loader=make_kb_object_loader(
            path=tmp_path / "entity_linker" / "kb",
            nlp_path=tmp_path,
            desc_path=tmp_path / "desc.csv",
        )
    )
    el_task._candidate_selector.initialize(spacy.load(tmp_path).vocab)
    prompt = list(el_task.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip().replace(" \n", "\n")
        == """
For each of the MENTIONS in the TEXT, resolve the MENTION to the correct entity listed in ENTITIES.
Each of the ENTITIES is prefixed by its ENTITY ID. Each of the MENTIONS in the TEXT is surrounded by *.
For each of the MENTIONS appearing in the text, output the ID of the description fitting them best.
This ID has to be surrounded by single <>, for example <1>. Make sure you make a choice for each MENTION. If no
candidate seems plausible, respond with <NIL> instead of an ENTITY ID.
Output "REASONING:". Describe, step by step, which MENTION should be linked to which ENTITY ID.
Output "SOLUTION:". After that, list the correct ENTITY ID (or NIL) per MENTION. Wrap the ENTITY ID in <>. Each ENTITY ID
should be in a new line, prefixed by the corresponding MENTION and " ::: ".

TEXT:
'''
Alice goes to *Boston* to see the *Boston Celtics* game.
'''
MENTIONS: *Boston*, *Boston Celtics*
ENTITIES:
- For *Boston*:
    Q100. city in and state capital of Massachusetts, United States
    Q131371. NBA team based in Boston; tied with most NBA Championships
    Q204289. American rock band
    Q311975. town in Lincolnshire, England
    Q671475. airport in Boston, Massachusetts, United States
- For *Boston Celtics*:
    Q131371. NBA team based in Boston; tied with most NBA Championships
    Q107723060. The 2021â€“22 Boston Celtics season was the 76th season of the franchise in the National Basketball Association (NBA). Following the Celtics' first-round exit to the Brooklyn Nets in five games from las
    Q3643001. NBA basketball team season
    Q3466394. season of National Basketball Association team the Boston Celtics
    Q3642995. NBA basketball team season
""".strip().replace(
            " \n", "\n"
        )
    )


@pytest.mark.parametrize(
    "examples_path",
    [
        str(EXAMPLES_DIR / "entity_linker.json"),
        str(EXAMPLES_DIR / "entity_linker.yml"),
        str(EXAMPLES_DIR / "entity_linker.jsonl"),
    ],
)
def test_jinja_template_rendering_with_examples(examples_path, tmp_path):
    """Test if jinja2 template renders as expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    nlp = spacy.blank("en")
    text = "Alice goes to Boston to see the Boston Celtics game."
    doc = nlp.make_doc(text)
    doc.ents = [
        Span(doc=doc, start=3, end=4, label="LOC"),
        Span(doc=doc, start=7, end=9, label="ORG"),
    ]

    build_el_pipeline(nlp_path=tmp_path, desc_path=tmp_path / "desc.csv")
    el_task = make_entitylinker_task(examples=fewshot_reader(examples_path))
    el_task._candidate_selector = make_candidate_selector_pipeline(
        kb_loader=make_kb_object_loader(
            path=tmp_path / "entity_linker" / "kb",
            nlp_path=tmp_path,
            desc_path=tmp_path / "desc.csv",
        )
    )
    el_task._candidate_selector.initialize(spacy.load(tmp_path).vocab)
    prompt = list(el_task.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip().replace(" \n", "\n")
        == """
For each of the MENTIONS in the TEXT, resolve the MENTION to the correct entity listed in ENTITIES.
Each of the ENTITIES is prefixed by its ENTITY ID. Each of the MENTIONS in the TEXT is surrounded by *.
For each of the MENTIONS appearing in the text, output the ID of the description fitting them best.
This ID has to be surrounded by single <>, for example <1>. Make sure you make a choice for each MENTION. If no
candidate seems plausible, respond with <NIL> instead of an ENTITY ID.
Output "REASONING:". Describe, step by step, which MENTION should be linked to which ENTITY ID.
Output "SOLUTION:". After that, list the correct ENTITY ID (or NIL) per MENTION. Wrap the ENTITY ID in <>. Each ENTITY ID
should be in a new line, prefixed by the corresponding MENTION and " ::: ".

Below are some examples (only use these as a guide):

TEXT:
'''
Alice goes to *New York* to see the *New York Knicks* game.
'''
MENTIONS:
ENTITIES:
- For *New York*:
    Q60. most populous city in the United States
    Q1384. U.S. state
- For *New York Knicks*:
    Q60. most populous city in the United States
    Q131364. National Basketball Association team in New York City
REASONING:
- The description of the chosen entity Q60 fits the presented mention *New York* best.
- The description of the chosen entity Q131364 fits the presented mention *New York Knicks* best.
SOLUTION:
*New York* ::: <Q60>
*New York Knicks* ::: <Q131364>

TEXT:
'''
*New York* is called the *Big Apple*. It also has *Apple* stores.
'''
MENTIONS:
ENTITIES:
- For *New York*:
    Q60. most populous city in the United States
    Q1384. U.S. state
- For *Big Apple*:
    Q14435. nickname for New York City
    Q89. fruit of the apple tree
- For *Apple*:
    Q89. fruit of the apple tree
    Q312. American multinational technology company
REASONING:
- The mention of "Big Apple" in the same context clarifies that this is about the city New York.
- Big Apple is a well-known nickname of New York.
- The context of "stores" indicates that this is about the technology company Apple, which operates "Apple stores".

SOLUTION:
*New York* ::: <Q60>
*Big Apple* ::: <Q14435>
*Apple* ::: <Q312>


End of examples.TEXT:
'''
Alice goes to *Boston* to see the *Boston Celtics* game.
'''
MENTIONS: *Boston*, *Boston Celtics*
ENTITIES:
- For *Boston*:
    Q100. city in and state capital of Massachusetts, United States
    Q131371. NBA team based in Boston; tied with most NBA Championships
    Q204289. American rock band
    Q311975. town in Lincolnshire, England
    Q671475. airport in Boston, Massachusetts, United States
- For *Boston Celtics*:
    Q131371. NBA team based in Boston; tied with most NBA Championships
    Q107723060. The 2021â€“22 Boston Celtics season was the 76th season of the franchise in the National Basketball Association (NBA). Following the Celtics' first-round exit to the Brooklyn Nets in five games from las
    Q3643001. NBA basketball team season
    Q3466394. season of National Basketball Association team the Boston Celtics
    Q3642995. NBA basketball team season
""".strip().replace(
            " \n", "\n"
        )
    )


def test_external_template_actually_loads(tmp_path):
    template_path = str(TEMPLATES_DIR / "entity_linker.jinja2")
    template = file_reader(template_path)
    text = "Alice and Bob went to the supermarket"
    nlp = spacy.blank("en")
    doc = nlp.make_doc(text)

    build_el_pipeline(nlp_path=tmp_path, desc_path=tmp_path / "desc.csv")
    el_task = make_entitylinker_task(template=template, examples=None)
    el_task._candidate_selector = make_candidate_selector_pipeline(
        kb_loader=make_kb_object_loader(
            path=tmp_path / "entity_linker" / "kb",
            nlp_path=tmp_path,
            desc_path=tmp_path / "desc.csv",
        )
    )
    el_task._candidate_selector.initialize(spacy.load(tmp_path).vocab)

    assert (
        list(el_task.generate_prompts([doc]))[0][0][0].strip()
        == f"""
This is a test entity linking template.
Here is the text: {text}
""".strip()
    )


@pytest.mark.parametrize("n_prompt_examples", [-1, 0, 1, 2])
def test_el_init(noop_config, n_prompt_examples: int, tmp_path):
    config = Config().from_str(
        noop_config,
        overrides={
            "paths.el_nlp": str(tmp_path),
            "paths.el_kb": str(tmp_path / "entity_linker" / "kb"),
            "paths.el_desc": str(tmp_path / "desc.csv"),
        },
    )
    build_el_pipeline(nlp_path=tmp_path, desc_path=tmp_path / "desc.csv")
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp = assemble_from_config(config)

    examples = []

    text = "Alice goes to Boston to see the Boston Celtics game."
    pred_1 = nlp.make_doc(text)
    pred_1.ents = [
        Span(doc=pred_1, start=3, end=4, label="LOC"),
        Span(doc=pred_1, start=7, end=9, label="ORG"),
    ]
    gold_1 = nlp.make_doc(text)
    gold_1.ents = [
        Span(doc=gold_1, start=3, end=4, label="LOC", kb_id="Q100"),
        Span(doc=gold_1, start=7, end=9, label="ORG", kb_id="Q131371"),
    ]
    examples.append(Example(pred_1, gold_1))

    text = "Alice goes to New York to see the New York Knicks game."
    pred_2 = nlp.make_doc(text)
    pred_2.ents = [
        Span(doc=pred_2, start=3, end=4, label="LOC"),
        Span(doc=pred_2, start=7, end=10, label="ORG"),
    ]
    gold_2 = nlp.make_doc(text)
    gold_2.ents = [
        Span(doc=gold_2, start=3, end=4, label="LOC", kb_id="Q60"),
        Span(doc=gold_2, start=7, end=10, label="ORG", kb_id="Q131364"),
    ]
    examples.append(Example(pred_2, gold_2))

    _, llm = nlp.pipeline[0]
    task: EntityLinkerTask = llm._task

    assert not task._prompt_examples

    nlp.config["initialize"]["components"]["llm"] = {
        **nlp.config["initialize"]["components"]["llm"],
        "n_prompt_examples": n_prompt_examples,
    }
    nlp.initialize(lambda: examples)

    if n_prompt_examples >= 0:
        assert len(task._prompt_examples) == n_prompt_examples
    else:
        assert len(task._prompt_examples) == len(examples)


def test_ent_highlighting():
    """Tests highlighting of entities in text."""
    nlp = spacy.blank("en")
    text = "Alice goes to Boston to see the Boston Celtics game."
    doc = nlp.make_doc(text)
    doc.ents = [
        Span(doc=doc, start=3, end=4, label="LOC"),
        Span(doc=doc, start=7, end=9, label="ORG"),
    ]

    assert (
        EntityLinkerTask.highlight_ents_in_doc(doc).text
        == "Alice goes to *Boston* to see the *Boston Celtics* game."
    )


@pytest.mark.parametrize(
    "text,ents,include_ents",
    [
        (
            "Alice goes to Boston to see the Boston Celtics game.",
            [
                {"start": 3, "end": 4, "label": "LOC"},
                {"start": 7, "end": 9, "label": "ORG"},
            ],
            [True, True],
        ),
        (
            "I went to see Boston in concert yesterday",
            [
                {"start": 4, "end": 5, "label": "GPE"},
                {"start": 7, "end": 8, "label": "DATE"},
            ],
            [True, False],
        ),
    ],
)
def test_ent_unhighlighting(text, ents, include_ents):
    """Tests unhighlighting of entities in text."""
    nlp = spacy.blank("en")
    doc = nlp.make_doc(text)
    doc.ents = [Span(doc=doc, **ents[0]), Span(doc=doc, **ents[1])]

    assert (
        EntityLinkerTask.unhighlight_ents_in_doc(
            EntityLinkerTask.highlight_ents_in_doc(doc, include_ents)
        ).text
        == doc.text
        == text
    )


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize("loader", ("yaml", "serialized_no_nlp"))
def test_entity_linker_predict_alternative_kb_inits(loader, request, tmp_path):
    """Use OpenAI to get EL results with different KB initialization methods."""
    cfg = request.getfixturevalue("fewshot_cfg_string")
    if loader == "serialized_no_nlp":
        overrides = {
            "paths.el_kb": str(tmp_path / "entity_linker" / "kb"),
            "paths.el_desc": str(tmp_path / "desc.csv"),
        }
        build_el_pipeline(nlp_path=tmp_path, desc_path=tmp_path / "desc.csv")
    else:
        overrides = {
            "paths.el_kb": str(
                Path(__file__).resolve().parent / "misc" / "el_kb_data.yml"
            ),
        }
    orig_config = Config().from_str(cfg, overrides=overrides)
    if loader == "yaml":
        orig_config["initialize"]["components"]["llm"]["candidate_selector"][
            "kb_loader"
        ]["@llm_misc"] = "spacy.KBFileLoader.v1"
        orig_config["initialize"]["components"]["llm"]["candidate_selector"][
            "kb_loader"
        ].pop("nlp_path")
        orig_config["initialize"]["components"]["llm"]["candidate_selector"][
            "kb_loader"
        ].pop("desc_path")

    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    nlp.initialize(lambda: [])

    text = "Alice goes to Boston to see the Boston Celtics game."
    doc = nlp.make_doc(text)
    doc.ents = [
        Span(doc=doc, start=3, end=4, label="LOC"),  # Q100
        Span(doc=doc, start=7, end=9, label="ORG"),  # Q131371
    ]
    doc = nlp(doc)
    assert len(doc.ents) == 2
    assert doc.ents[0].kb_id_ == "Q100"
    assert doc.ents[1].kb_id_ == "Q131371"


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_init_with_code():
    """Tests EntityLinker task example with only code, no config."""
    candidate_selector = KBCandidateSelector(
        kb_loader=KBFileLoader(path=Path(__file__).parent / "misc" / "el_kb_data.yml"),
        top_n=5,
    )
    nlp = spacy.blank("en")
    llm_ner = nlp.add_pipe("llm_ner")
    for label in ("PERSON", "ORGANISATION", "LOCATION", "SPORTS TEAM"):
        llm_ner.add_label(label)

    llm = nlp.add_pipe("llm_entitylinker")
    llm._task.set_candidate_selector(candidate_selector, nlp.vocab)
    nlp.initialize()

    assert (
        nlp("Thibeau Courtois plays for the Red Devils in New York").ents[2].kb_id_
        == "Q60"
    )

    # Should fail due to candidate selector not being set.
    nlp = spacy.blank("en")
    nlp.add_pipe("llm_entitylinker")
    with pytest.raises(ValueError, match="candidate_selector has to be provided"):
        nlp.initialize()



================================================
FILE: spacy_llm/tests/tasks/test_lemma.py
================================================
from pathlib import Path

import pytest
import spacy
from confection import Config
from spacy.tokens import Doc
from spacy.training import Example
from spacy.util import make_tempdir

from spacy_llm.registry import fewshot_reader, file_reader
from spacy_llm.tasks.lemma import LemmaTask
from spacy_llm.util import assemble_from_config

from ...tasks import make_lemma_task
from ..compat import has_openai_key

EXAMPLES_DIR = Path(__file__).parent / "examples"
TEMPLATES_DIR = Path(__file__).parent / "templates"


@pytest.fixture
def noop_config():
    return """
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.Lemma.v1"

    [components.llm.model]
    @llm_models = "test.NoOpModel.v1"
    """


@pytest.fixture
def zeroshot_cfg_string():
    return """
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.Lemma.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """


@pytest.fixture
def fewshot_cfg_string():
    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.Lemma.v1"

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str((Path(__file__).parent / "examples" / "lemma.yml"))}

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """


@pytest.fixture
def ext_template_cfg_string():
    """Simple zero-shot config with an external template"""

    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]
    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.Lemma.v1"

    [components.llm.task.template]
    @misc = "spacy.FileReader.v1"
    path = {str((Path(__file__).parent / "templates" / "lemma.jinja2"))}

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "fewshot_cfg_string",
        "ext_template_cfg_string",
    ],
)
def test_lemma_config(cfg_string, request):
    cfg_string = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg_string)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]

    # also test nlp config from a dict in add_pipe
    component_cfg = dict(orig_config["components"]["llm"])
    component_cfg.pop("factory")

    nlp2 = spacy.blank("en")
    nlp2.add_pipe("llm", config=component_cfg)
    assert nlp2.pipe_names == ["llm"]


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "cfg_string",
    [
        # "zeroshot_cfg_string",
        # "fewshot_cfg_string",
        "ext_template_cfg_string",
    ],
)
def test_lemma_predict(cfg_string, request):
    """Use OpenAI to get zero-shot LEMMA results.
    Note that this test may fail randomly, as the LLM's output is unguaranteed to be consistent/predictable
    """
    cfg = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    lemmas = [str(token.lemma_) for token in nlp("I've watered the plants.")]
    # Compare lemmas for correctness, if we are not using the external dummy template.
    if cfg_string != "ext_template_cfg_string":
        assert lemmas in (
            ["-PRON-", "have", "water", "the", "plant", "."],
            ["I", "have", "water", "the", "plant", "."],
        )


@pytest.mark.external
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "fewshot_cfg_string",
    ],
)
def test_lemma_io(cfg_string, request):
    cfg = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]
    # ensure you can save a pipeline to disk and run it after loading
    with make_tempdir() as tmpdir:
        nlp.to_disk(tmpdir)
        nlp2 = spacy.load(tmpdir)
    assert nlp2.pipe_names == ["llm"]
    lemmas = [str(token.lemma_) for token in nlp2("I've watered the plants.")]
    if cfg_string != "ext_template_cfg_string":
        assert lemmas in (
            ["-PRON-", "have", "water", "the", "plant", "."],
            ["I", "have", "water", "the", "plant", "."],
        )


def test_jinja_template_rendering_without_examples():
    """Test if jinja template renders as we expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    nlp = spacy.blank("en")
    text = "Alice and Bob went to the supermarket"
    doc = nlp.make_doc(text)

    lemma_task = make_lemma_task(examples=None)
    prompt = list(lemma_task.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == f"""
You are an expert lemmatization system. Your task is to accept Text as input and identify the lemma for every token in the Text.
Consider that contractions represent multiple words. Each word in a contraction should be annotated with its lemma separately.
Output each original word on a new line, followed by a colon and the word's lemma - like this:
'''
Word1: Lemma of Word1
Word2: Lemma of Word2
'''
Include the final punctuation token in this list.
Prefix with your output with "Lemmatized text".


Here is the text that needs to be lemmatized:
'''
{text}
'''
""".strip()
    )


@pytest.mark.parametrize(
    "examples_path",
    [
        str(EXAMPLES_DIR / "lemma.json"),
        str(EXAMPLES_DIR / "lemma.yml"),
        str(EXAMPLES_DIR / "lemma.jsonl"),
    ],
)
def test_jinja_template_rendering_with_examples(examples_path):
    """Test if jinja2 template renders as expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    nlp = spacy.blank("en")
    text = "Alice and Bob went to the supermarket."
    doc = nlp.make_doc(text)

    lemma_task = make_lemma_task(examples=fewshot_reader(examples_path))
    prompt = list(lemma_task.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == f"""
You are an expert lemmatization system. Your task is to accept Text as input and identify the lemma for every token in the Text.
Consider that contractions represent multiple words. Each word in a contraction should be annotated with its lemma separately.
Output each original word on a new line, followed by a colon and the word's lemma - like this:
'''
Word1: Lemma of Word1
Word2: Lemma of Word2
'''
Include the final punctuation token in this list.
Prefix with your output with "Lemmatized text".

Below are some examples (only use these as a guide):

Text:
'''
The arc of the moral universe is long, but it bends toward justice.
'''
Lemmas:
'''
The: The
arc: arc
of: of
the: the
moral: moral
universe: universe
is: be
long: long
,: ,
but: but
it: it
bends: bend
toward: toward
justice: justice
.: .
'''

Text:
'''
Life can only be understood backwards; but it must be lived forwards.
'''
Lemmas:
'''
Life: Life
can: can
only: only
be: be
understood: understand
backwards: backwards
;: ;
but: but
it: it
must: must
be: be
lived: lived
forwards: forwards
.: .
'''

Text:
'''
I'm buying ice cream.
'''
Lemmas:
'''
I: I
'm: be
buying: buy
ice: ice
cream: cream
.: .
'''

Here is the text that needs to be lemmatized:
'''
{text}
'''
""".strip()
    )


def test_external_template_actually_loads():
    template_path = str(TEMPLATES_DIR / "lemma.jinja2")
    template = file_reader(template_path)
    text = "Alice and Bob went to the supermarket"
    nlp = spacy.blank("en")
    doc = nlp.make_doc(text)

    lemma_task = make_lemma_task(template=template)
    prompt = list(lemma_task.generate_prompts([doc]))[0][0][0]
    assert (
        prompt.strip()
        == f"""
This is a test LEMMA template.
Here is the text: {text}
""".strip()
    )


@pytest.mark.parametrize("n_prompt_examples", [-1, 0, 1, 2])
def test_lemma_init(noop_config, n_prompt_examples: int):
    config = Config().from_str(noop_config)
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp = assemble_from_config(config)

    examples = []
    pred_words_1 = ["Alice", "works", "all", "evenings"]
    gold_lemmas_1 = ["Alice", "work", "all", "evening"]
    pred_1 = Doc(nlp.vocab, words=pred_words_1)
    gold_1 = Doc(nlp.vocab, words=pred_words_1, lemmas=gold_lemmas_1)
    examples.append(Example(pred_1, gold_1))

    pred_words_2 = ["Bob", "loves", "living", "cities"]
    gold_lemmas_2 = ["Bob", "love", "live", "city"]
    pred_2 = Doc(nlp.vocab, words=pred_words_2)
    gold_2 = Doc(nlp.vocab, words=pred_words_2, lemmas=gold_lemmas_2)
    examples.append(Example(pred_2, gold_2))

    _, llm = nlp.pipeline[0]
    task: LemmaTask = llm._task

    assert not task._prompt_examples

    nlp.config["initialize"]["components"]["llm"] = {
        "n_prompt_examples": n_prompt_examples
    }
    nlp.initialize(lambda: examples)

    if n_prompt_examples >= 0:
        assert len(task._prompt_examples) == n_prompt_examples
    else:
        assert len(task._prompt_examples) == len(examples)



================================================
FILE: spacy_llm/tests/tasks/test_ner.py
================================================
import json
import re
from pathlib import Path
from typing import Callable, List, Tuple, cast

import pytest
import spacy
import srsly
from confection import Config
from spacy.language import Language
from spacy.tokens import Span
from spacy.training import Example
from spacy.util import make_tempdir

from spacy_llm.compat import Literal, ValidationError
from spacy_llm.pipeline import LLMWrapper
from spacy_llm.registry import fewshot_reader, file_reader, lowercase_normalizer
from spacy_llm.registry import strip_normalizer
from spacy_llm.tasks.ner import NERTask, make_ner_task_v3
from spacy_llm.tasks.span import SpanReason
from spacy_llm.tasks.span.parser import _extract_span_reasons_cot
from spacy_llm.tasks.util import find_substrings
from spacy_llm.ty import LabeledTask, ShardingLLMTask
from spacy_llm.util import assemble_from_config, split_labels

from ..compat import has_openai_key

EXAMPLES_DIR = Path(__file__).parent / "examples"
TEMPLATES_DIR = Path(__file__).parent / "templates"


@pytest.fixture
def examples_dir():
    return EXAMPLES_DIR


@pytest.fixture
def templates_dir():
    return TEMPLATES_DIR


@pytest.fixture
def noop_config():
    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.NER.v3"
    labels = PER,ORG,LOC

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str((Path(__file__).parent / "examples" / "ner.json"))}

    [components.llm.model]
    @llm_models = "test.NoOpModel.v1"
    output = 1. Bob | True | PER | is the name of a person
        2. Alice | True | PER | is the name of a person
    """


@pytest.fixture
def fewshot_cfg_string_v3_lds():
    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.NER.v3"
    description = "This is a description"
    labels = PER,ORG,LOC

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str((Path(__file__).parent / "examples" / "ner.json"))}

    [components.llm.task.label_definitions]
    PER = "Any named individual in the text"
    ORG = "Any named organization in the text"
    LOC = "The name of any politically or geographically defined location"

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """


@pytest.fixture
def fewshot_cfg_string_v3():
    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.NER.v3"
    description = "This is a description"
    labels = ["PER", "ORG", "LOC"]

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str((Path(__file__).parent / "examples" / "ner.json"))}

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """


@pytest.fixture
def ext_template_cfg_string():
    """Simple zero-shot config with an external template"""

    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]
    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.NER.v3"
    description = "This is a description"
    labels = ["PER", "ORG", "LOC"]

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str((Path(__file__).parent / "examples" / "ner.json"))}

    [components.llm.task.template]
    @misc = "spacy.FileReader.v1"
    path = {str((Path(__file__).parent / "templates" / "ner.jinja2"))}

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """


@pytest.fixture(
    params=[
        "fewshot_cfg_string_v3_lds",
        "fewshot_cfg_string_v3",
        "ext_template_cfg_string",
    ]
)
def config(request) -> Config:
    cfg_str = request.getfixturevalue(request.param)
    config = Config().from_str(cfg_str)
    return config


@pytest.fixture
def nlp(config: Config) -> Language:
    nlp = assemble_from_config(config)
    return nlp


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_ner_config(config: Config):
    nlp = assemble_from_config(config)
    assert nlp.pipe_names == ["llm"]

    # also test nlp config from a dict in add_pipe
    component_cfg = dict(config["components"]["llm"])
    component_cfg.pop("factory")

    nlp2 = spacy.blank("en")
    nlp2.add_pipe("llm", config=component_cfg)
    assert nlp2.pipe_names == ["llm"]

    pipe = nlp.get_pipe("llm")
    assert isinstance(pipe, LLMWrapper)
    assert isinstance(pipe.task, ShardingLLMTask)

    labels = config["components"]["llm"]["task"]["labels"]
    labels = split_labels(labels)
    task = pipe.task
    assert isinstance(task, LabeledTask)
    assert sorted(task.labels) == sorted(tuple(labels))
    assert pipe.labels == task.labels
    assert nlp.pipe_labels["llm"] == list(task.labels)


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "cfg_str",
    ["fewshot_cfg_string_v3_lds", "fewshot_cfg_string_v3"],
)
@pytest.mark.parametrize(
    "text,gold_ents",
    [
        (
            "Marc and Bob both live in Ireland.",
            [("Marc", "PER"), ("Bob", "PER"), ("Ireland", "LOC")],
        ),
    ],
)
def test_ner_predict(cfg_str, text, gold_ents, request):
    """Use OpenAI to get NER results.
    Note that this test may fail randomly, as the LLM's output is unguaranteed to be consistent/predictable
    """
    config = Config().from_str(request.getfixturevalue(cfg_str))

    nlp = spacy.util.load_model_from_config(config, auto_fill=True)
    doc = nlp(text)

    assert len(doc.ents) == len(gold_ents)
    for pred_ent, gold_ent in zip(doc.ents, gold_ents):
        assert (
            gold_ent[0] in pred_ent.text
        )  # occassionally, the LLM predicts "in Ireland" instead of just "Ireland"
        assert pred_ent.label_ in gold_ent[1].split("|")


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "text,gold_ents",
    [
        (
            "Marc and Bob both live in Ireland.",
            [("Marc", "PER"), ("Bob", "PER"), ("Ireland", "LOC")],
        ),
    ],
)
def test_llm_ner_predict(text, gold_ents):
    """Use llm_ner factory with default OpenAI model to get NER results.
    Note that this test may fail randomly, as the LLM's output is unguaranteed to be consistent/predictable
    """
    nlp = spacy.blank("en")
    llm = nlp.add_pipe("llm_ner")
    for ent_str, ent_label in gold_ents:
        llm.add_label(ent_label)
    doc = nlp(text)

    assert len(doc.ents) == len(gold_ents)
    for pred_ent, gold_ent in zip(doc.ents, gold_ents):
        assert (
            gold_ent[0] in pred_ent.text
        )  # occassionally, the LLM predicts "in Ireland" instead of just "Ireland"
        assert pred_ent.label_ in gold_ent[1].split("|")


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_ner_io(nlp: Language):
    assert nlp.pipe_names == ["llm"]
    # ensure you can save a pipeline to disk and run it after loading
    with make_tempdir() as tmpdir:
        nlp.to_disk(tmpdir)
        nlp2 = spacy.load(tmpdir)
    assert nlp2.pipe_names == ["llm"]
    text = "Marc and Bob both live in Ireland."
    doc = nlp2(text)
    assert len(doc.ents) >= 0  # can be zero if template is too simple / test-like
    for ent in doc.ents:
        assert ent.label_ in ["PER", "ORG", "LOC"]


@pytest.mark.parametrize(
    "text,input_strings,result_strings,result_offsets",
    [
        (
            "Felipe and Jaime went to the library.",
            ["Felipe", "Jaime", "library"],
            ["Felipe", "Jaime", "library"],
            [(0, 6), (11, 16), (29, 36)],
        ),  # simple
        (
            "The Manila Observatory was founded in 1865 in Manila.",
            ["Manila", "The Manila Observatory"],
            ["Manila", "Manila", "The Manila Observatory"],
            [(4, 10), (46, 52), (0, 22)],
        ),  # overlapping and duplicated
        (
            "Take the road from downtown and turn left at the public market.",
            ["public market", "downtown"],
            ["public market", "downtown"],
            [(49, 62), (19, 27)]
            # flipped
        ),
    ],
)
def test_ensure_offsets_correspond_to_substrings(
    text, input_strings, result_strings, result_offsets
):
    offsets = find_substrings(text, input_strings)
    # Compare strings instead of offsets, but we need to get
    # those strings first from the text
    assert result_offsets == offsets
    found_substrings = [text[start:end] for start, end in offsets]
    assert result_strings == found_substrings


@pytest.mark.parametrize(
    "response,normalizer,gold_ents",
    [
        (
            "1. Jean Jacques | True | PER | is a person's name\n"
            "2. Jaime | True | PER | is a person's name\n",
            None,
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "1. Jean Jacques | True | PER | is a person's name\n"
            "2. Jaime | True | PER | is a person's name\n",
            strip_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "1. Jean Jacques | True | PER | is a person's name\n"
            "2. Jaime | True | PER | is a person's name\n",
            lowercase_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "1. Jean Jacques | True | per | is a person's name\n"
            "2. Jaime | True | per | is a person's name\n",
            strip_normalizer(),
            [],
        ),
        (
            "1. Jean Jacques | True | per | is a person's name\n"
            "2. Jaime | True | per | is a person's name\n",
            None,
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "1. Jean Jacques | True | per | is a person's name\n"
            "2. Jaime | True | PER | is a person's name\n",
            lowercase_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "1. Jean Jacques | True | per | is a person's name\n"
            "2. Jaime | True | per | is a person's name\n"
            "3. library | True | Org | is a organization\n",
            lowercase_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER"), ("library", "ORG")],
        ),
        (
            "1. Jean Jacques | True | per | is a person's name\n"
            "2. Jaime | True | per | is a person's name\n"
            "3. Jaime | True | RANDOM | is an entity\n",
            lowercase_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
    ],
)
def test_ner_labels(
    response: str, normalizer: Callable[[str], str], gold_ents: List[Tuple[str, str]]
):
    text = "Jean Jacques and Jaime went to the library."
    labels = "PER,ORG,LOC"

    llm_ner = make_ner_task_v3(examples=[], labels=labels, normalizer=normalizer)
    # Prepare doc
    nlp = spacy.blank("en")
    doc_in = nlp.make_doc(text)
    # Pass to the parser
    # Note: parser() returns a list
    doc_out = list(llm_ner.parse_responses([[doc_in]], [[response]]))[0]
    pred_ents = [(ent.text, ent.label_) for ent in doc_out.ents]
    assert pred_ents == gold_ents


@pytest.mark.parametrize(
    "response,alignment_mode,gold_ents",
    [
        (
            "1. Jacq | True | PER | is a person's name",
            "strict",
            [],
        ),
        (
            "1. Jacq | True | PER | is a person's name",
            "contract",
            [],
        ),
        (
            "1. Jacq | True | PER | is a person's name",
            "expand",
            [("Jacques", "PER")],
        ),
        (
            "1. Jean J | True | PER | is a person's name",
            "contract",
            [("Jean", "PER")],
        ),
        (
            "1. Jean Jacques | True | PER | is a person's name",
            "strict",
            [("Jean Jacques", "PER")],
        ),
        (
            "1. random | True | PER | is a person's name",
            "expand",
            [],
        ),
    ],
    ids=["strict_1", "contract_1", "expand_1", "strict_2", "contract_2", "expand_2"],
)
def test_ner_alignment(
    response: str,
    alignment_mode: Literal["strict", "contract", "expand"],
    gold_ents: List[Tuple[str, str]],
):
    text = "Jean Jacques and Jaime went to the library."
    labels = "PER,ORG,LOC"
    llm_ner = make_ner_task_v3(
        examples=[], labels=labels, alignment_mode=alignment_mode
    )
    # Prepare doc
    nlp = spacy.blank("en")
    doc_in = nlp.make_doc(text)
    # Pass to the parser
    # Note: parser() returns a list
    doc_out = list(llm_ner.parse_responses([[doc_in]], [[response]]))[0]
    pred_ents = [(ent.text, ent.label_) for ent in doc_out.ents]
    assert pred_ents == gold_ents


def test_invalid_alignment_mode():
    labels = "PER,ORG,LOC"
    with pytest.raises(ValueError, match="Unsupported alignment mode 'invalid"):
        make_ner_task_v3(examples=[], labels=labels, alignment_mode="invalid")  # type: ignore


@pytest.mark.parametrize(
    "response, case_sensitive, gold_ents",
    [
        (
            "1. Jean | True | PER | is a person's name",
            False,
            [("jean", "PER")],
        ),
        (
            "1. Jean | True | PER | is a person's name",
            True,
            [("Jean", "PER")],
        ),
        (
            "1. jean | True | PER | is a person's name\n"
            "2. Jean | True | PER | is a person's name\n"
            "3. Jean Foundation | True | ORG | is the name of an Organization name",
            False,
            [("jean", "PER"), ("Jean", "PER"), ("Jean Foundation", "ORG")],
        ),
    ],
    ids=[
        "single_ent_case_insensitive",
        "single_ent_case_sensitive",
        "multiple_ents_case_insensitive",
    ],
)
def test_ner_matching(
    response: str, case_sensitive: bool, gold_ents: List[Tuple[str, str]]
):
    text = "This guy jean (or Jean) is the president of the Jean Foundation."
    labels = "PER,ORG,LOC"
    llm_ner = make_ner_task_v3(
        examples=[], labels=labels, case_sensitive_matching=case_sensitive
    )
    # Prepare doc
    nlp = spacy.blank("en")
    doc_in = nlp.make_doc(text)
    # Pass to the parser
    # Note: parser() returns a list
    doc_out = list(llm_ner.parse_responses([[doc_in]], [[response]]))[0]
    pred_ents = [(ent.text, ent.label_) for ent in doc_out.ents]
    assert pred_ents == gold_ents


def test_jinja_template_rendering_without_examples():
    """Test if jinja2 template renders as expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    labels = "PER,ORG,LOC"
    nlp = spacy.blank("en")
    doc = nlp.make_doc("Alice and Bob went to the supermarket")
    llm_ner = make_ner_task_v3(labels=labels)
    prompt = list(llm_ner.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == """
You are an expert Named Entity Recognition (NER) system.
Your task is to accept Text as input and extract named entities.
Entities must have one of the following labels: LOC, ORG, PER.
If a span is not an entity label it: `==NONE==`.


Here is an example of the output format for a paragraph using different labels than this task requires.
Only use this output format but use the labels provided
above instead of the ones defined in the example below.
Do not output anything besides entities in this output format.
Output entities in the order they occur in the input paragraph regardless of label.

Q: Given the paragraph below, identify a list of entities, and for each entry explain why it is or is not an entity:

Paragraph: Sriracha sauce goes really well with hoisin stir fry, but you should add it after you use the wok.
Answer:
1. Sriracha sauce | True | INGREDIENT | is an ingredient to add to a stir fry
2. really well | False | ==NONE== | is a description of how well sriracha sauce goes with hoisin stir fry
3. hoisin stir fry | True | DISH | is a dish with stir fry vegetables and hoisin sauce
4. wok | True | EQUIPMENT | is a piece of cooking equipment used to stir fry ingredients

Paragraph: Alice and Bob went to the supermarket
Answer:
""".strip()
    )


@pytest.mark.parametrize("examples_file", ["ner.json", "ner.yml", "ner.jsonl"])
def test_jinja_template_rendering_with_examples(examples_dir: Path, examples_file: str):
    """Test if jinja2 template renders as expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """

    labels = "PER,ORG,LOC"
    nlp = spacy.blank("en")
    doc = nlp.make_doc("Alice and Bob went to the supermarket")
    examples = fewshot_reader(examples_dir / examples_file)
    llm_ner = make_ner_task_v3(examples=examples, labels=labels)
    prompt = list(llm_ner.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == """
You are an expert Named Entity Recognition (NER) system.
Your task is to accept Text as input and extract named entities.
Entities must have one of the following labels: LOC, ORG, PER.
If a span is not an entity label it: `==NONE==`.

Q: Given the paragraph below, identify a list of entities, and for each entry explain why it is or is not an entity:

Paragraph: Jack and Jill went up the hill.
Answer:
1. Jack | True | PER | is the name of a person
2. Jill | True | PER | is the name of a person
3. went up | False | ==NONE== | is a verb
4. hill | True | LOC | is a location

Paragraph: Alice and Bob went to the supermarket
Answer:
""".strip()
    )


@pytest.mark.parametrize("examples_file", ["ner.json", "ner.yml", "ner.jsonl"])
def test_jinja_template_rendering_with_label_definitions(
    examples_dir: Path, examples_file: str
):
    """Test if jinja2 template renders as expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    labels = "PER,ORG,LOC"
    nlp = spacy.blank("en")
    doc = nlp.make_doc("Alice and Bob went to the supermarket")
    examples = fewshot_reader(examples_dir / examples_file)
    llm_ner = make_ner_task_v3(
        examples=examples,
        labels=labels,
        label_definitions={
            "PER": "Person definition",
            "ORG": "Organization definition",
            "LOC": "Location definition",
        },
    )
    prompt = list(llm_ner.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == """
You are an expert Named Entity Recognition (NER) system.
Your task is to accept Text as input and extract named entities.
Entities must have one of the following labels: LOC, ORG, PER.
If a span is not an entity label it: `==NONE==`.

Below are definitions of each label to help aid you in what kinds of named entities to extract for each label.
Assume these definitions are written by an expert and follow them closely.
PER: Person definition
ORG: Organization definition
LOC: Location definition

Q: Given the paragraph below, identify a list of entities, and for each entry explain why it is or is not an entity:

Paragraph: Jack and Jill went up the hill.
Answer:
1. Jack | True | PER | is the name of a person
2. Jill | True | PER | is the name of a person
3. went up | False | ==NONE== | is a verb
4. hill | True | LOC | is a location

Paragraph: Alice and Bob went to the supermarket
Answer:
""".strip()
    )


@pytest.mark.parametrize(
    "value, expected_type",
    [
        (
            {
                "text": "I'm a wrong example. Entities should be a dict, not a list",
                # Should be: {"PER": ["Entities"], "ORG": ["dict", "list"]}
                "entities": [("PER", ("Entities")), ("ORG", ("dict", "list"))],
            },
            ValidationError,
        ),
        (
            {
                "text": "Jack is a name",
                "spans": [
                    {
                        "text": "Jack",
                        "is_entity": True,
                        "label": "PER",
                        "reason": "is a person's name",
                    }
                ],
            },
            NERTask,
        ),
    ],
)
def test_fewshot_example_data(value: dict, expected_type: type):
    with make_tempdir() as tmpdir:
        tmp_path = tmpdir / "wrong_example.yml"
        srsly.write_yaml(tmp_path, [value])

        try:
            task = make_ner_task_v3(
                examples=fewshot_reader(tmp_path), labels=["PER", "ORG", "LOC"]
            )
        except (ValidationError, ValueError) as e:
            assert type(e) == expected_type
        else:
            assert type(task) == expected_type


def test_external_template_actually_loads():
    template_path = str(TEMPLATES_DIR / "ner.jinja2")
    template = file_reader(template_path)
    labels = "PER,ORG,LOC"
    nlp = spacy.blank("en")
    doc = nlp.make_doc("Alice and Bob went to the supermarket")

    llm_ner = make_ner_task_v3(examples=[], labels=labels, template=template)
    prompt = list(llm_ner.generate_prompts([doc]))[0][0][0]
    assert prompt.strip().startswith("Here's the test template for the tests and stuff")


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize("n_detections", [0, 1, 2])
def test_ner_scoring(fewshot_cfg_string_v3: str, n_detections: int):
    config = Config().from_str(fewshot_cfg_string_v3)
    nlp = assemble_from_config(config)

    examples = []
    for text in ["Alice works with Bob.", "Bob lives with Alice."]:
        predicted = nlp.make_doc(text)
        reference = nlp.make_doc(text)
        ent1 = Span(reference, 0, 1, label="PER")
        ent2 = Span(reference, 3, 4, label="PER")
        reference.set_ents([ent1, ent2][:n_detections])
        examples.append(Example(predicted, reference))

    scores = nlp.evaluate(examples)
    assert scores["ents_p"] == n_detections / 2
    assert scores["ents_r"] == (1 if n_detections != 0 else 0)
    assert scores["ents_f"] == (
        pytest.approx(0.666666666) if n_detections == 1 else n_detections / 2
    )


@pytest.mark.parametrize("n_prompt_examples", [-1, 0, 1, 2])
def test_ner_init(noop_config: str, n_prompt_examples: int):
    config = Config().from_str(noop_config)
    config["components"]["llm"]["task"]["labels"] = ["PER", "LOC"]
    config["components"]["llm"]["task"]["examples"] = []
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp = assemble_from_config(config)

    examples = []
    for text in [
        "Alice works with Bob in London.",
        "Bob lives with Alice in Manchester.",
    ]:
        predicted = nlp.make_doc(text)
        reference = predicted.copy()

        reference.set_ents(
            [
                Span(reference, 0, 1, label="PER"),
                Span(reference, 3, 4, label="PER"),
                Span(reference, 5, 6, label="LOC"),
            ]
        )
        examples.append(Example(predicted, reference))

    task = cast(NERTask, nlp.get_pipe("llm").task)
    nlp.config["initialize"]["components"]["llm"] = {
        "n_prompt_examples": n_prompt_examples
    }
    nlp.initialize(lambda: examples)

    assert set(task._label_dict.values()) == {"PER", "LOC"}

    if n_prompt_examples == -1:
        assert len(task._prompt_examples) == len(examples)
    else:
        assert len(task._prompt_examples) == n_prompt_examples

    if n_prompt_examples > 0:
        for eg in task._prompt_examples:
            prompt_example_labels = {ent.label for ent in eg.spans}
            if "==NONE==" not in prompt_example_labels:
                prompt_example_labels.add("==NONE==")
            assert prompt_example_labels == {"==NONE==", "PER", "LOC"}


def test_ner_serde(noop_config: str):
    config = Config().from_str(noop_config)

    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp1 = assemble_from_config(config)
        nlp2 = assemble_from_config(config)

    labels = {"loc": "LOC", "per": "PER"}

    task1 = cast(NERTask, nlp1.get_pipe("llm").task)
    task2 = cast(NERTask, nlp2.get_pipe("llm").task)

    # Artificially add labels to task1
    task1._label_dict = labels
    task2._label_dict = {}

    assert task1._label_dict == labels
    assert task2._label_dict == dict()

    b = nlp1.to_bytes()
    nlp2.from_bytes(b)

    assert task1._label_dict == task2._label_dict == labels


def test_ner_to_disk(noop_config: str, tmp_path: Path):
    config = Config().from_str(noop_config)
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp1 = assemble_from_config(config)
        nlp2 = assemble_from_config(config)

    labels = {"loc": "LOC", "org": "ORG", "per": "PER"}

    task1 = cast(NERTask, nlp1.get_pipe("llm").task)
    task2 = cast(NERTask, nlp2.get_pipe("llm").task)

    # Artificially add labels to task1
    task1._label_dict = labels
    task2._label_dict = {}

    assert task1._label_dict == labels
    assert task2._label_dict == dict()

    path = tmp_path / "model"
    nlp1.to_disk(path)

    cfgs = list(path.rglob("cfg"))
    assert len(cfgs) == 1

    cfg = json.loads(cfgs[0].read_text())
    assert cfg["_label_dict"] == labels

    nlp2.from_disk(path)
    assert task1._label_dict == task2._label_dict == labels


@pytest.mark.filterwarnings("ignore:Task supports sharding")
def test_label_inconsistency():
    """Test whether inconsistency between specified labels and labels in examples is detected."""
    cfg = f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.NER.v3"
    labels = ["PERSON", "LOCATION"]

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str((Path(__file__).parent / "examples" / "ner_inconsistent.yml"))}

    [components.llm.model]
    @llm_models = "test.NoOpModel.v1"
    """

    config = Config().from_str(cfg)
    with pytest.warns(
        UserWarning,
        match=re.escape(
            "Examples contain labels that are not specified in the task configuration. The latter contains the "
            "following labels: ['LOCATION', 'PERSON']. Labels in examples missing from the task configuration: "
            "['TECH']. Please ensure your label specification and example labels are consistent."
        ),
    ):
        nlp = assemble_from_config(config)

    prompt_examples = nlp.get_pipe("llm")._task._prompt_examples
    assert len(prompt_examples) == 2
    assert prompt_examples[0].text == "Jack and Jill went up the hill."
    assert prompt_examples[0].spans == [
        SpanReason(
            text="Jack",
            is_entity=True,
            label="PERSON",
            reason="is the name of a person",
        ),
        SpanReason(
            text="Jill",
            is_entity=True,
            label="PERSON",
            reason="is the name of a person",
        ),
        SpanReason(
            text="went up", is_entity=False, label="==NONE==", reason="is a verb"
        ),
        SpanReason(
            text="hill", is_entity=True, label="LOCATION", reason="is a location"
        ),
    ]
    assert (
        prompt_examples[1].text
        == "Jack and Jill went up the hill and spaCy is a great tool."
    )
    assert prompt_examples[1].spans == [
        SpanReason(
            text="Jack",
            is_entity=True,
            label="PERSON",
            reason="is the name of a person",
        ),
        SpanReason(
            text="Jill",
            is_entity=True,
            label="PERSON",
            reason="is the name of a person",
        ),
        SpanReason(
            text="went up", is_entity=False, label="==NONE==", reason="is a verb"
        ),
        SpanReason(
            text="hill", is_entity=True, label="LOCATION", reason="is a location"
        ),
    ]


@pytest.mark.parametrize(
    "text, response, gold_ents",
    [
        (
            "The woman Paris was walking around in Paris, talking to her friend Paris",
            "1. Paris | True | PER | is the name of the woman\n"
            "2. Paris | True | LOC | is a city in France\n"
            "3. Paris | True | PER | is the name of the woman\n",
            [("Paris", "PER"), ("Paris", "LOC"), ("Paris", "PER")],
        ),
        (
            "Walking around Paris as a woman named Paris is quite a trip.",
            "1. Paris | True | LOC | is a city in France\n"
            "2. Paris | True | PER | is the name of the woman\n",
            [("Paris", "LOC"), ("Paris", "PER")],
        ),
    ],
    ids=["3_ents", "2_ents"],
)
def test_regression_span_task_response_parse(
    text: str, response: str, gold_ents: List[Tuple[str, str]]
):
    """Test based on spaCy issue: https://github.com/explosion/spaCy/discussions/12812
    where parsing wasn't working for NER when the same text could map to 2 labels.
    In the user's case "Paris" could be a person's name or a location.
    """

    nlp = spacy.blank("en")
    example_doc = nlp.make_doc(text)
    ner_task = make_ner_task_v3(examples=[], labels=["PER", "LOC"])
    span_reasons = _extract_span_reasons_cot(ner_task, response)
    assert len(span_reasons) == len(gold_ents)

    docs = list(ner_task.parse_responses([[example_doc]], [[response]]))
    assert len(docs) == 1

    doc = docs[0]
    pred_ents = [(ent.text, ent.label_) for ent in doc.ents]
    assert pred_ents == gold_ents


@pytest.mark.parametrize(
    "text, response, gold_ents",
    [
        (
            "FooBar, Inc. is a large organization in the U.S.",
            "1. FooBar, Inc. | True | ORG | is the name of an organization\n"
            "2. U.S. | True | LOC | is a country\n",
            [("FooBar, Inc.", "ORG"), ("U.S.", "LOC")],
        ),
    ],
)
def test_regression_span_task_comma(
    text: str, response: str, gold_ents: List[Tuple[str, str]]
):
    """Test that spacy.NER.v3 can deal with comma's in entities"""

    nlp = spacy.blank("en")
    example_doc = nlp.make_doc(text)
    ner_task = make_ner_task_v3(examples=[], labels=["ORG", "LOC"])
    span_reasons = _extract_span_reasons_cot(ner_task, response)
    assert len(span_reasons) == len(gold_ents)
    docs = list(ner_task.parse_responses([[example_doc]], [[response]]))
    assert len(docs) == 1
    doc = docs[0]
    pred_ents = [(ent.text, ent.label_) for ent in doc.ents]
    assert pred_ents == gold_ents


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_add_label():
    nlp = spacy.blank("en")
    llm = nlp.add_pipe(
        "llm",
        config={
            "task": {
                "@llm_tasks": "spacy.NER.v3",
            },
            "model": {
                "@llm_models": "spacy.GPT-3-5.v1",
            },
        },
    )

    nlp.initialize()
    text = "Jack and Jill visited France."
    doc = nlp(text)
    assert len(doc.ents) == 0

    for label, definition in [
        ("PERSON", "Every person with the name Jack"),
        ("LOCATION", "A geographical location, like a country or a city"),
        ("COMPANY", None),
    ]:
        llm.add_label(label, definition)
    doc = nlp(text)
    assert len(doc.ents) > 1


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_clear_label():
    nlp = spacy.blank("en")
    llm = nlp.add_pipe(
        "llm",
        config={
            "task": {
                "@llm_tasks": "spacy.NER.v3",
            },
            "model": {
                "@llm_models": "spacy.GPT-3-5.v1",
            },
        },
    )

    nlp.initialize()
    text = "Jack and Jill visited France."
    doc = nlp(text)

    for label in ["PERSON", "LOCATION"]:
        llm.add_label(label)
    doc = nlp(text)
    assert len(doc.ents) == 3

    llm.clear()

    doc = nlp(text)
    assert len(doc.ents) == 0



================================================
FILE: spacy_llm/tests/tasks/test_raw.py
================================================
from pathlib import Path

import pytest
import spacy
from confection import Config
from spacy.training import Example
from spacy.util import make_tempdir

from spacy_llm.registry import file_reader
from spacy_llm.util import assemble_from_config

from ...tasks import RawTask, make_raw_task
from ..compat import has_openai_key

EXAMPLES_DIR = Path(__file__).parent / "examples"
TEMPLATES_DIR = Path(__file__).parent / "templates"


@pytest.fixture
def noop_config():
    return """
    [nlp]
    lang = "en"
    pipeline = ["llm"]

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.Raw.v1"

    [components.llm.model]
    @llm_models = "test.NoOpModel.v1"
    """


@pytest.fixture
def zeroshot_cfg_string():
    return """
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.Raw.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v3"
    """


@pytest.fixture
def fewshot_cfg_string():
    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.Raw.v1"

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str((Path(__file__).parent / "examples" / "raw.yml"))}

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v3"
    """


@pytest.fixture
def ext_template_cfg_string():
    """Simple zero-shot config with an external template"""

    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]

    [components]
    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.Raw.v1"

    [components.llm.task.template]
    @misc = "spacy.FileReader.v1"
    path = {str((Path(__file__).parent / "templates" / "raw.jinja2"))}

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v3"
    """


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "fewshot_cfg_string",
        "ext_template_cfg_string",
    ],
)
def test_raw_config(cfg_string, request):
    cfg_string = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg_string)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]

    # also test nlp config from a dict in add_pipe
    component_cfg = dict(orig_config["components"]["llm"])
    component_cfg.pop("factory")

    nlp2 = spacy.blank("en")
    nlp2.add_pipe("llm", config=component_cfg)
    assert nlp2.pipe_names == ["llm"]


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "fewshot_cfg_string",
        "ext_template_cfg_string",
    ],
)
def test_raw_predict(cfg_string, request):
    """Use OpenAI to get zero-shot LEMMA results.
    Note that this test may fail randomly, as the LLM's output is unguaranteed to be consistent/predictable
    """
    cfg = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp("What's the weather like?")._.llm_reply


@pytest.mark.external
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "fewshot_cfg_string",
    ],
)
def test_raw_io(cfg_string, request):
    cfg = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]
    # ensure you can save a pipeline to disk and run it after loading
    with make_tempdir() as tmpdir:
        nlp.to_disk(tmpdir)
        nlp2 = spacy.load(tmpdir)
    assert nlp2.pipe_names == ["llm"]
    assert nlp2("I've watered the plants.")._.llm_reply


def test_jinja_template_rendering_without_examples():
    """Test if jinja template renders as we expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    nlp = spacy.blank("en")
    text = "How much wood would a woodchuck chuck if a woodchuck could chuck wood?"
    doc = nlp.make_doc(text)

    raw_task = make_raw_task(examples=None)
    prompt = list(raw_task.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == f"""
Text:
{text}
Reply:
""".strip()
    )


@pytest.mark.parametrize(
    "examples_path",
    [
        str(EXAMPLES_DIR / "raw.json"),
        str(EXAMPLES_DIR / "raw.yml"),
        str(EXAMPLES_DIR / "raw.jsonl"),
    ],
)
def test_jinja_template_rendering_with_examples(examples_path):
    """Test if jinja2 template renders as expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    nlp = spacy.blank("en")
    text = "How much wood would a woodchuck chuck if a woodchuck could chuck wood?"
    doc = nlp.make_doc(text)

    raw_task = make_raw_task(examples=None)
    prompt = list(raw_task.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == f"""
Text:
{text}
Reply:
""".strip()
    )


def test_external_template_actually_loads():
    template_path = str(TEMPLATES_DIR / "raw.jinja2")
    template = file_reader(template_path)
    text = "How much wood would a woodchuck chuck if a woodchuck could chuck wood?"
    nlp = spacy.blank("en")
    doc = nlp.make_doc(text)

    raw_task = make_raw_task(examples=None, template=template)
    prompt = list(raw_task.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == f"""
This is a test RAW template.
Here is the text: {text}
""".strip()
    )


@pytest.mark.parametrize("n_prompt_examples", [-1, 0, 1, 2])
def test_raw_init(noop_config, n_prompt_examples: int):
    config = Config().from_str(noop_config)
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp = assemble_from_config(config)

    examples = []
    text = "How much wood would a woodchuck chuck if a woodchuck could chuck wood?"
    gold_1 = nlp.make_doc(text)
    gold_1._.llm_reply = "Plenty"
    examples.append(Example(nlp.make_doc(text), gold_1))

    text = "Who sells seashells by the seashore?"
    gold_2 = nlp.make_doc(text)
    gold_2._.llm_reply = "Shelly"
    examples.append(Example(nlp.make_doc(text), gold_2))

    _, llm = nlp.pipeline[0]
    task: RawTask = llm._task

    assert not task._prompt_examples

    nlp.config["initialize"]["components"]["llm"] = {
        "n_prompt_examples": n_prompt_examples
    }
    nlp.initialize(lambda: examples)

    if n_prompt_examples >= 0:
        assert len(task._prompt_examples) == n_prompt_examples
    else:
        assert len(task._prompt_examples) == len(examples)



================================================
FILE: spacy_llm/tests/tasks/test_rel.py
================================================
import json
from pathlib import Path

import pytest
from confection import Config
from pytest import FixtureRequest
from spacy.tokens import Doc, Span
from spacy.training import Example
from spacy.util import get_lang_class

from spacy_llm.pipeline import LLMWrapper
from spacy_llm.tasks.rel import DEFAULT_REL_TEMPLATE, RelationItem, RELTask
from spacy_llm.ty import LabeledTask, ShardingLLMTask
from spacy_llm.util import assemble_from_config, split_labels

from ...tasks import make_rel_task
from ..compat import has_openai_key

EXAMPLES_DIR = Path(__file__).parent / "examples"


@pytest.fixture
def zeroshot_cfg_string():
    return """
    [nlp]
    lang = "en"
    pipeline = ["ner", "llm"]
    batch_size = 128

    [components]

    [components.ner]
    source = "en_core_web_md"

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.REL.v1"
    labels = "LivesIn,Visits"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"

    [initialize]
    vectors = "en_core_web_md"
    """


@pytest.fixture
def fewshot_cfg_string():
    return f"""
    [nlp]
    lang = "en"
    pipeline = ["ner", "llm"]
    batch_size = 128

    [components]

    [components.ner]
    source = "en_core_web_md"

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.REL.v1"
    labels = ["LivesIn", "Visits"]

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str(EXAMPLES_DIR / "rel.jsonl")}

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"

    [initialize]
    vectors = "en_core_web_md"
    """


@pytest.fixture
def noop_config():
    return """
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.REL.v1"
    labels = ["LivesIn", "Visits"]

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "test.NoOpModel.v1"
    """


@pytest.fixture
def task():
    text = "Joey rents a place in New York City."
    gold_relations = [RelationItem(dep=0, dest=1, relation="LivesIn")]
    return text, gold_relations


@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize("cfg_string", ["zeroshot_cfg_string", "fewshot_cfg_string"])
def test_rel_config(cfg_string, request: FixtureRequest):
    """Simple test to check if the config loads properly given different settings"""
    cfg_string = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg_string)
    nlp = assemble_from_config(orig_config)
    assert nlp.pipe_names == ["ner", "llm"]

    pipe = nlp.get_pipe("llm")
    assert isinstance(pipe, LLMWrapper)
    assert isinstance(pipe.task, ShardingLLMTask)

    task = pipe.task
    labels = orig_config["components"]["llm"]["task"]["labels"]
    labels = split_labels(labels)
    assert isinstance(task, LabeledTask)
    assert task.labels == tuple(labels)
    assert set(pipe.labels) == set(task.labels)
    assert nlp.pipe_labels["llm"] == list(task.labels)


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize("cfg_string", ["fewshot_cfg_string"])
def test_rel_predict(task, cfg_string, request):
    """Use OpenAI to get REL results.
    Note that this test may fail randomly, as the LLM's output is unguaranteed to be consistent/predictable
    """
    cfg_string = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg_string)
    nlp = assemble_from_config(orig_config)

    text, _ = task
    doc = nlp(text)

    assert doc.ents
    assert doc._.rel


@pytest.mark.parametrize("n_prompt_examples", [-1, 0, 1, 2])
def test_rel_init(noop_config, n_prompt_examples: int):
    RELTask._check_extension("rel")

    config = Config().from_str(noop_config)
    del config["components"]["llm"]["task"]["labels"]
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp = assemble_from_config(config)

    examples = []

    for text, rel in [
        ("Alice travelled to London.", "Visits"),
        ("Bob lives in Manchester.", "LivesIn"),
    ]:
        predicted = nlp.make_doc(text)
        reference = predicted.copy()

        # We might want to set those on the predicted example as well...
        reference.ents = [
            Span(reference, 0, 1, label="PER"),
            Span(reference, 3, 4, label="LOC"),
        ]

        reference._.rel = [RelationItem(dep=0, dest=1, relation=rel)]

        examples.append(Example(predicted, reference))

    _, llm = nlp.pipeline[0]
    task: RELTask = llm._task  # type: ignore[annotation-unchecked]

    assert set(task._label_dict.values()) == set()
    assert not task._prompt_examples

    nlp.config["initialize"]["components"]["llm"] = {
        "n_prompt_examples": n_prompt_examples
    }
    nlp.initialize(lambda: examples)

    assert set(task._label_dict.values()) == {"LivesIn", "Visits"}

    if n_prompt_examples >= 0:
        assert len(task._prompt_examples) == n_prompt_examples
    else:
        assert len(task._prompt_examples) == len(examples)


def test_rel_serde(noop_config, tmp_path: Path):
    config = Config().from_str(noop_config)
    del config["components"]["llm"]["task"]["labels"]

    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp1 = assemble_from_config(config)
        nlp2 = assemble_from_config(config)
        nlp3 = assemble_from_config(config)

    labels = {"livesin": "LivesIn", "visits": "Visits"}

    task1: RELTask = nlp1.get_pipe("llm")._task
    task2: RELTask = nlp2.get_pipe("llm")._task
    task3: RELTask = nlp3.get_pipe("llm")._task

    # Artificially add labels to task1
    task1._label_dict = labels

    assert task1._label_dict == labels
    assert task2._label_dict == dict()
    assert task3._label_dict == dict()

    path = tmp_path / "model"

    nlp1.to_disk(path)

    cfgs = list(path.rglob("cfg"))
    assert len(cfgs) == 1

    cfg = json.loads(cfgs[0].read_text())
    assert cfg["_label_dict"] == labels

    nlp2.from_disk(path)
    nlp3.from_bytes(nlp1.to_bytes())

    assert task1._label_dict == task2._label_dict == task3._label_dict == labels


def test_incorrect_indexing():
    """Tests whether incorrect indexing is handled properly (i. e. when the LLM response indices non-existent
    entities).
    """
    task = make_rel_task(
        labels=["LivesIn", "WorksIn"],
        template=DEFAULT_REL_TEMPLATE,
        verbose=False,
    )

    doc = Doc(get_lang_class("en")().vocab, words=["This", "is", "a", "test"])
    doc.ents = [Span(doc, 0, 1, label="TEST")]
    assert (
        len(
            list(
                task._parse_responses(
                    task, [[doc]], [['{"dep": 0, "dest": 0, "relation": "LivesIn"}']]
                )
            )[0][0]
        )
        == 1
    )
    assert (
        len(
            list(
                task._parse_responses(
                    task, [[doc]], [['{"dep": 0, "dest": 1, "relation": "LivesIn"}']]
                )
            )[0][0]
        )
        == 0
    )


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_labels_in_prompt(request: FixtureRequest):
    """See https://github.com/explosion/spacy-llm/issues/366."""
    config = Config().from_str(request.getfixturevalue("zeroshot_cfg_string"))
    config["components"].pop("ner")
    config.pop("initialize")
    config["nlp"]["pipeline"] = ["llm"]
    config["components"]["llm"]["task"]["labels"] = ["A", "B", "C"]
    nlp = assemble_from_config(config)

    doc = Doc(get_lang_class("en")().vocab, words=["Well", "hello", "there"])
    doc.ents = [Span(doc, 0, 1, "A"), Span(doc, 1, 2, "B"), Span(doc, 2, 3, "C")]

    assert (
        "Well[ENT0:A] hello[ENT1:B] there[ENT2:C]"
        in list(nlp.get_pipe("llm")._task.generate_prompts([doc]))[0][0][0]
    )



================================================
FILE: spacy_llm/tests/tasks/test_sentiment.py
================================================
from pathlib import Path

import numpy
import pytest
import spacy
from confection import Config
from spacy.training import Example
from spacy.util import make_tempdir

from spacy_llm.registry import fewshot_reader, file_reader

from ...tasks import make_sentiment_task
from ..compat import has_openai_key

EXAMPLES_DIR = Path(__file__).parent / "examples"
TEMPLATES_DIR = Path(__file__).parent / "templates"


@pytest.fixture
def zeroshot_cfg_string():
    return """
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.Sentiment.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """


@pytest.fixture
def fewshot_cfg_string():
    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.Sentiment.v1"

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str((Path(__file__).parent / "examples" / "sentiment.yml"))}

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """


@pytest.fixture
def ext_template_cfg_string():
    """Simple zero-shot config with an external template"""

    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]
    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.Sentiment.v1"

    [components.llm.task.template]
    @misc = "spacy.FileReader.v1"
    path = {str((Path(__file__).parent / "templates" / "sentiment.jinja2"))}

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "fewshot_cfg_string",
        "ext_template_cfg_string",
    ],
)
def test_sentiment_config(cfg_string, request):
    cfg_string = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg_string)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]

    # also test nlp config from a dict in add_pipe
    component_cfg = dict(orig_config["components"]["llm"])
    component_cfg.pop("factory")

    nlp2 = spacy.blank("en")
    nlp2.add_pipe("llm", config=component_cfg)
    assert nlp2.pipe_names == ["llm"]


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "fewshot_cfg_string",
        "ext_template_cfg_string",
    ],
)
def test_sentiment_predict(cfg_string, request):
    """Use OpenAI to get zero-shot sentiment results.
    Note that this test may fail randomly, as the LLM's output is unguaranteed to be consistent/predictable
    """
    cfg = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    if cfg_string != "ext_template_cfg_string":
        assert nlp("This is horrible.")._.sentiment == 0.0
        assert 0 < nlp("This is meh.")._.sentiment <= 0.5
        assert nlp("This is perfect.")._.sentiment == 1.0


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "cfg_string_field",
    [
        ("zeroshot_cfg_string", None),
        ("fewshot_cfg_string", None),
        ("zeroshot_cfg_string", "sentiment_x"),
    ],
)
def test_sentiment_io(cfg_string_field, request):
    cfg_string, field = cfg_string_field
    cfg = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg)
    if field:
        orig_config["components"]["llm"]["task"]["field"] = field
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]
    # ensure you can save a pipeline to disk and run it after loading
    with make_tempdir() as tmpdir:
        nlp.to_disk(tmpdir)
        nlp2 = spacy.load(tmpdir)
    assert nlp2.pipe_names == ["llm"]
    score = getattr(nlp2("This is perfect.")._, field if field else "sentiment")
    if cfg_string != "ext_template_cfg_string":
        assert score == 1


def test_jinja_template_rendering_without_examples():
    """Test if jinja template renders as we expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    nlp = spacy.blank("en")
    text = "They're indifferent."
    doc = nlp.make_doc(text)

    sentiment_task = make_sentiment_task(examples=None)
    prompt = list(sentiment_task.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == f"""
Analyse whether the text surrounded by ''' is positive or negative. Respond with a float value between 0 and 1. 1 represents an exclusively positive sentiment, 0 an exclusively negative sentiment.

Text:
'''
{text}
'''
Answer:""".strip()
    )


@pytest.mark.parametrize(
    "examples_path",
    [
        str(EXAMPLES_DIR / "sentiment.json"),
        str(EXAMPLES_DIR / "sentiment.yml"),
        str(EXAMPLES_DIR / "sentiment.jsonl"),
    ],
)
def test_jinja_template_rendering_with_examples(examples_path):
    """Test if jinja2 template renders as expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    nlp = spacy.blank("en")
    text = "It was the happiest day of her life."
    doc = nlp.make_doc(text)

    sentiment_task = make_sentiment_task(examples=fewshot_reader(examples_path))
    prompt = list(sentiment_task.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == """
Analyse whether the text surrounded by ''' is positive or negative. Respond with a float value between 0 and 1. 1 represents an exclusively positive sentiment, 0 an exclusively negative sentiment.
Below are some examples (only use these as a guide):

Text:
'''
This is horrifying.
'''
Answer: 0.0

Text:
'''
This is underwhelming.
'''
Answer: 0.25

Text:
'''
This is ok.
'''
Answer: 0.5

Text:
'''
I'm looking forward to this!
'''
Answer: 1.0

Text:
'''
It was the happiest day of her life.
'''
Answer:""".strip()
    )


def test_external_template_actually_loads():
    template_path = str(TEMPLATES_DIR / "sentiment.jinja2")
    template = file_reader(template_path)
    text = "There is a silver lining."
    nlp = spacy.blank("en")
    doc = nlp.make_doc(text)

    sentiment_task = make_sentiment_task(template=template)
    prompt = list(sentiment_task.generate_prompts([doc]))[0][0][0]
    assert (
        prompt.strip()
        == f"""
Text: {text}
Sentiment:
""".strip()
    )


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_sentiment_score(request):
    """Test scoring mechanism."""
    cfg = request.getfixturevalue("zeroshot_cfg_string")
    orig_config = Config().from_str(cfg)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)

    sent_diff = 0.2
    doc1 = nlp("This works well.")
    doc2 = doc1.copy()
    doc2._.sentiment -= sent_diff
    assert numpy.isclose(
        nlp.get_pipe("llm").score([Example(doc1, doc2)])["acc_sentiment"], 1 - sent_diff
    )



================================================
FILE: spacy_llm/tests/tasks/test_span_utils.py
================================================
import pytest

from spacy_llm.tasks.span import SpanReason

SPAN_REASON_FROM_STR_TEST_CASES = {
    "invalid_order_no_error": (
        "1. Golden State Warriors | BASKETBALL_TEAM | is a basketball team in the NBA | True",
        SpanReason(
            text="Golden State Warriors",
            is_entity=False,
            label="is a basketball team in the NBA",
            reason="True",
        ),
    ),
    "invalid_number_of_components": (
        "1. Golden State Warriors | BASKETBALL_TEAM | OTHER SECTION | MORE THINGS | is a basketball team in the NBA | True",
        ValueError(),
    ),
    "valid_entity_numbered": (
        "1. Golden State Warriors | True | BASKETBALL_TEAM | is a basketball team in the NBA",
        SpanReason(
            text="Golden State Warriors",
            is_entity=True,
            label="BASKETBALL_TEAM",
            reason="is a basketball team in the NBA",
        ),
    ),
    "valid_entity_unnumbered": (
        "Golden State Warriors | True | BASKETBALL_TEAM | is a basketball team in the NBA",
        SpanReason(
            text="Golden State Warriors",
            is_entity=True,
            label="BASKETBALL_TEAM",
            reason="is a basketball team in the NBA",
        ),
    ),
}


@pytest.mark.parametrize(
    "response, expected",
    SPAN_REASON_FROM_STR_TEST_CASES.values(),
    ids=SPAN_REASON_FROM_STR_TEST_CASES.keys(),
)
def test_span_reason_from_str(response: str, expected: SpanReason):
    try:
        span_reason = SpanReason.from_str(response)
    except ValueError:
        assert isinstance(expected, ValueError)
    else:
        assert span_reason == expected



================================================
FILE: spacy_llm/tests/tasks/test_spancat.py
================================================
from pathlib import Path
from typing import Callable, List, Tuple, cast

import pytest
import spacy
from confection import Config
from spacy.language import Language
from spacy.tokens import Span
from spacy.training import Example
from spacy.util import make_tempdir

from spacy_llm.pipeline import LLMWrapper
from spacy_llm.registry import fewshot_reader, lowercase_normalizer, strip_normalizer
from spacy_llm.tasks import make_spancat_task_v3
from spacy_llm.tasks.spancat import SpanCatTask
from spacy_llm.tasks.util import find_substrings
from spacy_llm.ty import LabeledTask, ShardingLLMTask
from spacy_llm.util import assemble_from_config, split_labels

from ..compat import has_openai_key

EXAMPLES_DIR = Path(__file__).parent / "examples"


@pytest.fixture
def examples_dir():
    return EXAMPLES_DIR


@pytest.fixture
def noop_config():
    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.SpanCat.v3"
    labels = ["PER", "ORG", "LOC", "DESTINATION"]

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str((Path(__file__).parent / "examples" / "spancat.yml"))}

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "test.NoOpModel.v1"
    output = 1. Bob | True | PER | is the name of a person
        2. Alice | True | PER | is the name of a person
    """


@pytest.fixture
def fewshot_cfg_string():
    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.SpanCat.v3"
    labels = ["PER", "ORG", "LOC", "DESTINATION"]

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str((Path(__file__).parent / "examples" / "spancat.yml"))}

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """


@pytest.fixture
def ext_template_cfg_string():
    """Simple zero-shot config with an external template"""

    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]
    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.SpanCat.v3"
    description = "This is a description"
    labels = ["PER", "ORG", "LOC", "DESTINATION"]

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str((Path(__file__).parent / "examples" / "spancat.json"))}

    [components.llm.task.template]
    @misc = "spacy.FileReader.v1"
    path = {str((Path(__file__).parent / "templates" / "spancat.jinja2"))}

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """


@pytest.fixture(
    params=[
        "fewshot_cfg_string",
        "ext_template_cfg_string",
    ]
)
def config(request) -> Config:
    cfg_str = request.getfixturevalue(request.param)
    config = Config().from_str(cfg_str)
    return config


@pytest.fixture
def nlp(config: Config) -> Language:
    nlp = assemble_from_config(config)
    return nlp


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_spancat_config(config: Config):
    nlp = assemble_from_config(config)
    assert nlp.pipe_names == ["llm"]

    pipe = nlp.get_pipe("llm")
    assert isinstance(pipe, LLMWrapper)
    assert isinstance(pipe.task, ShardingLLMTask)

    labels = config["components"]["llm"]["task"]["labels"]
    labels = split_labels(labels)
    task = pipe.task
    assert isinstance(task, LabeledTask)
    assert sorted(task.labels) == sorted(tuple(labels))
    assert pipe.labels == task.labels
    assert nlp.pipe_labels["llm"] == list(task.labels)


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_spancat_predict(nlp: Language):
    """Use OpenAI to get zero-shot spancat results.
    Note that this test may fail randomly, as the LLM's output is unguaranteed to be consistent/predictable
    """
    text = "Marc and Bob both live in Ireland."
    doc = nlp(text)
    assert len(doc.spans["sc"]) > 0
    for ent in doc.spans["sc"]:
        assert ent.label_ in ["PER", "ORG", "LOC", "DESTINATION"]


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_spancat_io(nlp: Language):
    assert nlp.pipe_names == ["llm"]
    # ensure you can save a pipeline to disk and run it after loading
    with make_tempdir() as tmpdir:
        nlp.to_disk(tmpdir)
        nlp2 = spacy.load(tmpdir)
    assert nlp2.pipe_names == ["llm"]
    text = "Marc and Bob both live in Ireland."
    doc = nlp2(text)
    assert len(doc.spans["sc"]) > 0
    for ent in doc.spans["sc"]:
        assert ent.label_ in ["PER", "ORG", "LOC", "DESTINATION"]


@pytest.mark.parametrize(
    "text,input_strings,result_strings,result_offsets",
    [
        (
            "Felipe and Jaime went to the library.",
            ["Felipe", "Jaime", "library"],
            ["Felipe", "Jaime", "library"],
            [(0, 6), (11, 16), (29, 36)],
        ),  # simple
        (
            "The Manila Observatory was founded in 1865 in Manila.",
            ["Manila", "The Manila Observatory"],
            ["Manila", "Manila", "The Manila Observatory"],
            [(4, 10), (46, 52), (0, 22)],
        ),  # overlapping and duplicated
        (
            "Take the road from downtown and turn left at the public market.",
            ["public market", "downtown"],
            ["public market", "downtown"],
            [(49, 62), (19, 27)]
            # flipped
        ),
    ],
)
def test_ensure_offsets_correspond_to_substrings(
    text, input_strings, result_strings, result_offsets
):
    offsets = find_substrings(text, input_strings)
    # Compare strings instead of offsets, but we need to get
    # those strings first from the text
    assert result_offsets == offsets
    found_substrings = [text[start:end] for start, end in offsets]
    assert result_strings == found_substrings


@pytest.mark.parametrize(
    "text,response,gold_spans",
    [
        # simple
        (
            "Jean Jacques and Jaime went to the library.",
            "1. Jean Jacques | True | PER | is the name of a person\n"
            "2. Jaime | True | PER | is the name of a person\n"
            "3. library | True | LOC | is a place you can go to with lots of books\n",
            [("Jean Jacques", "PER"), ("Jaime", "PER"), ("library", "LOC")],
        ),
        # overlapping: should only return all spans
        (
            "The Manila Observatory was founded in 1865.",
            "1. The Manila Observatory | True | LOC | is a place\n"
            "2. Manila | True | LOC | is a city\n"
            "3. Manila Observatory | True | LOC | is a place\n",
            [
                ("The Manila Observatory", "LOC"),
                ("Manila", "LOC"),
                ("Manila Observatory", "LOC"),
            ],
        ),
    ],
    ids=["simple", "overlapping"],
)
def test_spancat_matching_shot_task(text: str, response: str, gold_spans):
    labels = "PER,ORG,LOC"
    llm_spancat = make_spancat_task_v3(examples=[], labels=labels)
    # Prepare doc
    nlp = spacy.blank("en")
    doc_in = nlp.make_doc(text)
    # Pass to the parser
    # Note: parser() returns a list so we get what's inside
    doc_out = list(llm_spancat.parse_responses([[doc_in]], [[response]]))[0]
    pred_spans = [(span.text, span.label_) for span in doc_out.spans["sc"]]
    assert pred_spans == gold_spans


@pytest.mark.parametrize(
    "response,normalizer,gold_spans",
    [
        (
            "1. Jean Jacques | True | PER | is the name of a person\n"
            "2. Jaime | True | PER | is the name of a person\n",
            None,
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "1. Jean Jacques | True | PER | is the name of a person\n"
            "2. Jaime | True | PER | is the name of a person\n",
            strip_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "1. Jean Jacques | True | PER | is the name of a person\n"
            "2. Jaime | True | PER | is the name of a person\n",
            lowercase_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "1. Jean Jacques | True | per | is the name of a person\n"
            "2. Jaime | True | per | is the name of a person\n",
            strip_normalizer(),
            [],
        ),
        (
            "1. Jean Jacques | True | PER | is the name of a person\n"
            "2. Jaime | True | PER | is the name of a person\n",
            None,
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "1. Jean Jacques | True | per | is the name of a person\n"
            "2. Jaime | True | PER | is the name of a person\n",
            lowercase_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "1. Jean Jacques | True | per | is the name of a person\n"
            "2. Jaime | True | per | is the name of a person\n"
            "3. library | True | Org | is an organization\n",
            lowercase_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER"), ("library", "ORG")],
        ),
        (
            "1. Jean Jacques | True | per | is the name of a person\n"
            "2. Jaime | True | per | is the name of a person\n"
            "3. library | True | RANDOM | is an organization\n",
            lowercase_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
    ],
)
def test_spancat_labels(
    response: str, normalizer: Callable[[str], str], gold_spans: List[Tuple[str, str]]
):
    text = "Jean Jacques and Jaime went to the library."
    labels = "PER,ORG,LOC"
    llm_spancat = make_spancat_task_v3(
        examples=[], labels=labels, normalizer=normalizer
    )
    # Prepare doc
    nlp = spacy.blank("en")
    doc_in = nlp.make_doc(text)
    # Pass to the parser
    # Note: parser() returns a list
    doc_out = list(llm_spancat.parse_responses([[doc_in]], [[response]]))[0]
    pred_spans = [(span.text, span.label_) for span in doc_out.spans["sc"]]
    assert pred_spans == gold_spans


@pytest.mark.parametrize(
    "response,alignment_mode,gold_spans",
    [
        (
            "1. Jacq | True | PER | is a person's name",
            "strict",
            [],
        ),
        (
            "1. Jacq | True | PER | is a person's name",
            "contract",
            [],
        ),
        (
            "1. Jacq | True | PER | is a person's name",
            "expand",
            [("Jacques", "PER")],
        ),
        (
            "1. Jean J | True | PER | is a person's name",
            "contract",
            [("Jean", "PER")],
        ),
        (
            "1. Jean Jacques | True | PER | is a person's name",
            "strict",
            [("Jean Jacques", "PER")],
        ),
        (
            "1. random | True | PER | is a person's name",
            "expand",
            [],
        ),
    ],
    ids=["strict_1", "contract_1", "expand_1", "strict_2", "contract_2", "expand_2"],
)
def test_spancat_alignment(response, alignment_mode, gold_spans):
    text = "Jean Jacques and Jaime went to the library."
    labels = "PER,ORG,LOC"
    llm_spancat = make_spancat_task_v3(
        examples=[], labels=labels, alignment_mode=alignment_mode
    )
    # Prepare doc
    nlp = spacy.blank("en")
    doc_in = nlp.make_doc(text)
    # Pass to the parser
    # Note: parser() returns a list
    doc_out = list(llm_spancat.parse_responses([[doc_in]], [[response]]))[0]
    pred_spans = [(span.text, span.label_) for span in doc_out.spans["sc"]]
    assert pred_spans == gold_spans


def test_invalid_alignment_mode():
    labels = "PER,ORG,LOC"
    with pytest.raises(ValueError, match="Unsupported alignment mode 'invalid"):
        make_spancat_task_v3(examples=[], labels=labels, alignment_mode="invalid")


@pytest.mark.parametrize(
    "response, case_sensitive, gold_spans",
    [
        (
            "1. Jean | True | PER | is a person's name",
            False,
            [("jean", "PER")],
        ),
        (
            "1. Jean | True | PER | is a person's name",
            True,
            [("Jean", "PER")],
        ),
        (
            "1. jean | True | PER | is a person's name\n"
            "2. Jean | True | PER | is a person's name\n"
            "3. Jean Foundation | True | ORG | is the name of an Organization name",
            False,
            [("jean", "PER"), ("Jean", "PER"), ("Jean Foundation", "ORG")],
        ),
    ],
    ids=[
        "single_ent_case_insensitive",
        "single_ent_case_sensitive",
        "multiple_ents_case_insensitive",
    ],
)
def test_spancat_matching(response, case_sensitive, gold_spans):
    text = "This guy jean (or Jean) is the president of the Jean Foundation."
    labels = "PER,ORG,LOC"
    llm_spancat = make_spancat_task_v3(
        examples=[], labels=labels, case_sensitive_matching=case_sensitive
    )
    # Prepare doc
    nlp = spacy.blank("en")
    doc_in = nlp.make_doc(text)
    # Pass to the parser
    # Note: parser() returns a list
    doc_out = list(llm_spancat.parse_responses([[doc_in]], [[response]]))[0]
    pred_spans = [(span.text, span.label_) for span in doc_out.spans["sc"]]
    assert pred_spans == gold_spans


def test_jinja_template_rendering_without_examples():
    """Test if jinja2 template renders as expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    labels = "PER,ORG,LOC"
    nlp = spacy.blank("en")
    doc = nlp.make_doc("Alice and Bob went to the supermarket")
    llm_spancat = make_spancat_task_v3(labels=labels)
    prompt = list(llm_spancat.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == """
You are an expert Entity Recognition system.
Your task is to accept Text as input and extract named entities.
The entities you extract can overlap with each other.

Entities must have one of the following labels: LOC, ORG, PER.
If a span is not an entity label it: `==NONE==`.



Here is an example of the output format for a paragraph using different labels than this task requires.
Only use this output format but use the labels provided
above instead of the ones defined in the example below.
Do not output anything besides entities in this output format.
Output entities in the order they occur in the input paragraph regardless of label.

Q: Given the paragraph below, identify a list of entities, and for each entry explain why it is or is not an entity:

Paragraph: Sriracha sauce goes really well with hoisin stir fry, but you should add it after you use the wok.
Answer:
1. Sriracha sauce | True | INGREDIENT | is an ingredient to add to a stir fry
2. really well | False | ==NONE== | is a description of how well sriracha sauce goes with hoisin stir fry
3. hoisin stir fry | True | DISH | is a dish with stir fry vegetables and hoisin sauce
4. wok | True | EQUIPMENT | is a piece of cooking equipment used to stir fry ingredients

Paragraph: Alice and Bob went to the supermarket
Answer:
""".strip()
    )


@pytest.mark.parametrize(
    "examples_path",
    [
        str(EXAMPLES_DIR / "spancat.json"),
        str(EXAMPLES_DIR / "spancat.yml"),
        str(EXAMPLES_DIR / "spancat.jsonl"),
    ],
)
def test_jinja_template_rendering_with_examples(examples_path: Path):
    """Test if jinja2 template renders as expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    labels = "PER,ORG,LOC,DESTINATION"
    nlp = spacy.blank("en")
    doc = nlp.make_doc("Alice and Bob went to the supermarket")

    examples = fewshot_reader(examples_path)
    llm_spancat = make_spancat_task_v3(labels=labels, examples=examples)
    prompt = list(llm_spancat.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == """
You are an expert Entity Recognition system.
Your task is to accept Text as input and extract named entities.
The entities you extract can overlap with each other.

Entities must have one of the following labels: DESTINATION, LOC, ORG, PER.
If a span is not an entity label it: `==NONE==`.


Q: Given the paragraph below, identify a list of entities, and for each entry explain why it is or is not an entity:

Paragraph: Jack and Jill went up the hill.
Answer:
1. Jack | True | PER | is the name of a person
2. Jill | True | PER | is the name of a person
3. went up | False | ==NONE== | is a verb
4. hill | True | LOC | is a location
5. hill | True | DESTINATION | is a destination

Paragraph: Alice and Bob went to the supermarket
Answer:
""".strip()
    )


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize("n_detections", [0, 1, 2])
def test_spancat_scoring(fewshot_cfg_string: str, n_detections: int):
    config = Config().from_str(fewshot_cfg_string)
    nlp = assemble_from_config(config)

    examples = []
    for text in ["Alice works with Bob.", "Bob lives with Alice."]:
        predicted = nlp.make_doc(text)
        reference = nlp.make_doc(text)
        ent1 = Span(reference, 0, 1, label="PER")
        ent2 = Span(reference, 3, 4, label="PER")
        reference.spans["sc"] = [ent1, ent2][:n_detections]
        examples.append(Example(predicted, reference))

    scores = nlp.evaluate(examples)
    assert scores["spans_sc_p"] == n_detections / 2
    assert scores["spans_sc_r"] == (1 if n_detections != 0 else 0)
    assert scores["spans_sc_f"] == (
        pytest.approx(0.666666666) if n_detections == 1 else n_detections / 2
    )


@pytest.mark.parametrize("n_prompt_examples", [-1, 0, 1, 2])
def test_spancat_init(noop_config: str, n_prompt_examples: bool):
    config = Config().from_str(noop_config)
    config["components"]["llm"]["task"]["labels"] = ["PER", "LOC", "DESTINATION"]
    config["components"]["llm"]["task"]["examples"] = []
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp = assemble_from_config(config)

    examples = []

    for text in [
        "Alice works with Bob in London.",
        "Bob lives with Alice in Manchester.",
    ]:
        predicted = nlp.make_doc(text)
        reference = predicted.copy()

        reference.spans["sc"] = [
            Span(reference, 0, 1, label="PER"),
            Span(reference, 3, 4, label="PER"),
            Span(reference, 5, 6, label="LOC"),
        ]

        examples.append(Example(predicted, reference))

    _, llm = nlp.pipeline[0]
    task: SpanCatTask = llm._task

    nlp.config["initialize"]["components"]["llm"] = {
        "n_prompt_examples": n_prompt_examples
    }

    nlp.initialize(lambda: examples)

    assert set(task._label_dict.values()) == {"PER", "LOC", "DESTINATION"}
    if n_prompt_examples == -1:
        assert len(task._prompt_examples) == len(examples)
    else:
        assert len(task._prompt_examples) == n_prompt_examples

    if n_prompt_examples > 0:
        for eg in task._prompt_examples:
            prompt_example_labels = {ent.label for ent in eg.spans}
            if "==NONE==" not in prompt_example_labels:
                prompt_example_labels.add("==NONE==")
            assert prompt_example_labels == {"==NONE==", "PER", "LOC"}


def test_spancat_serde(noop_config):
    config = Config().from_str(noop_config)
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp1 = assemble_from_config(config)
        nlp2 = assemble_from_config(config)

    labels = {"loc": "LOC", "per": "PER"}

    task1 = cast(SpanCatTask, nlp1.get_pipe("llm").task)
    task2 = cast(SpanCatTask, nlp2.get_pipe("llm").task)

    # Artificially add labels to task1
    task1._label_dict = labels
    task2._label_dict = dict()

    assert task1._label_dict == labels
    assert task2._label_dict == dict()

    b = nlp1.to_bytes()
    nlp2.from_bytes(b)

    assert task1._label_dict == task2._label_dict == labels



================================================
FILE: spacy_llm/tests/tasks/test_summarization.py
================================================
import re
from pathlib import Path

import pytest
import spacy
from confection import Config
from spacy.util import make_tempdir

from spacy_llm.pipeline import LLMWrapper
from spacy_llm.registry import fewshot_reader, file_reader
from spacy_llm.ty import ShardingLLMTask
from spacy_llm.util import assemble_from_config

from ...tasks import make_summarization_task
from ..compat import has_openai_key

EXAMPLES_DIR = Path(__file__).parent / "examples"
TEMPLATES_DIR = Path(__file__).parent / "templates"


@pytest.fixture
def zeroshot_cfg_string():
    return """
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.Summarization.v1"
    max_n_words = 20

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """


@pytest.fixture
def fewshot_cfg_string():
    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.Summarization.v1"
    max_n_words = 20

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str((Path(__file__).parent / "examples" / "summarization.yml"))}

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """


@pytest.fixture
def ext_template_cfg_string():
    """Simple zero-shot config with an external template"""

    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]
    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.Summarization.v1"
    max_n_words = 20

    [components.llm.task.template]
    @misc = "spacy.FileReader.v1"
    path = {str((Path(__file__).parent / "templates" / "summarization.jinja2"))}

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """


@pytest.fixture
def noop_config():
    return """
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.Summarization.v1"

    [components.llm.model]
    @llm_models = "test.NoOpModel.v1"
    """


@pytest.fixture
def example_text() -> str:
    """Returns string to be used as example in tests."""
    return (
        "The atmosphere of Earth is the layer of gases, known collectively as air, retained by Earth's gravity "
        "that surrounds the planet and forms its planetary atmosphere. The atmosphere of Earth creates pressure, "
        "absorbs most meteoroids and ultraviolet solar radiation, warms the surface through heat retention "
        "(greenhouse effect), allowing life and liquid water to exist on the Earth's surface, and reduces "
        "temperature extremes between day and night (the diurnal temperature variation)."
    )


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "fewshot_cfg_string",
        "ext_template_cfg_string",
    ],
)
def test_summarization_config(cfg_string, request):
    cfg_string = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg_string)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]

    # also test nlp config from a dict in add_pipe
    component_cfg = dict(orig_config["components"]["llm"])
    component_cfg.pop("factory")

    nlp2 = spacy.blank("en")
    nlp2.add_pipe("llm", config=component_cfg)
    assert nlp2.pipe_names == ["llm"]

    pipe = nlp.get_pipe("llm")
    assert isinstance(pipe, LLMWrapper)
    assert isinstance(pipe.task, ShardingLLMTask)


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "fewshot_cfg_string",
        "ext_template_cfg_string",
    ],
)
def test_summarization_predict(cfg_string, example_text, request):
    """Use OpenAI to get summarize text.
    Note that this test may fail randomly, as the LLM's output is unguaranteed to be consistent/predictable
    """
    orig_cfg_string = cfg_string
    cfg_string = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg_string)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)

    # One of the examples exceeds the set max_n_words, so we expect a warning to be emitted.
    if orig_cfg_string == "fewshot_cfg_string":
        with pytest.warns(
            UserWarning,
            match=re.escape(
                "The provided example 'Life is a quality th...' has a summary of length 28, but `max_n_words` == 20."
            ),
        ):
            doc = nlp(example_text)
    else:
        doc = nlp(example_text)

    # Check whether a non-empty summary was written and we are somewhat close to the desired upper length limit.
    assert 0 < len(doc._.summary)
    if "ext" not in orig_cfg_string:
        nlp.select_pipes(disable=["llm"])
        assert (
            len(nlp(doc._.summary))
            <= orig_config["components"]["llm"]["task"]["max_n_words"] * 1.5
        )


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "cfg_string_and_field",
    [
        ("zeroshot_cfg_string", None),
        ("fewshot_cfg_string", None),
        ("ext_template_cfg_string", None),
        ("zeroshot_cfg_string", "summary_x"),
    ],
)
def test_summarization_io(cfg_string_and_field, example_text, request):
    cfg_string, field = cfg_string_and_field
    orig_cfg_string = cfg_string
    cfg_string = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg_string)
    if field:
        orig_config["components"]["llm"]["task"]["field"] = field
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]
    # ensure you can save a pipeline to disk and run it after loading
    with make_tempdir() as tmpdir:
        nlp.to_disk(tmpdir)
        nlp2 = spacy.load(tmpdir)
    assert nlp2.pipe_names == ["llm"]

    if orig_cfg_string == "fewshot_cfg_string":
        with pytest.warns(
            UserWarning,
            match=re.escape(
                "The provided example 'Life is a quality th...' has a summary of length 28, but `max_n_words` == 20."
            ),
        ):
            doc = nlp2(example_text)
    else:
        doc = nlp2(example_text)

    field = "summary" if field is None else field
    nlp2.select_pipes(disable=["llm"])
    assert 0 < len(nlp2(getattr(doc._, field)))
    if "ext" not in orig_cfg_string:
        assert (
            len(nlp2(getattr(doc._, field)))
            <= orig_config["components"]["llm"]["task"]["max_n_words"] * 1.5
        )


def test_jinja_template_rendering_without_examples(example_text):
    """Test if jinja template renders as we expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    nlp = spacy.blank("en")
    doc = nlp.make_doc(example_text)

    llm_ner = make_summarization_task(examples=None, max_n_words=10)
    prompt = list(llm_ner.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == f"""
You are an expert summarization system. Your task is to accept Text as input and summarize the Text in a concise way.
The summary must not, under any circumstances, contain more than 10 words.
Here is the Text that needs to be summarized:
'''
{example_text}
'''
Summary:""".strip()
    )


@pytest.mark.parametrize(
    "examples_path",
    [
        str(EXAMPLES_DIR / "summarization.json"),
        str(EXAMPLES_DIR / "summarization.yml"),
        str(EXAMPLES_DIR / "summarization.jsonl"),
    ],
)
def test_jinja_template_rendering_with_examples(examples_path, example_text):
    """Test if jinja2 template renders as expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    nlp = spacy.blank("en")
    doc = nlp.make_doc(example_text)

    prompt_examples = fewshot_reader(examples_path)
    llm_ner = make_summarization_task(examples=prompt_examples, max_n_words=20)

    with pytest.warns(
        UserWarning,
        match=re.escape(
            "The provided example 'Life is a quality th...' has a summary of length 28, but `max_n_words` == 20."
        ),
    ):
        prompt = list(llm_ner.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == f"""
You are an expert summarization system. Your task is to accept Text as input and summarize the Text in a concise way.
The summary must not, under any circumstances, contain more than 20 words.
Below are some examples (only use these as a guide):

Text:
'''
The United Nations, referred to informally as the UN, is an intergovernmental organization whose stated purposes are to maintain international peace and security, develop friendly relations among nations, achieve international cooperation, and serve as a centre for harmonizing the actions of nations. It is the world's largest international organization. The UN is headquartered on international territory in New York City, and the organization has other offices in Geneva, Nairobi, Vienna, and The Hague, where the International Court of Justice is headquartered.

The UN was established after World War II with the aim of preventing future world wars, and succeeded the League of Nations, which was characterized as ineffective. On 25 April 1945, 50 nations met in San Francisco, California for a conference and started drafting the UN Charter, which was adopted on 25 June 1945. The charter took effect on 24 October 1945, when the UN began operations. The organization's objectives, as defined by its charter, include maintaining international peace and security, protecting human rights, delivering humanitarian aid, promoting sustainable development, and upholding international law. At its founding, the UN had 51 member states; as of 2023, it has 193 â€“ almost all of the world's sovereign states.
'''
Summary:
'''
UN is an intergovernmental organization to foster international peace, security, and cooperation. Established after WW2 with 51 members, now 193.
'''

Text:
'''
Life is a quality that distinguishes matter that has biological processes, such as signaling and self-sustaining processes, from matter that does not, and is defined by the capacity for growth, reaction to stimuli, metabolism, energy transformation, and reproduction. Various forms of life exist, such as plants, animals, fungi, protists, archaea, and bacteria. Biology is the science that studies life.

The gene is the unit of heredity, whereas the cell is the structural and functional unit of life. There are two kinds of cells, prokaryotic and eukaryotic, both of which consist of cytoplasm enclosed within a membrane and contain many biomolecules such as proteins and nucleic acids. Cells reproduce through a process of cell division, in which the parent cell divides into two or more daughter cells and passes its genes onto a new generation, sometimes producing genetic variation.

Organisms, or the individual entities of life, are generally thought to be open systems that maintain homeostasis, are composed of cells, have a life cycle, undergo metabolism, can grow, adapt to their environment, respond to stimuli, reproduce and evolve over multiple generations. Other definitions sometimes include non-cellular life forms such as viruses and viroids, but they are usually excluded because they do not function on their own; rather, they exploit the biological processes of hosts.
'''
Summary:
'''
Life is a quality defined by biological processes, including reproduction, genetics, and metabolism. There are two types of cells and organisms that can grow, respond, reproduce, and evolve.
'''

Here is the Text that needs to be summarized:
'''
{example_text}
'''
Summary:
""".strip()
    )


def test_external_template_actually_loads(example_text):
    template_path = str(TEMPLATES_DIR / "summarization.jinja2")
    template = file_reader(template_path)
    nlp = spacy.blank("en")
    doc = nlp.make_doc(example_text)

    llm_ner = make_summarization_task(template=template)
    prompt = list(llm_ner.generate_prompts([doc]))[0][0][0]
    assert (
        prompt.strip()
        == """
This is a test summarization template.
Here is the text: The atmosphere of Earth is the layer of gases, known collectively as air, retained by Earth's gravity that surrounds the planet and forms its planetary atmosphere. The atmosphere of Earth creates pressure, absorbs most meteoroids and ultraviolet solar radiation, warms the surface through heat retention (greenhouse effect), allowing life and liquid water to exist on the Earth's surface, and reduces temperature extremes between day and night (the diurnal temperature variation).
""".strip()
    )


def test_ner_serde(noop_config):
    config = Config().from_str(noop_config)
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp1 = assemble_from_config(config)
        nlp2 = assemble_from_config(config)
    nlp2.from_bytes(nlp1.to_bytes())


def test_ner_to_disk(noop_config, tmp_path: Path):
    config = Config().from_str(noop_config)
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp1 = assemble_from_config(config)
        nlp2 = assemble_from_config(config)

    path = tmp_path / "model"
    nlp1.to_disk(path)

    cfgs = list(path.rglob("cfg"))
    assert len(cfgs) == 1

    nlp2.from_disk(path)



================================================
FILE: spacy_llm/tests/tasks/test_textcat.py
================================================
import json
from pathlib import Path
from typing import Iterable

import pytest
import spacy
import srsly
from confection import Config
from spacy.training import Example
from spacy.util import make_tempdir

from spacy_llm.pipeline import LLMWrapper
from spacy_llm.registry import fewshot_reader, file_reader, lowercase_normalizer
from spacy_llm.registry import registry
from spacy_llm.tasks.textcat import TextCatTask, make_textcat_task_v3
from spacy_llm.ty import LabeledTask, ShardingLLMTask
from spacy_llm.util import assemble_from_config, split_labels

from ..compat import has_openai_key

EXAMPLES_DIR = Path(__file__).parent / "examples"
TEMPLATES_DIR = Path(__file__).parent / "templates"


@pytest.fixture
def zeroshot_cfg_string():
    return """
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.TextCat.v1"
    labels = "Recipe"
    exclusive_classes = true

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """


@pytest.fixture
def fewshot_cfg_string():
    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.TextCat.v1"
    labels = "Recipe"
    exclusive_classes = true

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str(EXAMPLES_DIR / "textcat.yml")}

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """


@pytest.fixture
def ext_template_cfg_string():
    """Simple zero-shot config with an external template"""

    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.TextCat.v2"
    labels = ["Recipe"]
    exclusive_classes = true

    [components.llm.task.template]
    @misc = "spacy.FileReader.v1"
    path = {str((Path(__file__).parent / "templates" / "textcat.jinja2"))}

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """


@pytest.fixture
def zeroshot_cfg_string_v3_lds():
    return """
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.TextCat.v3"
    labels = "Recipe"
    exclusive_classes = true

    [components.llm.task.label_definitions]
    Recipe = "A recipe is a set of instructions for preparing a meal, including a list of the ingredients required."

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v2"
    """


@pytest.fixture
def binary():
    text = "Get 1 cup of sugar, half a cup of butter, and mix them together to make a cream"
    labels = "Recipe"
    gold_cats = ["Recipe"]
    exclusive_classes = True
    examples_path = str(EXAMPLES_DIR / "textcat_binary.yml")
    return text, labels, gold_cats, exclusive_classes, examples_path


@pytest.fixture
def multilabel_excl():
    text = "You need to increase the temperature when baking, it looks undercooked."
    labels = "Recipe,Feedback,Comment"
    gold_cats = ["Recipe", "Feedback", "Comment"]
    exclusive_classes = True
    examples_path = str(EXAMPLES_DIR / "textcat_multi_excl.yml")
    return text, labels, gold_cats, exclusive_classes, examples_path


@pytest.fixture
def multilabel_nonexcl():
    text = "I suggest you add some bananas. Mix 3 pieces of banana to your batter before baking."
    labels = "Recipe,Feedback,Comment"
    gold_cats = ["Recipe", "Feedback", "Comment"]
    exclusive_classes = False
    examples_path = str(EXAMPLES_DIR / "textcat_multi_nonexcl.yml")
    return text, labels, gold_cats, exclusive_classes, examples_path


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "task",
    ["binary", "multilabel_nonexcl", "multilabel_excl"],
)
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "fewshot_cfg_string",
        "ext_template_cfg_string",
        "zeroshot_cfg_string_v3_lds",
    ],
)
def test_textcat_config(task, cfg_string, request):
    """Simple test to check if the config loads properly given different settings"""

    task = request.getfixturevalue(task)
    _, labels, _, exclusive_classes, examples = task
    overrides = {
        "components.llm.task.labels": labels,
        "components.llm.task.exclusive_classes": exclusive_classes,
    }

    if cfg_string == "fewshot_cfg_string":
        overrides["components.llm.task.examples.path"] = examples

    cfg_string = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg_string, overrides=overrides)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]

    pipe = nlp.get_pipe("llm")
    assert isinstance(pipe, LLMWrapper)
    assert isinstance(pipe.task, ShardingLLMTask)

    labels = split_labels(labels)
    task = pipe.task
    assert isinstance(task, LabeledTask)
    assert sorted(task.labels) == sorted(tuple(labels))
    assert pipe.labels == task.labels
    assert nlp.pipe_labels["llm"] == list(task.labels)


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize("task", ["binary", "multilabel_nonexcl", "multilabel_excl"])
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "fewshot_cfg_string",
        "ext_template_cfg_string",
        "zeroshot_cfg_string_v3_lds",
    ],
)
def test_textcat_predict(task, cfg_string, request):
    """Use OpenAI to get Textcat results.
    Note that this test may fail randomly, as the LLM's output is unguaranteed
    to be consistent/predictable.
    """
    task = request.getfixturevalue(task)
    text, labels, gold_cats, exclusive_classes, examples = task
    overrides = {
        "components.llm.task.labels": labels,
        "components.llm.task.exclusive_classes": exclusive_classes,
    }

    if cfg_string == "fewshot_cfg_string":
        overrides["components.llm.task.examples.path"] = examples

    cfg_string = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg_string, overrides=overrides)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    doc = nlp(text)
    assert len(doc.cats) >= 0  # can be 0 if binary and negative
    for cat in list(doc.cats.keys()):
        assert cat in gold_cats


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize("task", ["binary", "multilabel_nonexcl", "multilabel_excl"])
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "fewshot_cfg_string",
        "ext_template_cfg_string",
        "zeroshot_cfg_string_v3_lds",
    ],
)
def test_textcat_io(task, cfg_string, request):
    task = request.getfixturevalue(task)
    text, labels, gold_cats, exclusive_classes, examples = task
    overrides = {
        "components.llm.task.labels": labels,
        "components.llm.task.exclusive_classes": exclusive_classes,
    }

    if cfg_string == "fewshot_cfg_string":
        overrides["components.llm.task.examples.path"] = examples

    cfg_string = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg_string, overrides=overrides)

    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]
    # ensure you can save a pipeline to disk and run it after loading
    with make_tempdir() as tmpdir:
        nlp.to_disk(tmpdir)
        nlp2 = spacy.load(tmpdir)
    assert nlp2.pipe_names == ["llm"]
    doc = nlp2(text)
    assert len(doc.cats) >= 0  # can be 0 if binary and negative
    for cat in list(doc.cats.keys()):
        assert cat in gold_cats


def test_textcat_sets_exclusive_classes_if_binary():
    """Test if the textcat task automatically sets exclusive classes to True if binary"""
    llm_textcat = make_textcat_task_v3(labels="Recipe", exclusive_classes=False)
    assert llm_textcat._exclusive_classes


@pytest.mark.parametrize(
    "text,response,expected_score",
    [
        ("Some test text with positive response", "POS", 1.0),
        ("Some test text with negative response", "NEG", 0.0),
        ("Some test text with weird response", "WeIrD OUtpuT", 0.0),
        ("Some test text with lowercase response", "pos", 1.0),
        ("Some test text with lowercase response", "neg", 0.0),
        ("Some test text with unstripped response", "\n\n\nPOS", 1.0),
        ("Some test text with unstripped response", "\n\n\nNEG", 0.0),
    ],
)
def test_textcat_binary_labels_are_correct(text, response, expected_score):
    """Test if positive label for textcat binary is always the label name and the negative
    label is an empty dictionary
    """
    label = "Recipe"
    llm_textcat = make_textcat_task_v3(
        labels=label, exclusive_classes=True, normalizer=lowercase_normalizer()
    )

    nlp = spacy.blank("en")
    doc = nlp(text)
    pred = list(llm_textcat.parse_responses([[doc]], [[response]]))[0]
    assert list(pred.cats.keys())[0] == label
    assert list(pred.cats.values())[0] == expected_score


@pytest.mark.parametrize(
    "text,exclusive_classes,response,expected",
    [
        # fmt: off
        ("Golden path for exclusive", True, "Recipe", ["Recipe"]),
        ("Golden path for non-exclusive", False, "Recipe,Feedback", ["Recipe", "Feedback"]),
        ("Non-exclusive but responded with a single label", False, "Recipe", ["Recipe"]),  # shouldn't matter
        ("Exclusive but responded with multilabel", True, "Recipe,Comment", []),  # don't store anything
        ("Weird outputs for exclusive", True, "reCiPe", ["Recipe"]),
        ("Weird outputs for non-exclusive", False, "reciPE,CoMMeNt,FeedBack", ["Recipe", "Comment", "Feedback"]),
        ("Extra spaces for exclusive", True, "Recipe   ", ["Recipe"]),
        ("Extra spaces for non-exclusive", False, "Recipe,   Comment,    Feedback", ["Recipe", "Comment", "Feedback"]),
        ("One weird value", False, "Recipe,Comment,SomeOtherUnnecessaryLabel", ["Recipe", "Comment"]),
        # fmt: on
    ],
)
def test_textcat_multilabel_labels_are_correct(
    text, exclusive_classes, response, expected
):
    labels = "Recipe,Comment,Feedback"
    llm_textcat = make_textcat_task_v3(
        labels=labels,
        exclusive_classes=exclusive_classes,
        normalizer=lowercase_normalizer(),
    )
    nlp = spacy.blank("en")
    doc = nlp.make_doc(text)
    pred = list(llm_textcat.parse_responses([[doc]], [[response]]))[0]
    # Take only those that have scores
    pred_cats = [cat for cat, score in pred.cats.items() if score == 1.0]
    assert set(pred_cats) == set(expected)


@pytest.mark.parametrize(
    "examples_path",
    [
        str(EXAMPLES_DIR / "textcat_binary.json"),
        str(EXAMPLES_DIR / "textcat_binary.yml"),
        str(EXAMPLES_DIR / "textcat_binary.jsonl"),
    ],
)
def test_jinja_template_rendering_with_examples_for_binary(examples_path, binary):
    """Test if jinja2 template renders as expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    text, labels, _, exclusive_classes, _ = binary
    nlp = spacy.blank("en")
    doc = nlp(text)

    prompt_examples = fewshot_reader(examples_path)
    llm_textcat = make_textcat_task_v3(
        labels=labels,
        examples=prompt_examples,
        exclusive_classes=exclusive_classes,
    )
    prompt = list(llm_textcat.generate_prompts([doc]))[0][0][0]
    assert (
        prompt.strip()
        == """
You are an expert Text Classification system. Your task is to accept Text as input
and provide a category for the text based on the predefined labels.

Classify whether the text below belongs to the Recipe category or not.
If it is a Recipe, answer `POS`. If it is not a Recipe, answer `NEG`.
Do not put any other text in your answer, only one of 'POS' or 'NEG' with nothing before or after.
Below are some examples (only use these as a guide):


Text:
'''
Macaroni and cheese is the best budget meal for students, unhealthy tho
'''

NEG

Text:
'''
2 cups soy sauce, 1/2 lb. of chicken, 1/2 cup vinegar, then salt and paper, mix then well and you get an adobo
'''

POS

Text:
'''
You can still add more layers to that croissant, get extra butter and add a few cups of flour
'''

POS


Here is the text that needs classification


Text:
'''
Get 1 cup of sugar, half a cup of butter, and mix them together to make a cream
'''""".strip()
    )


@pytest.mark.parametrize(
    "examples_path",
    [
        str(EXAMPLES_DIR / "textcat_multi_excl.json"),
        str(EXAMPLES_DIR / "textcat_multi_excl.yml"),
        str(EXAMPLES_DIR / "textcat_multi_excl.jsonl"),
    ],
)
def test_jinja_template_rendering_with_examples_for_multilabel_exclusive(
    examples_path, multilabel_excl
):
    text, labels, _, exclusive_classes, _ = multilabel_excl
    nlp = spacy.blank("en")
    doc = nlp(text)

    prompt_examples = fewshot_reader(examples_path)
    llm_textcat = make_textcat_task_v3(
        labels=labels,
        examples=prompt_examples,
        exclusive_classes=exclusive_classes,
    )
    prompt = list(llm_textcat.generate_prompts([doc]))[0][0][0]
    assert (
        prompt.strip()
        == """
You are an expert Text Classification system. Your task is to accept Text as input
and provide a category for the text based on the predefined labels.

Classify the text below to any of the following labels: Comment, Feedback, Recipe

The task is exclusive, so only choose one label from what I provided.
Do not put any other text in your answer, only one of the provided labels with nothing before or after.
Below are some examples (only use these as a guide):


Text:
'''
Macaroni and cheese is the best budget meal for students, unhealthy tho
'''

Comment

Text:
'''
2 cups soy sauce, 1/2 lb. of chicken, 1/2 cup vinegar, then salt and paper, mix then well and you get an adobo
'''

Recipe

Text:
'''
You can still add more layers to that croissant, get extra butter and add a few cups of flour
'''

Feedback


Here is the text that needs classification


Text:
'''
You need to increase the temperature when baking, it looks undercooked.
'''""".strip()
    )


@pytest.mark.parametrize(
    "examples_path",
    [
        str(EXAMPLES_DIR / "textcat_multi_nonexcl.json"),
        str(EXAMPLES_DIR / "textcat_multi_nonexcl.yml"),
        str(EXAMPLES_DIR / "textcat_multi_nonexcl.jsonl"),
    ],
)
def test_jinja_template_rendering_with_examples_for_multilabel_nonexclusive(
    examples_path, multilabel_nonexcl
):
    text, labels, _, exclusive_classes, _ = multilabel_nonexcl
    nlp = spacy.blank("en")
    doc = nlp(text)

    prompt_examples = fewshot_reader(examples_path)
    llm_textcat = make_textcat_task_v3(
        labels=labels,
        examples=prompt_examples,
        exclusive_classes=exclusive_classes,
    )
    prompt = list(llm_textcat.generate_prompts([doc]))[0][0][0]
    assert (
        prompt.strip()
        == """
You are an expert Text Classification system. Your task is to accept Text as input
and provide a category for the text based on the predefined labels.

Classify the text below to any of the following labels: Comment, Feedback, Recipe

The task is non-exclusive, so you can provide more than one label as long as
they're comma-delimited. For example: Label1, Label2, Label3.
Do not put any other text in your answer, only one or more of the provided labels with nothing before or after.
If the text cannot be classified into any of the provided labels, answer `==NONE==`.
Below are some examples (only use these as a guide):


Text:
'''
Macaroni and cheese is the best budget meal for students, unhealthy tho
'''

Comment,Feedback

Text:
'''
2 cups soy sauce, 1/2 lb. of chicken, 1/2 cup vinegar, then salt and paper, mix then well and you get an adobo
'''

Recipe

Text:
'''
You can still add more layers to that croissant, get extra butter and add a few cups of flour
'''

Feedback,Recipe


Here is the text that needs classification


Text:
'''
I suggest you add some bananas. Mix 3 pieces of banana to your batter before baking.
'''""".strip()
    )


@pytest.mark.parametrize(
    "wrong_example,labels,exclusive_classes",
    [
        # fmt: off
        ([{"text": "wrong example for binary", "answer": [0]}], "Label", True),
        ([{"text": "wrong example for multilabel excl", "answer": [12345]}], "Label1,Label2", True),
        ([{"text": "wrong example for multilabel nonexcl", "answer": ["Label1", "Label2"]}], "Label,Label2", False),
        # fmt: on
    ],
)
def test_example_not_following_basemodel(wrong_example, labels, exclusive_classes):
    with make_tempdir() as tmpdir:
        tmp_path = tmpdir / "wrong_example.yml"
        srsly.write_yaml(tmp_path, wrong_example)

        with pytest.raises(ValueError):
            make_textcat_task_v3(
                labels=labels,
                examples=fewshot_reader(tmp_path),
                exclusive_classes=exclusive_classes,
            )


def test_external_template_actually_loads():
    template_path = str(TEMPLATES_DIR / "textcat.jinja2")
    template = file_reader(template_path)
    labels = "Recipe"
    nlp = spacy.blank("en")
    doc = nlp.make_doc("Combine 2 cloves of garlic with soy sauce")

    llm_textcat = make_textcat_task_v3(labels=labels, template=template)
    prompt = list(llm_textcat.generate_prompts([doc]))[0][0][0]
    assert (
        prompt.strip()
        == """
This is a test textcat template. Here is/are the label/s
Recipe

Here is the text: Combine 2 cloves of garlic with soy sauce
""".strip()
    )


INSULTS = [
    "Gobbledygooks!",
    "Filibusters!",
    "Slubberdegullions!",
    "Vampires!",
    "Sycophant!",
    "Kleptomaniacs!",
    "Egoists!",
    "Tramps!",
    "Monopolizers!",
    "Pockmarks!",
    "Belemnite!",
    "Crooks!",
    "Miserable earthworms!",
    "Harlequin!",
    "Parasites!",
    "Macrocephalic baboon!",
    "Brutes!",
    "Pachyrhizus!",
    "Toads!",
    "Gyroscope!",
    "Bougainvillea!",
    "Bloodsuckers!",
    "Nincompoop!",
    "Shipwreckers!",
]


@pytest.mark.parametrize("n_insults", range(len(INSULTS) + 1))
def test_textcat_scoring(zeroshot_cfg_string, n_insults):
    @registry.llm_models("Dummy")
    def factory():
        def b(prompts: Iterable[Iterable[str]]) -> Iterable[Iterable[str]]:
            for _ in prompts:
                yield ["POS"]

        return b

    config = Config().from_str(zeroshot_cfg_string)
    config["components"]["llm"]["model"] = {"@llm_models": "Dummy"}
    config["components"]["llm"]["task"]["labels"] = "Insult"
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp = assemble_from_config(config)

    examples = []

    for i, text in enumerate(INSULTS):
        predicted = nlp.make_doc(text)
        reference = predicted.copy()

        if i < n_insults:
            reference.cats = {"Insult": 1.0}

        examples.append(Example(predicted, reference))

    scores = nlp.evaluate(examples)
    pos = n_insults / len(INSULTS)
    assert scores["cats_micro_p"] == pos
    assert not n_insults or scores["cats_micro_r"] == 1


def test_jinja_template_rendering_with_label_definitions(multilabel_excl):
    text, labels, _, exclusive_classes, _ = multilabel_excl
    nlp = spacy.blank("en")
    doc = nlp(text)

    llm_textcat = make_textcat_task_v3(
        labels=labels,
        label_definitions={
            "Recipe": "A Recipe is a set of instructions to make a food of some kind",
            "Feedback": "Feedback description",
            "Comment": "Comment description",
        },
        exclusive_classes=exclusive_classes,
    )
    prompt = list(llm_textcat.generate_prompts([doc]))[0][0][0]
    assert (
        prompt.strip()
        == """
You are an expert Text Classification system. Your task is to accept Text as input
and provide a category for the text based on the predefined labels.

Classify the text below to any of the following labels: Comment, Feedback, Recipe

The task is exclusive, so only choose one label from what I provided.
Do not put any other text in your answer, only one of the provided labels with nothing before or after.

Below are definitions of each label to help aid you in correctly classifying the text.
Assume these definitions are written by an expert and follow them closely.

Recipe: A Recipe is a set of instructions to make a food of some kind
Feedback: Feedback description
Comment: Comment description


Here is the text that needs classification


Text:
'''
You need to increase the temperature when baking, it looks undercooked.
'''""".strip()
    )


@pytest.fixture
def noop_config():
    return """
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.TextCat.v1"

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "test.NoOpModel.v1"
    """


@pytest.mark.parametrize("n_prompt_examples", [-1, 0, 1, 2])
@pytest.mark.parametrize("init_from_config", [True, False])
def test_textcat_init(
    noop_config,
    init_from_config: bool,
    n_prompt_examples: bool,
):
    config = Config().from_str(noop_config)
    if init_from_config:
        config["initialize"] = {"components": {"llm": {"labels": ["Test"]}}}
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp = assemble_from_config(config)

    examples = []

    for i, text in enumerate(INSULTS):
        predicted = nlp.make_doc(text)
        reference = predicted.copy()

        if i < (len(INSULTS) // 2):
            reference.cats = {"Insult": 1.0, "Compliment": 0.0}
        else:
            reference.cats = {"Insult": 0.0, "Compliment": 1.0}

        examples.append(Example(predicted, reference))

    _, llm = nlp.pipeline[0]
    task: TextCatTask = llm._task

    if init_from_config:
        target = {"Test"}
    else:
        target = set()
    assert set(task._label_dict.values()) == target
    assert not task._prompt_examples

    nlp.config["initialize"]["components"]["llm"] = {
        "n_prompt_examples": n_prompt_examples
    }

    nlp.initialize(lambda: examples)

    if init_from_config:
        target = {"Test"}
    else:
        target = {"Insult", "Compliment"}
    assert set(task._label_dict.values()) == target
    if n_prompt_examples >= 0:
        assert len(task._prompt_examples) == n_prompt_examples
    else:
        assert len(task._prompt_examples) == len(INSULTS)


def test_textcat_serde(noop_config, tmp_path: Path):
    config = Config().from_str(noop_config)
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp1 = assemble_from_config(config)
        nlp2 = assemble_from_config(config)
        nlp3 = assemble_from_config(config)

    labels = {"insult": "INSULT", "compliment": "COMPLIMENT"}

    task1: TextCatTask = nlp1.get_pipe("llm")._task
    task2: TextCatTask = nlp2.get_pipe("llm")._task
    task3: TextCatTask = nlp3.get_pipe("llm")._task

    # Artificially add labels to task1
    task1._label_dict = labels

    assert task1._label_dict == labels
    assert task2._label_dict == dict()
    assert task3._label_dict == dict()

    path = tmp_path / "model"

    nlp1.to_disk(path)

    cfgs = list(path.rglob("cfg"))
    assert len(cfgs) == 1

    cfg = json.loads(cfgs[0].read_text())
    assert cfg["_label_dict"] == labels

    nlp2.from_disk(path)
    nlp3.from_bytes(nlp1.to_bytes())

    assert task1._label_dict == task2._label_dict == task3._label_dict == labels


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
def test_add_label():
    nlp = spacy.blank("en")
    llm = nlp.add_pipe(
        "llm",
        config={
            "task": {
                "@llm_tasks": "spacy.TextCat.v3",
            },
            "model": {
                "@llm_models": "spacy.GPT-3-5.v1",
            },
        },
    )

    nlp.initialize()
    text = "I am feeling great."
    doc = nlp(text)
    assert len(doc.cats) == 0

    for label in ["HAPPY", "SAD"]:
        llm.add_label(label)
    doc = nlp(text)
    assert len(doc.cats) == 2
    assert set(doc.cats.keys()) == {"HAPPY", "SAD"}



================================================
FILE: spacy_llm/tests/tasks/test_translation.py
================================================
from pathlib import Path

import pytest
import spacy
from confection import Config
from spacy.util import make_tempdir

from spacy_llm.registry import fewshot_reader, file_reader

from ...tasks import make_sentiment_task, make_translation_task
from ..compat import has_openai_key

EXAMPLES_DIR = Path(__file__).parent / "examples"
TEMPLATES_DIR = Path(__file__).parent / "templates"


@pytest.fixture
def zeroshot_cfg_string():
    return """
    [nlp]
    lang = "en"
    pipeline = ["llm"]

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.Translation.v1"
    source_lang = "English"
    target_lang = "Spanish"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v3"
    """


@pytest.fixture
def fewshot_cfg_string():
    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.Translation.v1"
    source_lang = "English"
    target_lang = "Spanish"

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str((Path(__file__).parent / "examples" / "translation.yml"))}

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v3"
    """


@pytest.fixture
def ext_template_cfg_string():
    """Simple zero-shot config with an external template"""

    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]

    [components]
    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.Translation.v1"
    source_lang = "English"
    target_lang = "Spanish"

    [components.llm.task.template]
    @misc = "spacy.FileReader.v1"
    path = {str((Path(__file__).parent / "templates" / "translation.jinja2"))}

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v3"
    """


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "fewshot_cfg_string",
        "ext_template_cfg_string",
    ],
)
def test_translation_config(cfg_string, request):
    cfg_string = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg_string)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]

    # also test nlp config from a dict in add_pipe
    component_cfg = dict(orig_config["components"]["llm"])
    component_cfg.pop("factory")

    nlp2 = spacy.blank("en")
    nlp2.add_pipe("llm", config=component_cfg)
    assert nlp2.pipe_names == ["llm"]


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "fewshot_cfg_string",
        "ext_template_cfg_string",
    ],
)
def test_translate_predict(cfg_string, request):
    cfg = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    doc = nlp("This is the sun")
    if cfg_string != "ext_template_cfg_string":
        assert doc._.translation.strip(".") == "Este es el sol"


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "cfg_string_field",
    [
        ("zeroshot_cfg_string", None),
        ("fewshot_cfg_string", None),
        ("zeroshot_cfg_string", "sentiment_x"),
    ],
)
def test_translation_io(cfg_string_field, request):
    cfg_string, field = cfg_string_field
    cfg = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg)
    if field:
        orig_config["components"]["llm"]["task"]["field"] = field
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]
    # ensure you can save a pipeline to disk and run it after loading
    with make_tempdir() as tmpdir:
        nlp.to_disk(tmpdir)
        nlp2 = spacy.load(tmpdir)
    assert nlp2.pipe_names == ["llm"]
    translation = getattr(nlp2("This is perfect.")._, field if field else "translation")
    if cfg_string != "ext_template_cfg_string":
        assert translation == "Esto es perfecto."


@pytest.mark.parametrize("source_lang", [None, "English"])
def test_jinja_template_rendering_without_examples(source_lang: str):
    """Test if jinja template renders as we expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    nlp = spacy.blank("en")
    text = "They're indifferent."
    doc = nlp.make_doc(text)

    translation_task = (
        make_translation_task(
            examples=None, target_lang="Spanish", source_lang=source_lang
        )
        if source_lang
        else make_translation_task(examples=None, target_lang="Spanish")
    )
    prompt = list(translation_task.generate_prompts([doc]))[0][0][0]

    if source_lang:
        assert (
            prompt.strip()
            == f"""
Translate the text after "Text:" from English to Spanish.

Respond after "Translation:" with nothing but the translated text.

Text:
{text}
Translation:""".strip()
        )
    else:
        assert (
            prompt.strip()
            == f"""
Translate the text after "Text:" to Spanish.

Respond after "Translation:" with nothing but the translated text.

Text:
{text}
Translation:""".strip()
        )


@pytest.mark.parametrize("source_lang", [None, "English"])
@pytest.mark.parametrize(
    "examples_path",
    [
        str(EXAMPLES_DIR / "translation.json"),
        str(EXAMPLES_DIR / "translation.yml"),
        str(EXAMPLES_DIR / "translation.jsonl"),
    ],
)
def test_jinja_template_rendering_with_examples(examples_path, source_lang):
    """Test if jinja2 template renders as expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    nlp = spacy.blank("en")
    text = "It was the happiest day of her life."
    doc = nlp.make_doc(text)

    examples = fewshot_reader(examples_path)
    translation_task = (
        make_translation_task(
            examples=examples, target_lang="Spanish", source_lang=source_lang
        )
        if source_lang
        else make_translation_task(examples=examples, target_lang="Spanish")
    )
    prompt = list(translation_task.generate_prompts([doc]))[0][0][0]

    if source_lang:
        assert (
            prompt.strip()
            == """
Translate the text after "Text:" from English to Spanish.

Respond after "Translation:" with nothing but the translated text.
Below are some examples (only use these as a guide):

Text:
Top of the morning to you!
Translation:
Â¡Muy buenos dÃ­as!

Text:
The weather is great today.
Translation:
El clima estÃ¡ fantÃ¡stico hoy.

Text:
Do you know what will happen tomorrow?
Translation:
Â¿Sabes quÃ© pasarÃ¡ maÃ±ana?

Text:
It was the happiest day of her life.
Translation:""".strip()
        )

    else:
        assert (
            prompt.strip()
            == """
Translate the text after "Text:" to Spanish.

Respond after "Translation:" with nothing but the translated text.
Below are some examples (only use these as a guide):

Text:
Top of the morning to you!
Translation:
Â¡Muy buenos dÃ­as!

Text:
The weather is great today.
Translation:
El clima estÃ¡ fantÃ¡stico hoy.

Text:
Do you know what will happen tomorrow?
Translation:
Â¿Sabes quÃ© pasarÃ¡ maÃ±ana?

Text:
It was the happiest day of her life.
Translation:""".strip()
        )


def test_external_template_actually_loads():
    template_path = str(TEMPLATES_DIR / "translation.jinja2")
    template = file_reader(template_path)
    text = "There is a silver lining."
    nlp = spacy.blank("en")
    doc = nlp.make_doc(text)

    sentiment_task = make_sentiment_task(template=template)
    prompt = list(sentiment_task.generate_prompts([doc]))[0][0][0]
    assert (
        prompt.strip()
        == f"""
Text: {text}
Translation:""".strip()
    )



================================================
FILE: spacy_llm/tests/tasks/examples/entity_linker.json
================================================
[
  {
    "text": "Alice goes to *New York* to see the *New York Knicks* game.",
    "mentions_str": "*New York*, *New York Knicks*",
    "mentions": [
      "New York", "New York Knicks"
    ],
    "entity_descriptions": [
      ["most populous city in the United States", "U.S. state"],
      ["most populous city in the United States", "National Basketball Association team in New York City"]
    ],
    "entity_ids": [
      ["Q60", "Q1384"],
      ["Q60", "Q131364"]
    ],
    "solutions": ["Q60", "Q131364"],
    "reasons": []
  },
  {
    "text": "*New York* is called the *Big Apple*. It also has *Apple* stores.",
    "mentions_str": "*New York*, *Big Apple*, *Apple*",
    "mentions": [
      "New York", "Big Apple", "Apple"
    ],
    "entity_descriptions": [
      ["most populous city in the United States", "U.S. state"],
      ["nickname for New York City", "fruit of the apple tree"],
      ["fruit of the apple tree", "American multinational technology company"]
    ],
    "entity_ids": [
      ["Q60", "Q1384"],
      ["Q14435", "Q89"],
      ["Q89", "Q312"]
    ],
    "solutions": ["Q60", "Q14435", "Q312"],
    "reasons": [
      "The mention of \"Big Apple\" in the same context clarifies that this is about the city New York.",
      "Big Apple is a well-known nickname of New York.",
      "The context of \"stores\" indicates that this is about the technology company Apple, which operates \"Apple stores\"."
    ]
  }
]



================================================
FILE: spacy_llm/tests/tasks/examples/entity_linker.jsonl
================================================
{"text": "Alice goes to *New York* to see the *New York Knicks* game.", "mentions_str": "*New York*, *New York Knicks*", "mentions": ["New York", "New York Knicks"], "entity_descriptions": [["most populous city in the United States", "U.S. state"], ["most populous city in the United States", "National Basketball Association team in New York City"]], "entity_ids": [["Q60", "Q1384"], ["Q60", "Q131364"]], "solutions": ["Q60", "Q131364"], "reasons": []}
{"text": "*New York* is called the *Big Apple*. It also has *Apple* stores.", "mentions_str": "*New York*, *Big Apple*, *Apple*", "mentions": ["New York", "Big Apple", "Apple"], "entity_descriptions": [["most populous city in the United States", "U.S. state"], ["nickname for New York City", "fruit of the apple tree"], ["fruit of the apple tree", "American multinational technology company"]], "entity_ids": [["Q60", "Q1384"], ["Q14435", "Q89"], ["Q89", "Q312"]], "solutions": ["Q60", "Q14435", "Q312"], "reasons": ["The mention of \"Big Apple\" in the same context clarifies that this is about the city New York.", "Big Apple is a well-known nickname of New York.", "The context of \"stores\" indicates that this is about the technology company Apple, which operates \"Apple stores\"."]}



================================================
FILE: spacy_llm/tests/tasks/examples/entity_linker.yml
================================================
- text: "Alice goes to *New York* to see the *New York Knicks* game."
  mentions_str: "*New York*, *New York Knicks*"
  mentions:
    - New York
    - New York Knicks
  entity_descriptions:
    -
      - most populous city in the United States
      - U.S. state
    -
      - most populous city in the United States
      - National Basketball Association team in New York City
  entity_ids:
    -
      - Q60
      - Q1384
    -
      - Q60
      - Q131364
  solutions:
    - Q60
    - Q131364
  reasons: []

- text: "*New York* is called the *Big Apple*. It also has *Apple* stores."
  mentions_str: "*New York*, *Big Apple*, *Apple*"
  mentions:
    - New York
    - Big Apple
    - Apple
  entity_descriptions:
    -
      - most populous city in the United States
      - U.S. state
    -
      - nickname for New York City
      - fruit of the apple tree
    -
      - fruit of the apple tree
      - American multinational technology company
  entity_ids:
    -
      - Q60
      - Q1384
    -
      - Q14435
      - Q89
    -
      - Q89
      - Q312
  solutions:
    - Q60
    - Q14435
    - Q312
  reasons:
    - The mention of "Big Apple" in the same context clarifies that this is about the city New York.
    - Big Apple is a well-known nickname of New York.
    - The context of "stores" indicates that this is about the technology company Apple, which operates "Apple stores".


================================================
FILE: spacy_llm/tests/tasks/examples/lemma.json
================================================
[
  {
    "text": "The arc of the moral universe is long, but it bends toward justice.",
    "lemmas": [
      {"The": "The"},
      {"arc": "arc"},
      {"of": "of"},
      {"the": "the"},
      {"moral": "moral"},
      {"universe": "universe"},
      {"is": "be"},
      {"long": "long"},
      {",": ","},
      {"but": "but"},
      {"it": "it"},
      {"bends": "bend"},
      {"toward": "toward"},
      {"justice": "justice"},
      {".": "."}
    ]
  },
  {
    "text": "Life can only be understood backwards; but it must be lived forwards.",
    "lemmas": [
      {"Life": "Life"},
      {"can": "can"},
      {"only": "only"},
      {"be": "be"},
      {"understood": "understand"},
      {"backwards": "backwards"},
      {";": ";"},
      {"but": "but"},
      {"it": "it"},
      {"must": "must"},
      {"be": "be"},
      {"lived": "lived"},
      {"forwards": "forwards"},
      {".": "."}
    ]
  },
  {
    "text": "I'm buying ice cream.",
    "lemmas": [
      {"I": "I"},
      {"'m": "be"},
      {"buying": "buy"},
      {"ice": "ice"},
      {"cream": "cream"},
      {".": "."}
    ]
  }
]


================================================
FILE: spacy_llm/tests/tasks/examples/lemma.jsonl
================================================
{"text": "The arc of the moral universe is long, but it bends toward justice.", "lemmas": [{"The": "The"}, {"arc": "arc"}, {"of": "of"}, {"the": "the"}, {"moral": "moral"}, {"universe": "universe"}, {"is": "be"}, {"long": "long"}, {",": ","}, {"but": "but"}, {"it": "it"}, {"bends": "bend"}, {"toward": "toward"}, {"justice": "justice"}, {".": "."}]}
{"text": "Life can only be understood backwards; but it must be lived forwards.", "lemmas": [ {"Life": "Life"}, {"can": "can"}, {"only": "only"}, {"be": "be"}, {"understood": "understand"}, {"backwards": "backwards"}, {";": ";"}, {"but": "but"}, {"it": "it"}, {"must": "must"}, {"be": "be"}, {"lived": "lived"}, {"forwards": "forwards"}, {".": "."}]}
{"text": "I'm buying ice cream.", "lemmas": [ {"I": "I"}, {"'m": "be"}, {"buying": "buy"}, {"ice": "ice"}, {"cream": "cream"}, {".": "."}]}


================================================
FILE: spacy_llm/tests/tasks/examples/lemma.yml
================================================
- text: The arc of the moral universe is long, but it bends toward justice.
  lemmas:
    - "The": "The"
    - "arc": "arc"
    - "of": "of"
    - "the": "the"
    - "moral": "moral"
    - "universe": "universe"
    - "is": "be"
    - "long": "long"
    - ",": ","
    - "but": "but"
    - "it": "it"
    - "bends": "bend"
    - "toward": "toward"
    - "justice": "justice"
    - ".": "."

- text: Life can only be understood backwards; but it must be lived forwards.
  lemmas:
    - "Life": "Life"
    - "can": "can"
    - "only": "only"
    - "be": "be"
    - "understood": "understand"
    - "backwards": "backwards"
    - ";": ";"
    - "but": "but"
    - "it": "it"
    - "must": "must"
    - "be": "be"
    - "lived": "lived"
    - "forwards": "forwards"
    - "." : "."

- text: I'm buying ice cream.
  lemmas:
    - "I": "I"
    - "'m": "be"
    - "buying": "buy"
    - "ice": "ice"
    - "cream": "cream"
    - ".": "."



================================================
FILE: spacy_llm/tests/tasks/examples/ner.json
================================================
[
  {
    "text": "Jack and Jill went up the hill.",
    "spans": [
      {
        "text": "Jack",
        "is_entity": true,
        "label": "PER",
        "reason": "is the name of a person"
      },
      {
        "text": "Jill",
        "is_entity": true,
        "label": "PER",
        "reason": "is the name of a person"
      },
      {
        "text": "went up",
        "is_entity": false,
        "label": "==NONE==",
        "reason": "is a verb"
      },
      {
        "text": "hill",
        "is_entity": true,
        "label": "LOC",
        "reason": "is a location"
      }
    ]
  }
]



================================================
FILE: spacy_llm/tests/tasks/examples/ner.jsonl
================================================
{"text":"Jack and Jill went up the hill.","spans":[{"text":"Jack","is_entity":true,"label":"PER","reason":"is the name of a person"},{"text":"Jill","is_entity":true,"label":"PER","reason":"is the name of a person"},{"text":"went up","is_entity":false,"label":"==NONE==","reason":"is a verb"},{"text":"hill","is_entity":true,"label":"LOC","reason":"is a location"}]}



================================================
FILE: spacy_llm/tests/tasks/examples/ner.yml
================================================
- text: Jack and Jill went up the hill.
  spans:
    - text: Jack
      is_entity: true
      label: PER
      reason: is the name of a person
    - text: Jill
      is_entity: true
      label: PER
      reason: is the name of a person
    - text: went up
      is_entity: false
      label: ==NONE==
      reason: is a verb
    - text: hill
      is_entity: true
      label: LOC
      reason: is a location



================================================
FILE: spacy_llm/tests/tasks/examples/ner_inconsistent.yml
================================================
- text: Jack and Jill went up the hill.
  spans:
    - text: Jack
      is_entity: true
      label: PERSON
      reason: is the name of a person
    - text: Jill
      is_entity: true
      label: PERSON
      reason: is the name of a person
    - text: went up
      is_entity: false
      label: ==NONE==
      reason: is a verb
    - text: hill
      is_entity: true
      label: LOCATION
      reason: is a location

- text: spaCy is a great tool
  spans:
    - text: spaCy
      is_entity: true
      label: TECH
      reason: is a software product

- text: Jack and Jill went up the hill and spaCy is a great tool.
  spans:
    - text: Jack
      is_entity: true
      label: PERSON
      reason: is the name of a person
    - text: Jill
      is_entity: true
      label: PERSON
      reason: is the name of a person
    - text: went up
      is_entity: false
      label: ==NONE==
      reason: is a verb
    - text: hill
      is_entity: true
      label: LOCATION
      reason: is a location
    - text: spaCy
      is_entity: true
      label: TECH
      reason: is a software product



================================================
FILE: spacy_llm/tests/tasks/examples/raw.json
================================================
[
  {"text": "3 + 5 = x. What's x?", "reply": "8"},
  {"text": "Write me a limerick.", "reply": "There was an Old Man with a beard, Who said, 'It is just as I feared! Two Owls and a Hen, Four Larks and a Wren, Have all built their nests in my beard!"},
  {"text": "Analyse the sentiment of the text 'This is great'.", "reply": "'This is great' expresses a very positive sentiment."}
]


================================================
FILE: spacy_llm/tests/tasks/examples/raw.jsonl
================================================
{"text": "3 + 5 = x. What's x?", "reply": "8"}
{"text": "Write me a limerick.", "reply": "There was an Old Man with a beard, Who said, 'It is just as I feared! Two Owls and a Hen, Four Larks and a Wren, Have all built their nests in my beard!"}
{"text": "Analyse the sentiment of the text 'This is great'.", "reply": "'This is great' expresses a very positive sentiment."}



================================================
FILE: spacy_llm/tests/tasks/examples/raw.yml
================================================
- text: "3 + 5 = x. What's x?"
  reply: "8"

- text: "Write me a limerick."
  reply: "There was an Old Man with a beard, Who said, 'It is just as I feared! Two Owls and a Hen, Four Larks and a Wren, Have all built their nests in my beard!"

- text: "Analyse the sentiment of the text 'This is great'."
  reply: "'This is great' expresses a very positive sentiment."



================================================
FILE: spacy_llm/tests/tasks/examples/rel.jsonl
================================================
{"text": "Laura bought a house in Boston with her husband Mark.", "ents": [{"start_char": 0, "end_char": 5, "label": "PERSON"}, {"start_char": 24, "end_char": 30, "label": "GPE"}, {"start_char": 48, "end_char": 52, "label": "PERSON"}], "relations": [{"dep": 0, "dest": 1, "relation": "LivesIn"}, {"dep": 2, "dest": 1, "relation": "LivesIn"}]}
{"text": "Michael travelled through South America by bike.", "ents": [{"start_char": 0, "end_char": 7, "label": "PERSON"}, {"start_char": 26, "end_char": 39, "label": "LOC"}], "relations": [{"dep": 0, "dest": 1, "relation": "Visits"}]}



================================================
FILE: spacy_llm/tests/tasks/examples/sentiment.json
================================================
[
  {
    "text": "This is horrifying.",
    "score": 0
  },
  {
    "text": "This is underwhelming.",
    "score": 0.25
  },
  {
    "text": "This is ok.",
    "score": 0.5
  },
  {
    "text": "I'm looking forward to this!",
    "score": 1.0
  }
]


================================================
FILE: spacy_llm/tests/tasks/examples/sentiment.jsonl
================================================
{"text": "This is horrifying.", "score": 0}
{"text": "This is underwhelming.", "score": 0.25}
{"text": "This is ok.", "score": 0.5}
{"text": "I'm looking forward to this!", "score": 1.0}


================================================
FILE: spacy_llm/tests/tasks/examples/sentiment.yml
================================================
- text: "This is horrifying."
  score: 0
- text: "This is underwhelming."
  score: 0.25
- text: "This is ok."
  score: 0.5
- text: "I'm looking forward to this!"
  score: 1.0


================================================
FILE: spacy_llm/tests/tasks/examples/spancat.json
================================================
[
  {
    "text": "Jack and Jill went up the hill.",
    "spans": [
      {
        "text": "Jack",
        "is_entity": true,
        "label": "PER",
        "reason": "is the name of a person"
      },
      {
        "text": "Jill",
        "is_entity": true,
        "label": "PER",
        "reason": "is the name of a person"
      },
      {
        "text": "went up",
        "is_entity": false,
        "label": "==NONE==",
        "reason": "is a verb"
      },
      {
        "text": "hill",
        "is_entity": true,
        "label": "LOC",
        "reason": "is a location"
      },
      {
        "text": "hill",
        "is_entity": true,
        "label": "DESTINATION",
        "reason": "is a destination"
      }
    ]
  }
]



================================================
FILE: spacy_llm/tests/tasks/examples/spancat.jsonl
================================================
{"text":"Jack and Jill went up the hill.","spans":[{"text":"Jack","is_entity":true,"label":"PER","reason":"is the name of a person"},{"text":"Jill","is_entity":true,"label":"PER","reason":"is the name of a person"},{"text":"went up","is_entity":false,"label":"==NONE==","reason":"is a verb"},{"text":"hill","is_entity":true,"label":"LOC","reason":"is a location"},{"text":"hill","is_entity":true,"label":"DESTINATION","reason":"is a destination"}]}



================================================
FILE: spacy_llm/tests/tasks/examples/spancat.yml
================================================
  - text: Jack and Jill went up the hill.
    spans:
      - text: Jack
        is_entity: true
        label: PER
        reason: is the name of a person
      - text: Jill
        is_entity: true
        label: PER
        reason: is the name of a person
      - text: went up
        is_entity: false
        label: ==NONE==
        reason: is a verb
      - text: hill
        is_entity: true
        label: LOC
        reason: is a location
      - text: hill
        is_entity: true
        label: DESTINATION
        reason: is a destination



================================================
FILE: spacy_llm/tests/tasks/examples/summarization.json
================================================
[
  {
    "text": "The United Nations, referred to informally as the UN, is an intergovernmental organization whose stated purposes are to maintain international peace and security, develop friendly relations among nations, achieve international cooperation, and serve as a centre for harmonizing the actions of nations. It is the world's largest international organization. The UN is headquartered on international territory in New York City, and the organization has other offices in Geneva, Nairobi, Vienna, and The Hague, where the International Court of Justice is headquartered.\n\nThe UN was established after World War II with the aim of preventing future world wars, and succeeded the League of Nations, which was characterized as ineffective. On 25 April 1945, 50 nations met in San Francisco, California for a conference and started drafting the UN Charter, which was adopted on 25 June 1945. The charter took effect on 24 October 1945, when the UN began operations. The organization's objectives, as defined by its charter, include maintaining international peace and security, protecting human rights, delivering humanitarian aid, promoting sustainable development, and upholding international law. At its founding, the UN had 51 member states; as of 2023, it has 193 â€“ almost all of the world's sovereign states.",
    "summary": "UN is an intergovernmental organization to foster international peace, security, and cooperation. Established after WW2 with 51 members, now 193."
  },
  {
    "text": "Life is a quality that distinguishes matter that has biological processes, such as signaling and self-sustaining processes, from matter that does not, and is defined by the capacity for growth, reaction to stimuli, metabolism, energy transformation, and reproduction. Various forms of life exist, such as plants, animals, fungi, protists, archaea, and bacteria. Biology is the science that studies life.\n\nThe gene is the unit of heredity, whereas the cell is the structural and functional unit of life. There are two kinds of cells, prokaryotic and eukaryotic, both of which consist of cytoplasm enclosed within a membrane and contain many biomolecules such as proteins and nucleic acids. Cells reproduce through a process of cell division, in which the parent cell divides into two or more daughter cells and passes its genes onto a new generation, sometimes producing genetic variation.\n\nOrganisms, or the individual entities of life, are generally thought to be open systems that maintain homeostasis, are composed of cells, have a life cycle, undergo metabolism, can grow, adapt to their environment, respond to stimuli, reproduce and evolve over multiple generations. Other definitions sometimes include non-cellular life forms such as viruses and viroids, but they are usually excluded because they do not function on their own; rather, they exploit the biological processes of hosts.",
    "summary": "Life is a quality defined by biological processes, including reproduction, genetics, and metabolism. There are two types of cells and organisms that can grow, respond, reproduce, and evolve."
  }
]


================================================
FILE: spacy_llm/tests/tasks/examples/summarization.jsonl
================================================
{"text": "The United Nations, referred to informally as the UN, is an intergovernmental organization whose stated purposes are to maintain international peace and security, develop friendly relations among nations, achieve international cooperation, and serve as a centre for harmonizing the actions of nations. It is the world's largest international organization. The UN is headquartered on international territory in New York City, and the organization has other offices in Geneva, Nairobi, Vienna, and The Hague, where the International Court of Justice is headquartered.\n\nThe UN was established after World War II with the aim of preventing future world wars, and succeeded the League of Nations, which was characterized as ineffective. On 25 April 1945, 50 nations met in San Francisco, California for a conference and started drafting the UN Charter, which was adopted on 25 June 1945. The charter took effect on 24 October 1945, when the UN began operations. The organization's objectives, as defined by its charter, include maintaining international peace and security, protecting human rights, delivering humanitarian aid, promoting sustainable development, and upholding international law. At its founding, the UN had 51 member states; as of 2023, it has 193 â€“ almost all of the world's sovereign states.", "summary": "UN is an intergovernmental organization to foster international peace, security, and cooperation. Established after WW2 with 51 members, now 193."}
{"text": "Life is a quality that distinguishes matter that has biological processes, such as signaling and self-sustaining processes, from matter that does not, and is defined by the capacity for growth, reaction to stimuli, metabolism, energy transformation, and reproduction. Various forms of life exist, such as plants, animals, fungi, protists, archaea, and bacteria. Biology is the science that studies life.\n\nThe gene is the unit of heredity, whereas the cell is the structural and functional unit of life. There are two kinds of cells, prokaryotic and eukaryotic, both of which consist of cytoplasm enclosed within a membrane and contain many biomolecules such as proteins and nucleic acids. Cells reproduce through a process of cell division, in which the parent cell divides into two or more daughter cells and passes its genes onto a new generation, sometimes producing genetic variation.\n\nOrganisms, or the individual entities of life, are generally thought to be open systems that maintain homeostasis, are composed of cells, have a life cycle, undergo metabolism, can grow, adapt to their environment, respond to stimuli, reproduce and evolve over multiple generations. Other definitions sometimes include non-cellular life forms such as viruses and viroids, but they are usually excluded because they do not function on their own; rather, they exploit the biological processes of hosts.", "summary": "Life is a quality defined by biological processes, including reproduction, genetics, and metabolism. There are two types of cells and organisms that can grow, respond, reproduce, and evolve."}


================================================
FILE: spacy_llm/tests/tasks/examples/summarization.yml
================================================
- text: "The United Nations, referred to informally as the UN, is an intergovernmental organization whose stated purposes are to maintain international peace and security, develop friendly relations among nations, achieve international cooperation, and serve as a centre for harmonizing the actions of nations. It is the world's largest international organization. The UN is headquartered on international territory in New York City, and the organization has other offices in Geneva, Nairobi, Vienna, and The Hague, where the International Court of Justice is headquartered.\n\nThe UN was established after World War II with the aim of preventing future world wars, and succeeded the League of Nations, which was characterized as ineffective. On 25 April 1945, 50 nations met in San Francisco, California for a conference and started drafting the UN Charter, which was adopted on 25 June 1945. The charter took effect on 24 October 1945, when the UN began operations. The organization's objectives, as defined by its charter, include maintaining international peace and security, protecting human rights, delivering humanitarian aid, promoting sustainable development, and upholding international law. At its founding, the UN had 51 member states; as of 2023, it has 193 â€“ almost all of the world's sovereign states."
  summary: "UN is an intergovernmental organization to foster international peace, security, and cooperation. Established after WW2 with 51 members, now 193."

- text: "Life is a quality that distinguishes matter that has biological processes, such as signaling and self-sustaining processes, from matter that does not, and is defined by the capacity for growth, reaction to stimuli, metabolism, energy transformation, and reproduction. Various forms of life exist, such as plants, animals, fungi, protists, archaea, and bacteria. Biology is the science that studies life.\n\nThe gene is the unit of heredity, whereas the cell is the structural and functional unit of life. There are two kinds of cells, prokaryotic and eukaryotic, both of which consist of cytoplasm enclosed within a membrane and contain many biomolecules such as proteins and nucleic acids. Cells reproduce through a process of cell division, in which the parent cell divides into two or more daughter cells and passes its genes onto a new generation, sometimes producing genetic variation.\n\nOrganisms, or the individual entities of life, are generally thought to be open systems that maintain homeostasis, are composed of cells, have a life cycle, undergo metabolism, can grow, adapt to their environment, respond to stimuli, reproduce and evolve over multiple generations. Other definitions sometimes include non-cellular life forms such as viruses and viroids, but they are usually excluded because they do not function on their own; rather, they exploit the biological processes of hosts."
  summary: "Life is a quality defined by biological processes, including reproduction, genetics, and metabolism. There are two types of cells and organisms that can grow, respond, reproduce, and evolve."



================================================
FILE: spacy_llm/tests/tasks/examples/textcat_binary.json
================================================
[
  {
    "text": "Macaroni and cheese is the best budget meal for students, unhealthy tho",
    "answer": "NEG"
  },
  {
    "text": "2 cups soy sauce, 1/2 lb. of chicken, 1/2 cup vinegar, then salt and paper, mix then well and you get an adobo",
    "answer": "POS"
  },
  {
    "text": "You can still add more layers to that croissant, get extra butter and add a few cups of flour",
    "answer": "POS"
  }
]


================================================
FILE: spacy_llm/tests/tasks/examples/textcat_binary.jsonl
================================================
{"text":"Macaroni and cheese is the best budget meal for students, unhealthy tho","answer":"NEG"}
{"text":"2 cups soy sauce, 1/2 lb. of chicken, 1/2 cup vinegar, then salt and paper, mix then well and you get an adobo","answer":"POS"}
{"text":"You can still add more layers to that croissant, get extra butter and add a few cups of flour","answer":"POS"}



================================================
FILE: spacy_llm/tests/tasks/examples/textcat_binary.yml
================================================
- text: Macaroni and cheese is the best budget meal for students, unhealthy tho
  answer: NEG
- text:
    2 cups soy sauce, 1/2 lb. of chicken, 1/2 cup vinegar, then salt and paper,
    mix then well and you get an adobo
  answer: POS
- text:
    You can still add more layers to that croissant, get extra butter and add
    a few cups of flour
  answer: POS



================================================
FILE: spacy_llm/tests/tasks/examples/textcat_multi_excl.json
================================================
[
  {
    "text":"Macaroni and cheese is the best budget meal for students, unhealthy tho",
    "answer":"Comment"
  },
  {
    "text":"2 cups soy sauce, 1/2 lb. of chicken, 1/2 cup vinegar, then salt and paper, mix then well and you get an adobo",
    "answer":"Recipe"
  },
  {
    "text":"You can still add more layers to that croissant, get extra butter and add a few cups of flour",
    "answer":"Feedback"
  }
]


================================================
FILE: spacy_llm/tests/tasks/examples/textcat_multi_excl.jsonl
================================================
{"text":"Macaroni and cheese is the best budget meal for students, unhealthy tho","answer":"Comment"}
{"text":"2 cups soy sauce, 1/2 lb. of chicken, 1/2 cup vinegar, then salt and paper, mix then well and you get an adobo","answer":"Recipe"}
{"text":"You can still add more layers to that croissant, get extra butter and add a few cups of flour","answer":"Feedback"}



================================================
FILE: spacy_llm/tests/tasks/examples/textcat_multi_excl.yml
================================================
  - text: Macaroni and cheese is the best budget meal for students, unhealthy tho
    answer: Comment
  - text: 2 cups soy sauce, 1/2 lb. of chicken, 1/2 cup vinegar, then salt and paper,
      mix then well and you get an adobo
    answer: Recipe
  - text: You can still add more layers to that croissant, get extra butter and add
      a few cups of flour
    answer: Feedback



================================================
FILE: spacy_llm/tests/tasks/examples/textcat_multi_nonexcl.json
================================================
[
  {
    "text":"Macaroni and cheese is the best budget meal for students, unhealthy tho",
    "answer":"Comment,Feedback"
  },
  {
    "text":"2 cups soy sauce, 1/2 lb. of chicken, 1/2 cup vinegar, then salt and paper, mix then well and you get an adobo",
    "answer":"Recipe"
  },
  {
    "text":"You can still add more layers to that croissant, get extra butter and add a few cups of flour",
    "answer":"Feedback,Recipe"
  }
]


================================================
FILE: spacy_llm/tests/tasks/examples/textcat_multi_nonexcl.jsonl
================================================
{"text":"Macaroni and cheese is the best budget meal for students, unhealthy tho","answer":"Comment,Feedback"}
{"text":"2 cups soy sauce, 1/2 lb. of chicken, 1/2 cup vinegar, then salt and paper, mix then well and you get an adobo","answer":"Recipe"}
{"text":"You can still add more layers to that croissant, get extra butter and add a few cups of flour","answer":"Feedback,Recipe"}



================================================
FILE: spacy_llm/tests/tasks/examples/textcat_multi_nonexcl.yml
================================================
  - text: Macaroni and cheese is the best budget meal for students, unhealthy tho
    answer: Comment,Feedback
  - text: 2 cups soy sauce, 1/2 lb. of chicken, 1/2 cup vinegar, then salt and paper,
      mix then well and you get an adobo
    answer: Recipe
  - text: You can still add more layers to that croissant, get extra butter and add
      a few cups of flour
    answer: Feedback,Recipe



================================================
FILE: spacy_llm/tests/tasks/examples/translation.json
================================================
[
  {"text": "Top of the morning to you!", "translation":  "Â¡Muy buenos dÃ­as!"},
  {"text": "The weather is great today.", "translation":  "El clima estÃ¡ fantÃ¡stico hoy."},
  {"text": "Do you know what will happen tomorrow?", "translation":  "Â¿Sabes quÃ© pasarÃ¡ maÃ±ana?"}
]


================================================
FILE: spacy_llm/tests/tasks/examples/translation.jsonl
================================================
{"text": "Top of the morning to you!", "translation":  "Â¡Muy buenos dÃ­as!"}
{"text": "The weather is great today.", "translation":  "El clima estÃ¡ fantÃ¡stico hoy."}
{"text": "Do you know what will happen tomorrow?", "translation":  "Â¿Sabes quÃ© pasarÃ¡ maÃ±ana?"}


================================================
FILE: spacy_llm/tests/tasks/examples/translation.yml
================================================
- text: "Top of the morning to you!"
  translation: "Â¡Muy buenos dÃ­as!"
- text: "The weather is great today."
  translation: "El clima estÃ¡ fantÃ¡stico hoy."
- text: "Do you know what will happen tomorrow?"
  translation: "Â¿Sabes quÃ© pasarÃ¡ maÃ±ana?"


================================================
FILE: spacy_llm/tests/tasks/legacy/__init__.py
================================================
[Empty file]


================================================
FILE: spacy_llm/tests/tasks/legacy/test_ner.py
================================================
import json
import re
from pathlib import Path

import pytest
import spacy
import srsly
from confection import Config
from spacy.tokens import Span
from spacy.training import Example
from spacy.util import make_tempdir

from spacy_llm.pipeline import LLMWrapper
from spacy_llm.registry import fewshot_reader, file_reader, lowercase_normalizer
from spacy_llm.registry import strip_normalizer
from spacy_llm.tasks.ner import NERTask, make_ner_task_v2
from spacy_llm.tasks.util import find_substrings
from spacy_llm.ty import LabeledTask, ShardingLLMTask
from spacy_llm.util import assemble_from_config, split_labels

from ...compat import has_openai_key

EXAMPLES_DIR = Path(__file__).parent / "examples"
TEMPLATES_DIR = Path(__file__).parent / "templates"


@pytest.fixture
def zeroshot_cfg_string():
    return """
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.NER.v1"
    labels = PER,ORG,LOC

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v1"
    """


@pytest.fixture
def zeroshot_cfg_string_v2_lds():
    return """
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.NER.v2"
    labels = PER,ORG,LOC

    [components.llm.task.label_definitions]
    PER = "Any named individual in the text"
    ORG = "Any named organization in the text"
    LOC = "The name of any politically or geographically defined location"

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v1"
    """


@pytest.fixture
def fewshot_cfg_string():
    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.NER.v1"
    labels = PER,ORG,LOC

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str((Path(__file__).parent / "examples" / "ner.yml"))}

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v1"
    """


@pytest.fixture
def fewshot_cfg_string_v2():
    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.NER.v2"
    labels = ["PER", "ORG", "LOC"]

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str((Path(__file__).parent / "examples" / "ner.yml"))}

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v1"
    """


@pytest.fixture
def ext_template_cfg_string():
    """Simple zero-shot config with an external template"""

    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]
    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.NER.v2"
    labels = PER,ORG,LOC

    [components.llm.task.template]
    @misc = "spacy.FileReader.v1"
    path = {str((Path(__file__).parent / "templates" / "ner.jinja2"))}

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v1"
    """


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "zeroshot_cfg_string_v2_lds",
        "fewshot_cfg_string",
        "fewshot_cfg_string_v2",
        "ext_template_cfg_string",
    ],
)
def test_ner_config(cfg_string, request):
    cfg_string = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg_string)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]

    # also test nlp config from a dict in add_pipe
    component_cfg = dict(orig_config["components"]["llm"])
    component_cfg.pop("factory")

    nlp2 = spacy.blank("en")
    nlp2.add_pipe("llm", config=component_cfg)
    assert nlp2.pipe_names == ["llm"]

    pipe = nlp.get_pipe("llm")
    assert isinstance(pipe, LLMWrapper)
    assert isinstance(pipe.task, ShardingLLMTask)

    labels = orig_config["components"]["llm"]["task"]["labels"]
    labels = split_labels(labels)
    task = pipe.task
    assert isinstance(task, LabeledTask)
    assert sorted(task.labels) == sorted(tuple(labels))
    assert pipe.labels == task.labels
    assert nlp.pipe_labels["llm"] == list(task.labels)


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "zeroshot_cfg_string_v2_lds",
        "fewshot_cfg_string",
        "fewshot_cfg_string_v2",
        "ext_template_cfg_string",
    ],
)
def test_ner_predict(cfg_string, request):
    """Use OpenAI to get zero-shot NER results.
    Note that this test may fail randomly, as the LLM's output is unguaranteed to be consistent/predictable
    """
    orig_cfg_string = cfg_string
    cfg_string = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg_string)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    text = "Marc and Bob both live in Ireland."
    doc = nlp(text)

    if orig_cfg_string != "ext_template_cfg_string":
        assert len(doc.ents) > 0
        for ent in doc.ents:
            assert ent.label_ in ["PER", "ORG", "LOC"]


@pytest.mark.external
@pytest.mark.parametrize(
    "cfg_string",
    [
        "zeroshot_cfg_string",
        "zeroshot_cfg_string_v2_lds",
        "fewshot_cfg_string",
        "fewshot_cfg_string_v2",
        "ext_template_cfg_string",
    ],
)
def test_ner_io(cfg_string, request):
    cfg_string = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg_string)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]
    # ensure you can save a pipeline to disk and run it after loading
    with make_tempdir() as tmpdir:
        nlp.to_disk(tmpdir)
        nlp2 = spacy.load(tmpdir)
    assert nlp2.pipe_names == ["llm"]
    text = "Marc and Bob both live in Ireland."
    doc = nlp2(text)
    assert len(doc.ents) >= 0  # can be zero if template is too simple / test-like
    for ent in doc.ents:
        assert ent.label_ in ["PER", "ORG", "LOC"]


@pytest.mark.parametrize(
    "text,input_strings,result_strings,result_offsets",
    [
        (
            "Felipe and Jaime went to the library.",
            ["Felipe", "Jaime", "library"],
            ["Felipe", "Jaime", "library"],
            [(0, 6), (11, 16), (29, 36)],
        ),  # simple
        (
            "The Manila Observatory was founded in 1865 in Manila.",
            ["Manila", "The Manila Observatory"],
            ["Manila", "Manila", "The Manila Observatory"],
            [(4, 10), (46, 52), (0, 22)],
        ),  # overlapping and duplicated
        (
            "Take the road from downtown and turn left at the public market.",
            ["public market", "downtown"],
            ["public market", "downtown"],
            [(49, 62), (19, 27)]
            # flipped
        ),
    ],
)
def test_ensure_offsets_correspond_to_substrings(
    text, input_strings, result_strings, result_offsets
):
    offsets = find_substrings(text, input_strings)
    # Compare strings instead of offsets, but we need to get
    # those strings first from the text
    assert result_offsets == offsets
    found_substrings = [text[start:end] for start, end in offsets]
    assert result_strings == found_substrings


@pytest.mark.parametrize(
    "text,response,gold_ents",
    [
        # simple
        (
            "Jean Jacques and Jaime went to the library.",
            "PER: Jean Jacques, Jaime\nLOC: library",
            [("Jean Jacques", "PER"), ("Jaime", "PER"), ("library", "LOC")],
        ),
        # overlapping: should only return the longest span
        (
            "The Manila Observatory was founded in 1865.",
            "LOC: The Manila Observatory, Manila, Manila Observatory",
            [("The Manila Observatory", "LOC")],
        ),
        # flipped: order shouldn't matter
        (
            "Take the road from Downtown and turn left at the public market.",
            "LOC: public market, Downtown",
            [("Downtown", "LOC"), ("public market", "LOC")],
        ),
    ],
)
def test_ner_zero_shot_task(text, response, gold_ents):
    labels = "PER,ORG,LOC"
    llm_ner = make_ner_task_v2(labels=labels)
    # Prepare doc
    nlp = spacy.blank("en")
    doc_in = nlp.make_doc(text)
    # Pass to the parser
    # Note: parser() returns a list so we get what's inside
    doc_out = list(llm_ner.parse_responses([[doc_in]], [[response]]))[0]
    pred_ents = [(ent.text, ent.label_) for ent in doc_out.ents]
    assert pred_ents == gold_ents


@pytest.mark.parametrize(
    "response,normalizer,gold_ents",
    [
        (
            "PER: Jean Jacques, Jaime",
            None,
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "PER: Jean Jacques, Jaime",
            strip_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "PER: Jean Jacques, Jaime",
            lowercase_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "per: Jean Jacques, Jaime",
            strip_normalizer(),
            [],
        ),
        (
            "per: Jean Jacques, Jaime",
            None,
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "per: Jean Jacques\nPER: Jaime",
            lowercase_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "per: Jean Jacques, Jaime\nOrg: library",
            lowercase_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER"), ("library", "ORG")],
        ),
        (
            "per: Jean Jacques, Jaime\nRANDOM: library",
            lowercase_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
    ],
)
def test_ner_labels(response, normalizer, gold_ents):
    text = "Jean Jacques and Jaime went to the library."
    labels = "PER,ORG,LOC"
    llm_ner = make_ner_task_v2(labels=labels, normalizer=normalizer)
    # Prepare doc
    nlp = spacy.blank("en")
    doc_in = nlp.make_doc(text)
    # Pass to the parser
    # Note: parser() returns a list
    doc_out = list(llm_ner.parse_responses([[doc_in]], [[response]]))[0]
    pred_ents = [(ent.text, ent.label_) for ent in doc_out.ents]
    assert pred_ents == gold_ents


@pytest.mark.parametrize(
    "response,alignment_mode,gold_ents",
    [
        (
            "PER: Jacq",
            "strict",
            [],
        ),
        (
            "PER: Jacq",
            "contract",
            [],
        ),
        (
            "PER: Jacq",
            "expand",
            [("Jacques", "PER")],
        ),
        (
            "PER: Jean J",
            "contract",
            [("Jean", "PER")],
        ),
        (
            "PER: Jean Jacques, aim",
            "strict",
            [("Jean Jacques", "PER")],
        ),
        (
            "PER: random",
            "expand",
            [],
        ),
    ],
)
def test_ner_alignment(response, alignment_mode, gold_ents):
    text = "Jean Jacques and Jaime went to the library."
    labels = "PER,ORG,LOC"
    llm_ner = make_ner_task_v2(labels=labels, alignment_mode=alignment_mode)  # type: ignore
    # Prepare doc
    nlp = spacy.blank("en")
    doc_in = nlp.make_doc(text)
    # Pass to the parser
    # Note: parser() returns a list
    doc_out = list(llm_ner.parse_responses([[doc_in]], [[response]]))[0]
    pred_ents = [(ent.text, ent.label_) for ent in doc_out.ents]
    assert pred_ents == gold_ents


def test_invalid_alignment_mode():
    labels = "PER,ORG,LOC"
    with pytest.raises(ValueError, match="Unsupported alignment mode 'invalid"):
        make_ner_task_v2(labels=labels, alignment_mode="invalid")  # type: ignore


@pytest.mark.parametrize(
    "response,case_sensitive,single_match,gold_ents",
    [
        (
            "PER: Jean",
            False,
            False,
            [("jean", "PER"), ("Jean", "PER"), ("Jean", "PER")],
        ),
        (
            "PER: Jean",
            False,
            True,
            [("jean", "PER")],
        ),
        (
            "PER: Jean",
            True,
            False,
            [("Jean", "PER"), ("Jean", "PER")],
        ),
        (
            "PER: Jean",
            True,
            True,
            [("Jean", "PER")],
        ),
    ],
)
def test_ner_matching(response, case_sensitive, single_match, gold_ents):
    text = "This guy jean (or Jean) is the president of the Jean Foundation."
    labels = "PER,ORG,LOC"
    llm_ner = make_ner_task_v2(
        labels=labels, case_sensitive_matching=case_sensitive, single_match=single_match
    )
    # Prepare doc
    nlp = spacy.blank("en")
    doc_in = nlp.make_doc(text)
    # Pass to the parser
    # Note: parser() returns a list
    doc_out = list(llm_ner.parse_responses([[doc_in]], [[response]]))[0]
    pred_ents = [(ent.text, ent.label_) for ent in doc_out.ents]
    assert pred_ents == gold_ents


def test_jinja_template_rendering_without_examples():
    """Test if jinja template renders as we expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    labels = "PER,ORG,LOC"
    nlp = spacy.blank("en")
    doc = nlp.make_doc("Alice and Bob went to the supermarket")

    llm_ner = make_ner_task_v2(labels=labels, examples=None)
    prompt = list(llm_ner.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == """
You are an expert Named Entity Recognition (NER) system. Your task is to accept Text as input and extract named entities for the set of predefined entity labels.
From the Text input provided, extract named entities for each label in the following format:

LOC: <comma delimited list of strings>
ORG: <comma delimited list of strings>
PER: <comma delimited list of strings>


Here is the text that needs labeling:

Text:
'''
Alice and Bob went to the supermarket
'''
""".strip()
    )


@pytest.mark.parametrize(
    "examples_path",
    [
        str(EXAMPLES_DIR / "ner.json"),
        str(EXAMPLES_DIR / "ner.yml"),
        str(EXAMPLES_DIR / "ner.jsonl"),
    ],
)
def test_jinja_template_rendering_with_examples(examples_path):
    """Test if jinja2 template renders as expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    labels = "PER,ORG,LOC"
    nlp = spacy.blank("en")
    doc = nlp.make_doc("Alice and Bob went to the supermarket")

    examples = fewshot_reader(examples_path)
    llm_ner = make_ner_task_v2(labels=labels, examples=examples)
    prompt = list(llm_ner.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == """
You are an expert Named Entity Recognition (NER) system. Your task is to accept Text as input and extract named entities for the set of predefined entity labels.
From the Text input provided, extract named entities for each label in the following format:

LOC: <comma delimited list of strings>
ORG: <comma delimited list of strings>
PER: <comma delimited list of strings>


Below are some examples (only use these as a guide):

Text:
'''
Jack and Jill went up the hill.
'''

PER: Jack, Jill
LOC: hill

Text:
'''
Jack fell down and broke his crown.
'''

PER: Jack

Text:
'''
Jill came tumbling after.
'''

PER: Jill


Here is the text that needs labeling:

Text:
'''
Alice and Bob went to the supermarket
'''""".strip()
    )


def test_jinja_template_rendering_with_label_definitions():
    """Test if jinja2 template renders as expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    labels = "PER,ORG,LOC"
    nlp = spacy.blank("en")
    doc = nlp.make_doc("Alice and Bob went to the supermarket")
    llm_ner = make_ner_task_v2(
        labels=labels,
        label_definitions={
            "PER": "Person definition",
            "ORG": "Organization definition",
            "LOC": "Location definition",
        },
    )
    prompt = list(llm_ner.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == """
You are an expert Named Entity Recognition (NER) system. Your task is to accept Text as input and extract named entities for the set of predefined entity labels.
From the Text input provided, extract named entities for each label in the following format:

LOC: <comma delimited list of strings>
ORG: <comma delimited list of strings>
PER: <comma delimited list of strings>

Below are definitions of each label to help aid you in what kinds of named entities to extract for each label.
Assume these definitions are written by an expert and follow them closely.

PER: Person definition
ORG: Organization definition
LOC: Location definition


Here is the text that needs labeling:

Text:
'''
Alice and Bob went to the supermarket
'''""".strip()
    )


def test_example_not_following_basemodel():
    wrong_example = [
        {
            "text": "I'm a wrong example. Entities should be a dict, not a list",
            # Should be: {"PER": ["Entities"], "ORG": ["dict", "list"]}
            "entities": [("PER", ("Entities")), ("ORG", ("dict", "list"))],
        }
    ]
    with make_tempdir() as tmpdir:
        tmp_path = tmpdir / "wrong_example.yml"
        srsly.write_yaml(tmp_path, wrong_example)

        with pytest.raises(ValueError):
            make_ner_task_v2(labels="PER,ORG,LOC", examples=fewshot_reader(tmp_path))


def test_external_template_actually_loads():
    template_path = str(TEMPLATES_DIR / "ner.jinja2")
    template = file_reader(template_path)
    labels = "PER,ORG,LOC"
    nlp = spacy.blank("en")
    doc = nlp.make_doc("Alice and Bob went to the supermarket")

    llm_ner = make_ner_task_v2(labels=labels, template=template)
    prompt = list(llm_ner.generate_prompts([doc]))[0][0][0]
    assert (
        prompt.strip()
        == """
This is a test NER template. Here are the labels
LOC
ORG
PER

Here is the text: Alice and Bob went to the supermarket
""".strip()
    )


@pytest.fixture
def noop_config():
    return """
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.NER.v2"
    labels = PER,ORG,LOC

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "test.NoOpModel.v1"
    output = "PER: Alice,Bob"
    """


@pytest.mark.parametrize("n_detections", [0, 1, 2])
def test_ner_scoring(noop_config, n_detections):
    config = Config().from_str(noop_config)
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp = assemble_from_config(config)

    examples = []

    for text in ["Alice works with Bob.", "Bob lives with Alice."]:
        predicted = nlp.make_doc(text)
        reference = predicted.copy()

        reference.ents = [
            Span(reference, 0, 1, label="PER"),
            Span(reference, 3, 4, label="PER"),
        ][:n_detections]

        examples.append(Example(predicted, reference))

    scores = nlp.evaluate(examples)
    assert scores["ents_p"] == n_detections / 2


@pytest.mark.parametrize("n_prompt_examples", [-1, 0, 1, 2])
def test_ner_init(noop_config, n_prompt_examples: int):
    config = Config().from_str(noop_config)
    del config["components"]["llm"]["task"]["labels"]

    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp = assemble_from_config(config)

    examples = []

    for text in [
        "Alice works with Bob in London.",
        "Bob lives with Alice in Manchester.",
    ]:
        predicted = nlp.make_doc(text)
        reference = predicted.copy()

        reference.ents = [
            Span(reference, 0, 1, label="PER"),
            Span(reference, 3, 4, label="PER"),
            Span(reference, 5, 6, label="LOC"),
        ]

        examples.append(Example(predicted, reference))

    _, llm = nlp.pipeline[0]
    task: NERTask = llm._task

    assert set(task._label_dict.values()) == set()
    assert not task._prompt_examples

    nlp.config["initialize"]["components"]["llm"] = {
        "n_prompt_examples": n_prompt_examples
    }
    nlp.initialize(lambda: examples)

    assert set(task._label_dict.values()) == {"PER", "LOC"}
    if n_prompt_examples >= 0:
        assert len(task._prompt_examples) == n_prompt_examples
    else:
        assert len(task._prompt_examples) == len(examples)

    if n_prompt_examples > 0:
        for eg in task._prompt_examples:
            assert set(eg.entities.keys()) == {"PER", "LOC"}


def test_ner_serde(noop_config):
    config = Config().from_str(noop_config)
    del config["components"]["llm"]["task"]["labels"]
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp1 = assemble_from_config(config)
        nlp2 = assemble_from_config(config)

    labels = {"loc": "LOC", "per": "PER"}

    task1: NERTask = nlp1.get_pipe("llm")._task
    task2: NERTask = nlp2.get_pipe("llm")._task

    # Artificially add labels to task1
    task1._label_dict = labels

    assert task1._label_dict == labels
    assert task2._label_dict == dict()

    b = nlp1.to_bytes()
    nlp2.from_bytes(b)

    assert task1._label_dict == task2._label_dict == labels


def test_ner_to_disk(noop_config, tmp_path: Path):
    config = Config().from_str(noop_config)
    del config["components"]["llm"]["task"]["labels"]

    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp1 = assemble_from_config(config)
        nlp2 = assemble_from_config(config)

    labels = {"loc": "LOC", "per": "PER"}

    task1: NERTask = nlp1.get_pipe("llm")._task
    task2: NERTask = nlp2.get_pipe("llm")._task

    # Artificially add labels to task1
    task1._label_dict = labels

    assert task1._label_dict == labels
    assert task2._label_dict == dict()

    path = tmp_path / "model"

    nlp1.to_disk(path)

    cfgs = list(path.rglob("cfg"))
    assert len(cfgs) == 1

    cfg = json.loads(cfgs[0].read_text())
    assert cfg["_label_dict"] == labels

    nlp2.from_disk(path)

    assert task1._label_dict == task2._label_dict == labels


@pytest.mark.filterwarnings("ignore:Task supports sharding")
def test_label_inconsistency():
    """Test whether inconsistency between specified labels and labels in examples is detected."""
    cfg = f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.NER.v2"
    labels = ["PERSON", "LOCATION"]

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str((Path(__file__).parent / "examples" / "ner_inconsistent.yml"))}

    [components.llm.model]
    @llm_models = "test.NoOpModel.v1"
    """

    config = Config().from_str(cfg)
    with pytest.warns(
        UserWarning,
        match=re.escape(
            "Examples contain labels that are not specified in the task configuration. The latter contains the "
            "following labels: ['LOCATION', 'PERSON']. Labels in examples missing from the task configuration: "
            "['TECH']. Please ensure your label specification and example labels are consistent."
        ),
    ):
        nlp = assemble_from_config(config)

    prompt_examples = nlp.get_pipe("llm")._task._prompt_examples
    assert len(prompt_examples) == 2
    assert prompt_examples[0].text == "Jack and Jill went up the hill."
    assert prompt_examples[0].entities == {
        "LOCATION": ["hill"],
        "PERSON": ["Jack", "Jill"],
    }
    assert (
        prompt_examples[1].text
        == "Jack and Jill went up the hill and spaCy is a great tool."
    )
    assert prompt_examples[1].entities == {
        "LOCATION": ["hill"],
        "PERSON": ["Jack", "Jill"],
    }



================================================
FILE: spacy_llm/tests/tasks/legacy/test_spancat.py
================================================
from pathlib import Path

import pytest
import spacy
import srsly
from confection import Config
from spacy.tokens import Span
from spacy.training import Example
from spacy.util import make_tempdir

from spacy_llm.pipeline import LLMWrapper
from spacy_llm.registry import fewshot_reader, lowercase_normalizer, strip_normalizer
from spacy_llm.tasks.spancat import SpanCatTask, make_spancat_task_v2
from spacy_llm.tasks.util import find_substrings
from spacy_llm.ty import LabeledTask, ShardingLLMTask
from spacy_llm.util import assemble_from_config, split_labels

from ...compat import has_openai_key

EXAMPLES_DIR = Path(__file__).parent / "examples"


@pytest.fixture
def zeroshot_cfg_string():
    return """
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.SpanCat.v2"
    labels = PER,ORG,LOC

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v1"
    """


@pytest.fixture
def fewshot_cfg_string():
    return f"""
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.SpanCat.v2"
    labels = ["PER", "ORG", "LOC"]

    [components.llm.task.examples]
    @misc = "spacy.FewShotReader.v1"
    path = {str((Path(__file__).parent / "examples" / "ner.yml"))}

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "spacy.GPT-3-5.v1"
    """


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize("cfg_string", ["fewshot_cfg_string", "zeroshot_cfg_string"])
def test_spancat_config(cfg_string, request):
    cfg_string = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg_string)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]

    pipe = nlp.get_pipe("llm")
    assert isinstance(pipe, LLMWrapper)
    assert isinstance(pipe.task, ShardingLLMTask)

    labels = orig_config["components"]["llm"]["task"]["labels"]
    labels = split_labels(labels)
    task = pipe.task
    assert isinstance(task, LabeledTask)
    assert sorted(task.labels) == sorted(tuple(labels))
    assert pipe.labels == task.labels
    assert nlp.pipe_labels["llm"] == list(task.labels)


@pytest.mark.external
@pytest.mark.parametrize("cfg_string", ["zeroshot_cfg_string", "fewshot_cfg_string"])
def test_spancat_predict(cfg_string, request):
    """Use OpenAI to get zero-shot NER results.
    Note that this test may fail randomly, as the LLM's output is unguaranteed to be consistent/predictable
    """
    cfg_string = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg_string)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    text = "Marc and Bob both live in Ireland."
    doc = nlp(text)
    assert len(doc.spans["sc"]) > 0
    for ent in doc.spans["sc"]:
        assert ent.label_ in ["PER", "ORG", "LOC"]


@pytest.mark.external
@pytest.mark.parametrize("cfg_string", ["zeroshot_cfg_string", "fewshot_cfg_string"])
def test_spancat_io(cfg_string, request):
    cfg_string = request.getfixturevalue(cfg_string)
    orig_config = Config().from_str(cfg_string)
    nlp = spacy.util.load_model_from_config(orig_config, auto_fill=True)
    assert nlp.pipe_names == ["llm"]
    # ensure you can save a pipeline to disk and run it after loading
    with make_tempdir() as tmpdir:
        nlp.to_disk(tmpdir)
        nlp2 = spacy.load(tmpdir)
    assert nlp2.pipe_names == ["llm"]
    text = "Marc and Bob both live in Ireland."
    doc = nlp2(text)
    assert len(doc.spans["sc"]) > 0
    for ent in doc.spans["sc"]:
        assert ent.label_ in ["PER", "ORG", "LOC"]


@pytest.mark.parametrize(
    "text,input_strings,result_strings,result_offsets",
    [
        (
            "Felipe and Jaime went to the library.",
            ["Felipe", "Jaime", "library"],
            ["Felipe", "Jaime", "library"],
            [(0, 6), (11, 16), (29, 36)],
        ),  # simple
        (
            "The Manila Observatory was founded in 1865 in Manila.",
            ["Manila", "The Manila Observatory"],
            ["Manila", "Manila", "The Manila Observatory"],
            [(4, 10), (46, 52), (0, 22)],
        ),  # overlapping and duplicated
        (
            "Take the road from downtown and turn left at the public market.",
            ["public market", "downtown"],
            ["public market", "downtown"],
            [(49, 62), (19, 27)]
            # flipped
        ),
    ],
)
def test_ensure_offsets_correspond_to_substrings(
    text, input_strings, result_strings, result_offsets
):
    offsets = find_substrings(text, input_strings)
    # Compare strings instead of offsets, but we need to get
    # those strings first from the text
    assert result_offsets == offsets
    found_substrings = [text[start:end] for start, end in offsets]
    assert result_strings == found_substrings


@pytest.mark.parametrize(
    "text,response,gold_spans",
    [
        # simple
        (
            "Jean Jacques and Jaime went to the library.",
            "PER: Jean Jacques, Jaime\nLOC: library",
            [("Jean Jacques", "PER"), ("Jaime", "PER"), ("library", "LOC")],
        ),
        # overlapping: should only return all spans
        (
            "The Manila Observatory was founded in 1865.",
            "LOC: The Manila Observatory, Manila, Manila Observatory",
            [
                ("The Manila Observatory", "LOC"),
                ("Manila", "LOC"),
                ("Manila Observatory", "LOC"),
            ],
        ),
        # flipped: order shouldn't matter
        (
            "Take the road from Downtown and turn left at the public market.",
            "LOC: public market, Downtown",
            [("Downtown", "LOC"), ("public market", "LOC")],
        ),
    ],
)
def test_spancat_zero_shot_task(text, response, gold_spans):
    labels = "PER,ORG,LOC"
    llm_spancat = make_spancat_task_v2(labels=labels)
    # Prepare doc
    nlp = spacy.blank("en")
    doc_in = nlp.make_doc(text)
    # Pass to the parser
    # Note: parser() returns a list so we get what's inside
    doc_out = list(llm_spancat.parse_responses([[doc_in]], [[response]]))[0]
    pred_spans = [(span.text, span.label_) for span in doc_out.spans["sc"]]
    assert pred_spans == gold_spans


@pytest.mark.parametrize(
    "response,normalizer,gold_spans",
    [
        (
            "PER: Jean Jacques, Jaime",
            None,
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "PER: Jean Jacques, Jaime",
            strip_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "PER: Jean Jacques, Jaime",
            lowercase_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "per: Jean Jacques, Jaime",
            strip_normalizer(),
            [],
        ),
        (
            "per: Jean Jacques, Jaime",
            None,
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "per: Jean Jacques\nPER: Jaime",
            lowercase_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
        (
            "per: Jean Jacques, Jaime\nOrg: library",
            lowercase_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER"), ("library", "ORG")],
        ),
        (
            "per: Jean Jacques, Jaime\nRANDOM: library",
            lowercase_normalizer(),
            [("Jean Jacques", "PER"), ("Jaime", "PER")],
        ),
    ],
)
def test_spancat_labels(response, normalizer, gold_spans):
    text = "Jean Jacques and Jaime went to the library."
    labels = "PER,ORG,LOC"
    llm_spancat = make_spancat_task_v2(labels=labels, normalizer=normalizer)
    # Prepare doc
    nlp = spacy.blank("en")
    doc_in = nlp.make_doc(text)
    # Pass to the parser
    # Note: parser() returns a list
    doc_out = list(llm_spancat.parse_responses([[doc_in]], [[response]]))[0]
    pred_spans = [(span.text, span.label_) for span in doc_out.spans["sc"]]
    assert pred_spans == gold_spans


@pytest.mark.parametrize(
    "response,alignment_mode,gold_spans",
    [
        (
            "PER: Jacq",
            "strict",
            [],
        ),
        (
            "PER: Jacq",
            "contract",
            [],
        ),
        (
            "PER: Jacq",
            "expand",
            [("Jacques", "PER")],
        ),
        (
            "PER: Jean J",
            "contract",
            [("Jean", "PER")],
        ),
        (
            "PER: Jean Jacques, aim",
            "strict",
            [("Jean Jacques", "PER")],
        ),
        (
            "PER: random",
            "expand",
            [],
        ),
    ],
)
def test_spancat_alignment(response, alignment_mode, gold_spans):
    text = "Jean Jacques and Jaime went to the library."
    labels = "PER,ORG,LOC"
    llm_spancat = make_spancat_task_v2(labels=labels, alignment_mode=alignment_mode)  # type: ignore
    # Prepare doc
    nlp = spacy.blank("en")
    doc_in = nlp.make_doc(text)
    # Pass to the parser
    # Note: parser() returns a list
    doc_out = list(llm_spancat.parse_responses([[doc_in]], [[response]]))[0]
    pred_spans = [(span.text, span.label_) for span in doc_out.spans["sc"]]
    assert pred_spans == gold_spans


def test_invalid_alignment_mode():
    labels = "PER,ORG,LOC"
    with pytest.raises(ValueError, match="Unsupported alignment mode 'invalid"):
        make_spancat_task_v2(labels=labels, alignment_mode="invalid")  # type: ignore


@pytest.mark.parametrize(
    "response,case_sensitive,single_match,gold_spans",
    [
        (
            "PER: Jean",
            False,
            False,
            [("jean", "PER"), ("Jean", "PER"), ("Jean", "PER")],
        ),
        (
            "PER: Jean",
            False,
            True,
            [("jean", "PER")],
        ),
        (
            "PER: Jean",
            True,
            False,
            [("Jean", "PER"), ("Jean", "PER")],
        ),
        (
            "PER: Jean",
            True,
            True,
            [("Jean", "PER")],
        ),
    ],
)
def test_spancat_matching(response, case_sensitive, single_match, gold_spans):
    text = "This guy jean (or Jean) is the president of the Jean Foundation."
    labels = "PER,ORG,LOC"
    llm_spancat = make_spancat_task_v2(
        labels=labels, case_sensitive_matching=case_sensitive, single_match=single_match
    )
    # Prepare doc
    nlp = spacy.blank("en")
    doc_in = nlp.make_doc(text)
    # Pass to the parser
    # Note: parser() returns a list
    doc_out = list(llm_spancat.parse_responses([[doc_in]], [[response]]))[0]
    pred_spans = [(span.text, span.label_) for span in doc_out.spans["sc"]]
    assert pred_spans == gold_spans


def test_jinja_template_rendering_without_examples():
    """Test if jinja template renders as we expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    labels = "PER,ORG,LOC"
    nlp = spacy.blank("en")
    doc = nlp.make_doc("Alice and Bob went to the supermarket")

    llm_spancat = make_spancat_task_v2(labels=labels, examples=None)
    prompt = list(llm_spancat.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == """
You are an expert Named Entity Recognition (NER) system. Your task is to accept Text as input and extract named entities for the set of predefined entity labels.
The entities you extract for each label can overlap with each other.
From the Text input provided, extract named entities for each label in the following format:

LOC: <comma delimited list of strings>
ORG: <comma delimited list of strings>
PER: <comma delimited list of strings>


Here is the text that needs labeling:

Text:
'''
Alice and Bob went to the supermarket
'''
""".strip()
    )


@pytest.mark.parametrize(
    "examples_path",
    [
        str(EXAMPLES_DIR / "ner.json"),
        str(EXAMPLES_DIR / "ner.yml"),
        str(EXAMPLES_DIR / "ner.jsonl"),
    ],
)
def test_jinja_template_rendering_with_examples(examples_path):
    """Test if jinja2 template renders as expected

    We apply the .strip() method for each prompt so that we don't have to deal
    with annoying newlines and spaces at the edge of the text.
    """
    labels = "PER,ORG,LOC"
    nlp = spacy.blank("en")
    doc = nlp.make_doc("Alice and Bob went to the supermarket")

    examples = fewshot_reader(examples_path)
    llm_spancat = make_spancat_task_v2(labels=labels, examples=examples)
    prompt = list(llm_spancat.generate_prompts([doc]))[0][0][0]

    assert (
        prompt.strip()
        == """
You are an expert Named Entity Recognition (NER) system. Your task is to accept Text as input and extract named entities for the set of predefined entity labels.
The entities you extract for each label can overlap with each other.
From the Text input provided, extract named entities for each label in the following format:

LOC: <comma delimited list of strings>
ORG: <comma delimited list of strings>
PER: <comma delimited list of strings>


Below are some examples (only use these as a guide):

Text:
'''
Jack and Jill went up the hill.
'''

PER: Jack, Jill
LOC: hill

Text:
'''
Jack fell down and broke his crown.
'''

PER: Jack

Text:
'''
Jill came tumbling after.
'''

PER: Jill


Here is the text that needs labeling:

Text:
'''
Alice and Bob went to the supermarket
'''""".strip()
    )


def test_example_not_following_basemodel():
    wrong_example = [
        {
            "text": "I'm a wrong example. Entities should be a dict, not a list",
            # Should be: {"PER": ["Entities"], "ORG": ["dict", "list"]}
            "entities": [("PER", ("Entities")), ("ORG", ("dict", "list"))],
        }
    ]
    with make_tempdir() as tmpdir:
        tmp_path = tmpdir / "wrong_example.yml"
        srsly.write_yaml(tmp_path, wrong_example)

    with pytest.raises(ValueError):
        make_spancat_task_v2(labels="PER,ORG,LOC", examples=fewshot_reader(tmp_path))


@pytest.fixture
def noop_config():
    return """
    [nlp]
    lang = "en"
    pipeline = ["llm"]
    batch_size = 128

    [components]

    [components.llm]
    factory = "llm"

    [components.llm.task]
    @llm_tasks = "spacy.SpanCat.v2"
    labels = ["PER", "ORG", "LOC"]

    [components.llm.task.normalizer]
    @misc = "spacy.LowercaseNormalizer.v1"

    [components.llm.model]
    @llm_models = "test.NoOpModel.v1"
    output = "PER: Bob,Alice"
    """


@pytest.mark.parametrize("n_detections", [0, 1, 2])
def test_spancat_scoring(noop_config, n_detections):
    config = Config().from_str(noop_config)
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp = assemble_from_config(config)

    examples = []

    for text in ["Alice works with Bob.", "Bob lives with Alice."]:
        predicted = nlp.make_doc(text)
        reference = predicted.copy()

        reference.spans["sc"] = [
            Span(reference, 0, 1, label="PER"),
            Span(reference, 3, 4, label="PER"),
        ][:n_detections]

        examples.append(Example(predicted, reference))

    scores = nlp.evaluate(examples)
    assert scores["spans_sc_p"] == n_detections / 2


@pytest.mark.parametrize("n_prompt_examples", [-1, 0, 1, 2])
def test_spancat_init(noop_config, n_prompt_examples: bool):
    config = Config().from_str(noop_config)
    del config["components"]["llm"]["task"]["labels"]
    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp = assemble_from_config(config)

    examples = []

    for text in [
        "Alice works with Bob in London.",
        "Bob lives with Alice in Manchester.",
    ]:
        predicted = nlp.make_doc(text)
        reference = predicted.copy()

        reference.spans["sc"] = [
            Span(reference, 0, 1, label="PER"),
            Span(reference, 3, 4, label="PER"),
            Span(reference, 5, 6, label="LOC"),
        ]

        examples.append(Example(predicted, reference))

    _, llm = nlp.pipeline[0]
    task: SpanCatTask = llm._task  # type: ignore

    assert set(task._label_dict.values()) == set()
    assert not task._prompt_examples

    nlp.config["initialize"]["components"]["llm"] = {
        "n_prompt_examples": n_prompt_examples
    }

    nlp.initialize(lambda: examples)

    assert set(task._label_dict.values()) == {"PER", "LOC"}
    if n_prompt_examples >= 0:
        assert len(task._prompt_examples) == n_prompt_examples
    else:
        assert len(task._prompt_examples) == len(examples)

    if n_prompt_examples > 0:
        for eg in task._prompt_examples:
            assert set(eg.entities.keys()) == {"PER", "LOC"}


def test_spancat_serde(noop_config):
    config = Config().from_str(noop_config)
    del config["components"]["llm"]["task"]["labels"]

    with pytest.warns(UserWarning, match="Task supports sharding"):
        nlp1 = assemble_from_config(config)
        nlp2 = assemble_from_config(config)

    labels = {"loc": "LOC", "per": "PER"}

    task1: SpanCatTask = nlp1.get_pipe("llm")._task  # type: ignore
    task2: SpanCatTask = nlp2.get_pipe("llm")._task  # type: ignore

    # Artificially add labels to task1
    task1._label_dict = labels

    assert task1._label_dict == labels
    assert task2._label_dict == dict()

    b = nlp1.to_bytes()
    nlp2.from_bytes(b)

    assert task1._label_dict == task2._label_dict == labels



================================================
FILE: spacy_llm/tests/tasks/legacy/examples/ner.json
================================================
[
  {
    "text": "Jack and Jill went up the hill.",
    "entities": {
      "PER": [
        "Jack",
        "Jill"
      ],
      "LOC": [
        "hill"
      ]
    }
  },
  {
    "text": "Jack fell down and broke his crown.",
    "entities": {
      "PER": [
        "Jack"
      ]
    }
  },
  {
    "text": "Jill came tumbling after.",
    "entities": {
      "PER": [
        "Jill"
      ]
    }
  }
]


================================================
FILE: spacy_llm/tests/tasks/legacy/examples/ner.jsonl
================================================
{"text":"Jack and Jill went up the hill.","entities":{"PER":["Jack","Jill"],"LOC":["hill"]}}
{"text":"Jack fell down and broke his crown.","entities":{"PER":["Jack"]}}
{"text":"Jill came tumbling after.","entities":{"PER":["Jill"]}}



================================================
FILE: spacy_llm/tests/tasks/legacy/examples/ner.yml
================================================
- text: Jack and Jill went up the hill.
  entities:
    PER:
      - Jack
      - Jill
    LOC:
      - hill
- text: Jack fell down and broke his crown.
  entities:
    PER:
      - Jack
- text: Jill came tumbling after.
  entities:
    PER:
      - Jill



================================================
FILE: spacy_llm/tests/tasks/legacy/examples/ner_inconsistent.yml
================================================
- text: Jack and Jill went up the hill.
  entities:
    PERSON:
      - Jack
      - Jill
    LOCATION:
      - hill
- text: spaCy is a great tool
  entities:
    TECH:
      - spaCy
- text: Jack and Jill went up the hill and spaCy is a great tool.
  entities:
    PERSON:
      - Jack
      - Jill
    LOCATION:
      - hill
    TECH:
      - spaCy


================================================
FILE: spacy_llm/tests/tasks/legacy/templates/ner.jinja2
================================================
This is a test NER template. Here are the labels
{# whitespace #}
{%- for label in labels -%}
{{ label }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
Here is the text: {{ text }}


================================================
FILE: spacy_llm/tests/tasks/misc/el_kb_data.yml
================================================
entities:
  Q100:
      name: "Boston"
      desc: "city in and state capital of Massachusetts, United States"
  Q131371:
      name: "Boston Celtics"
      desc: "NBA team based in Boston; tied with most NBA Championships"
  Q204289:
    name: "Boston"
    desc: "American rock band"
  Q311975:
    name: "Boston"
    desc: "town in Lincolnshire, England"
  Q671475:
    name: "Logan International Airport"
    desc: "airport in Boston, Massachusetts, United States"
  Q107723060:
    name: "2021â€“22 Boston Celtics season"
    desc: "The 2021â€“22 Boston Celtics season was the 76th season of the franchise in the National Basketball Association (NBA). Following the Celtics' first-round exit to the Brooklyn Nets in five games from las"
  Q3643001:
    name: "Boston"
    desc: "NBA basketball team season"
  Q3466394:
    name: "Boston"
    desc: "season of National Basketball Association team the Boston Celtics"
  Q3642995:
    name: "Boston"
    desc: "NBA basketball team season"
  Q60:
    name: "New York"
    desc: "most populous city in the United States"
  Q1384:
    name: "New York"
    desc: "U.S. state"
  Q131364:
    name: "New York Knicks"
    desc: "National Basketball Association team in New York City"
  Q14435:
    name: "Big Apple"
    desc: "nickname for New York City"
  Q89:
    name: "Apple"
    desc: "fruit of the apple tree"
  Q312:
    name: "Apple"
    desc: "American multinational technology company"
aliases:
  - alias: "Boston"
    entities: ["Q100", "Q131371", "Q204289", "Q311975", "Q671475"]
    probabilities: [0.5, 0.2, 0.12, 0.1, 0.08]
  - alias: "Boston Celtics"
    entities: ["Q131371", "Q107723060", "Q3643001", "Q3466394", "Q3642995"]
    probabilities: [0.5, 0.2, 0.12, 0.1, 0.08]
  - alias: "New York"
    entities: ["Q60", "Q1384"]
    probabilities: [0.6, 0.4]
  - alias: "New York Knicks"
    entities: ["Q60", "Q131364"]
    probabilities: [0.6, 0.4]
  - alias: "Big Apple"
    entities: ["Q14435", "Q89"]
    probabilities: [0.6, 0.4]
  - alias: "Apple"
    entities: ["Q89", "Q312"]
    probabilities: [0.6, 0.4]
  


================================================
FILE: spacy_llm/tests/tasks/templates/entity_linker.jinja2
================================================
This is a test entity linking template.
Here is the text: {{ text }}


================================================
FILE: spacy_llm/tests/tasks/templates/lemma.jinja2
================================================
This is a test LEMMA template.
Here is the text: {{ text }}


================================================
FILE: spacy_llm/tests/tasks/templates/ner.jinja2
================================================
Here's the test template for the tests and stuff
{# whitespace #}
{{ description }}
{# whitespace #}
{%- if label_definitions -%}
Below are definitions of each label to help aid you in what kinds of named entities to extract for each label.
Assume these definitions are written by an expert and follow them closely.
{# whitespace #}
{%- for label, definition in label_definitions.items() -%}
{{ label }}: {{ definition }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- endif -%}
Q: Given the paragraph below, identify a list of entities, and for each entry explain why it is or is not an entity:
{# whitespace #}
{# whitespace #}
{%- for example in prompt_examples -%}
Paragraph: {{ example.text }}
Answer:
{# whitespace #}
{%- for span in example.spans -%}
{{ loop.index }}. {{ span.to_str() }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- endfor -%}
Paragraph: {{ text }}
Answer:



================================================
FILE: spacy_llm/tests/tasks/templates/raw.jinja2
================================================
This is a test RAW template.
Here is the text: {{ text }}


================================================
FILE: spacy_llm/tests/tasks/templates/sentiment.jinja2
================================================
Text: {{ text }}
Sentiment:


================================================
FILE: spacy_llm/tests/tasks/templates/spancat.jinja2
================================================
Here's the test template for the tests and stuff
The entities you extract can overlap with each other.
{# whitespace #}
{{ description }}
{# whitespace #}
{%- if label_definitions -%}
Below are definitions of each label to help aid you in what kinds of named entities to extract for each label.
Assume these definitions are written by an expert and follow them closely.
{# whitespace #}
{%- for label, definition in label_definitions.items() -%}
{{ label }}: {{ definition }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- endif -%}
Q: Given the paragraph below, identify a list of entities, and for each entry explain why it is or is not an entity:
{# whitespace #}
{# whitespace #}
{%- for example in prompt_examples -%}
Paragraph: {{ example.text }}
Answer:
{# whitespace #}
{%- for span in example.spans -%}
{{ loop.index }}. {{ span.to_str() }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{# whitespace #}
{%- endfor -%}
Paragraph: {{ text }}
Answer:



================================================
FILE: spacy_llm/tests/tasks/templates/summarization.jinja2
================================================
This is a test summarization template.
Here is the text: {{ text }}


================================================
FILE: spacy_llm/tests/tasks/templates/textcat.jinja2
================================================
This is a test textcat template. Here is/are the label/s
{# whitespace #}
{%- for label in labels -%}
{{ label }}
{# whitespace #}
{%- endfor -%}
{# whitespace #}
Here is the text: {{ text }}


================================================
FILE: spacy_llm/tests/tasks/templates/translation.jinja2
================================================
Text: {{ text }}
Translation:


================================================
FILE: usage_examples/README.md
================================================
# Usage examples

This directory contains different examples on how you can use `spacy-llm` to
simulate or prototype common NLP tasks. Each directory contains a sample
configuration and an optional `examples.yml` file for few-shot annotation.

## The configuration file

Each configuration file contains an `llm` component that takes in a `task` and a
`model` as its parameters. `task` defines how the prompt is structured and
how the corresponding LLM output will be parsed whereas `model` defines
which model to use and how to connect to it.

```ini
...
[components]

[components.llm]
factory = "llm"

# Defines the prompt you'll send to an LLM, and how the corresponding output
# will be parsed.
[components.llm.task]
...

# Defines which model to use (open-source or third-party API) and how to connect
# to it (e.g., REST, LangChain, locally via HuggingFace, ...).
[components.llm.model]
...
```

The configuration files are based on [spaCy's configuration
system](https://spacy.io/api/data-formats#config). This means that `spacy-llm`
is modular and it's easy to implement your own tasks.

## Writing your own task

The common use-case for `spacy-llm` is to use a large language model (LLM) to
power a natural language processing pipeline. In `spacy-llm`, we define these
actions as **tasks**.

Think of a `task` as something you want an LLM to do. In our examples, we ask an
LLM to find named entities or categorize a text. Note that an LLM's output
should eventually be stored in a spaCy [`Doc`](https://spacy.io/api/doc). For
example, named entities are stored in
[`doc.ents`](https://spacy.io/api/doc#ents) while text categorization results
are in [`doc.cats`](https://spacy.io/api/doc#cats).

To write a
[`task`](https://github.com/explosion/spacy-llm/blob/main/README.md#tasks), you
need to implement two functions:

- **`generate_prompts(docs: Iterable[Doc]) -> Iterable[str]`**: a function that
  takes in a list of spaCy [`Doc`](https://spacy.io/api/doc) objects and transforms
  them into a list of prompts. These prompts will then be sent to the LLM in the
  `model`.
- **`parse_responses(docs: Iterable[Doc], responses: Iterable[str]) -> Iterable[Doc]`**: a function for parsing the LLM's outputs into spaCy
  [`Doc`](https://spacy.io/api/doc) objects. You also have access to the input
  `Doc` objects so you can store the outputs into one of its attributes.

The `spacy-llm` library requires tasks to be defined as a class and registered in the `llm_tasks` registry:

```python
from spacy_llm.registry import registry
from spacy_llm.util import split_labels


@registry.llm_tasks("my_namespace.MyTask.v1")
def make_my_task(labels: str, my_other_config_val: float) -> "MyTask":
    labels_list = split_labels(labels)
    return MyTask(labels=labels_list, my_other_config_val=my_other_config_val)


class MyTask:
    def __init__(self, labels: List[str], my_other_config_val: float):
        ...

    def generate_prompts(self, docs: Iterable[Doc]) -> Iterable[str]:
        ...

    def parse_responses(
        self, docs: Iterable[Doc], responses: Iterable[str]
    ) -> Iterable[Doc]:
        ...
```

```ini
# config.cfg (excerpt)
[components.llm.task]
@llm_tasks = "my_namespace.MyTask.v1"
labels = LABEL1,LABEL2,LABEL3
my_other_config_val = 0.3
```

You can check sample tasks for Named Entity Recognition and text categorization
in the `spacy_llm/tasks/` directory. We also recommend checking out the
`spacy.NoOp.v1` task for a barebones implementation to pattern your task from.

## Using LangChain

`spacy-llm` integrates [LangChain](https://github.com/hwchase17/langchain) to allow users to leverage its features for
prompt management and LLM usage in their spaCy workflows.

LangChain can be used like so:

```ini
[components.llm.model]
@llm_models = "langchain.OpenAI.v1"
name = "gpt-3.5-turbo"
```

<!-- The `usage_examples` directory contains example for all integrated third-party -->

## Writing your own model

In `spacy-llm`, the [**model**](../README.md#models) is responsible for the
interaction with the actual LLM model. The latter can be an
[API-based service](../README.md#spacyrestv1), or a local model - whether
you [downloaded it from the Hugging Face Hub](../README.md#spacydollyhfv1)
directly or finetuned it with proprietary data.

`spacy-llm` lets you implement your own custom model so you can try out the
latest LLM interface out there. Bear in mind that tasks are responsible for
creating the prompt and parsing the response â€“ and both can be arbitrary objects.
Hence, a model's call signature should be consistent with that of the task you'd like it to run.

In other words, `spacy-llm` roughly performs the following pseudo-code behind the scenes:

```python
prompts = task.generate_prompts(docs)
responses = model(prompts)
docs = task.parse_responses(docs, responses)
```

Let's write a dummy model that provides a random output for the
[text classification task](../README.md#spacytextcatv1).

```python
from spacy_llm.registry import registry
import random
from typing import Iterable

@registry.llm_models("RandomClassification.v1")
def random_textcat(labels: str):
    labels = labels.split(",")
    def _classify(prompts: Iterable[str]) -> Iterable[str]:
        for _ in prompts:
            yield random.choice(labels)

    return _classify
```

```ini
...
[components.llm.task]
@llm_tasks = "spacy.TextCat.v1"
labels = LABEL1,LABEL2,LABEL3


[components.llm.model]
@llm_models = "RandomClassification.v1"
labels = ${components.llm.task.labels}  # Make sure to use the same label
...
```

Of course, this particular model is not very realistic
(it does not even interact with an actual LLM model!).
But it does show how you would go about writing custom
and arbitrary logic to interact with any LLM implementation.

Note that in all built-in tasks prompts and responses are expected to be of type `str`, while all built-in model
support `str` (or `Any`) types. All built-in tasks and models are therefore inter-operable. It's possible to work with
arbitrary objects instead of `str` though - which might be useful if you want some third-party abstractions for prompts
or responses.



================================================
FILE: usage_examples/__init__.py
================================================
[Empty file]


================================================
FILE: usage_examples/el_openai/README.md
================================================
# Linking entities with LLMs

This example shows how you can perform entity linking with LLMs.
This requires detecting named entities (i. e. performing NER) beforehand. You can do this using spaCy's `ner` 
component or `spacy-llm`'s NER task. The default config in this example utilizes the pretrained NER component from 
`en_core_web_md` for that.

> âš ï¸ Ensure `en_core_web_md` is installed (`spacy download en_core_web_md`) before running this example.

Note that linking entities requires a knowledge base that defines the unique identifiers. `spacy-llm` natively supports spaCy's knowledge base class, but 
this object can contain any arbitrary knowledge base as long as the required interface is implemented.
For this example we provide a toy KB that supports a very limited number of entities (see 
[`el_kb_data.yml`](el_kb_data.yml)) - entities not listed in this file won't be linked.

First, create a new API key from [openai.com](https://platform.openai.com/account/api-keys) or fetch an existing one. Record the secret key and make sure this is
available as an environmental variable:

```sh
export OPENAI_API_KEY="sk-..."
export OPENAI_API_ORG="org-..."
```

Then, you can run the pipeline on a sample text via:

```sh
python run_pipeline.py [TEXT] [PATH TO CONFIG] [PATH TO FILE WITH EXAMPLES]
```

For example:

```sh
python run_pipeline.py \
    "The city of New York where John lives, lies in the state of New York." \
    ./zeroshot.cfg
```
or, for few-shot:
```sh
python run_pipeline.py \
    "The city of New York where John lives, lies in the state of New York." \
    ./fewshot.cfg \
    ./examples.yml
```

You can also include examples to perform few-shot annotation. To do so, use the
`fewshot.cfg` file instead. You can find the few-shot examples in
the `examples.yml` file. Feel free to change and update it to your liking.
We also support other file formats, including `.yaml`, `.jsonl` and `.json`.



================================================
FILE: usage_examples/el_openai/__init__.py
================================================
from .run_pipeline import run_pipeline

__all__ = ["run_pipeline"]



================================================
FILE: usage_examples/el_openai/el_kb_data.yml
================================================
entities:
  Q100:
      name: "Boston"
      desc: "city in and state capital of Massachusetts, United States"
  Q131371:
      name: "Boston Celtics"
      desc: "NBA team based in Boston; tied with most NBA Championships"
  Q204289:
    name: "Boston"
    desc: "American rock band"
  Q311975:
    name: "Boston"
    desc: "town in Lincolnshire, England"
  Q671475:
    name: "Logan International Airport"
    desc: "airport in Boston, Massachusetts, United States"
  Q107723060:
    name: "2021â€“22 Boston Celtics season"
    desc: "The 2021â€“22 Boston Celtics season was the 76th season of the franchise in the National Basketball Association (NBA). Following the Celtics' first-round exit to the Brooklyn Nets in five games from las"
  Q3643001:
    name: "Boston"
    desc: "NBA basketball team season"
  Q3466394:
    name: "Boston"
    desc: "season of National Basketball Association team the Boston Celtics"
  Q3642995:
    name: "Boston"
    desc: "NBA basketball team season"
  Q60:
    name: "New York"
    desc: "most populous city in the United States"
  Q1384:
    name: "New York"
    desc: "U.S. state"
  Q131364:
    name: "New York Knicks"
    desc: "National Basketball Association team in New York City"
  Q14435:
    name: "Big Apple"
    desc: "nickname for New York City"
  Q89:
    name: "Apple"
    desc: "fruit of the apple tree"
  Q312:
    name: "Apple"
    desc: "American multinational technology company"
aliases:
  - alias: "Boston"
    entities: ["Q100", "Q131371", "Q204289", "Q311975", "Q671475"]
    probabilities: [0.5, 0.2, 0.12, 0.1, 0.08]
  - alias: "Boston Celtics"
    entities: ["Q131371", "Q107723060", "Q3643001", "Q3466394", "Q3642995"]
    probabilities: [0.5, 0.2, 0.12, 0.1, 0.08]
  - alias: "New York"
    entities: ["Q60", "Q1384"]
    probabilities: [0.6, 0.4]
  - alias: "New York Knicks"
    entities: ["Q60", "Q131364"]
    probabilities: [0.6, 0.4]
  - alias: "Big Apple"
    entities: ["Q14435", "Q89"]
    probabilities: [0.6, 0.4]
  - alias: "Apple"
    entities: ["Q89", "Q312"]
    probabilities: [0.6, 0.4]
  


================================================
FILE: usage_examples/el_openai/examples.yml
================================================
- text: "Alice goes to *New York* to see the *New York Knicks* game."
  mentions_str: "*New York*, *New York Knicks*"
  mentions:
    - New York
    - New York Knicks
  entity_descriptions:
    -
      - most populous city in the United States
      - U.S. state
    -
      - most populous city in the United States
      - National Basketball Association team in New York City
  entity_ids:
    -
      - Q60
      - Q1384
    -
      - Q60
      - Q131364
  solutions:
    - Q60
    - Q131364
  reasons: []

- text: "*New York* is called the *Big Apple*. It also has *Apple* stores."
  mentions_str: "*New York*, *Big Apple*, *Apple*"
  mentions:
    - New York
    - Big Apple
    - Apple
  entity_descriptions:
    -
      - most populous city in the United States
      - U.S. state
    -
      - nickname for New York City
      - fruit of the apple tree
    -
      - fruit of the apple tree
      - American multinational technology company
  entity_ids:
    -
      - Q60
      - Q1384
    -
      - Q14435
      - Q89
    -
      - Q89
      - Q312
  solutions:
    - Q60
    - Q14435
    - Q312
  reasons:
    - The mention of "Big Apple" in the same context clarifies that this is about the city New York.
    - Big Apple is a well-known nickname of New York.
    - The context of "stores" indicates that this is about the technology company Apple, which operates "Apple stores".


================================================
FILE: usage_examples/el_openai/fewshot.cfg
================================================
[paths]
el_kb = null
examples = null

[nlp]
lang = "en"
pipeline = ["ner", "llm-el"]
batch_size = 128

[components]

[components.ner]
source = "en_core_web_md"
component = "ner"

[components.llm-el]
factory = "llm"

[components.llm-el.task]
@llm_tasks = "spacy.EntityLinker.v1"

[components.llm-el.task.examples]
@misc = "spacy.FewShotReader.v1"
path = ${paths.examples}

[components.llm-el.model]
@llm_models = "spacy.GPT-3-5.v1"
config = {"temperature": 0}

[initialize]
vectors = "en_core_web_md"

[initialize.components]
[initialize.components.llm-el]

[initialize.components.llm-el.candidate_selector]
@llm_misc = "spacy.CandidateSelector.v1"

[initialize.components.llm-el.candidate_selector.kb_loader]
@llm_misc = "spacy.KBFileLoader.v1"
path = ${paths.el_kb}



================================================
FILE: usage_examples/el_openai/run_pipeline.py
================================================
import os
from pathlib import Path
from typing import Optional

import typer
from spacy.tokens import Doc
from wasabi import msg

from spacy_llm.util import assemble

Arg = typer.Argument
Opt = typer.Option


def run_pipeline(
    # fmt: off
    text: str = Arg("", help="Text to perform text categorization on."),
    config_path: Path = Arg(..., help="Path to the configuration file to use."),
    examples_path: Optional[Path] = Arg(None, help="Path to the examples file to use (few-shot only)."),
    verbose: bool = Opt(False, "--verbose", "-v", help="Show extra information."),
    # fmt: on
) -> Doc:
    if not os.getenv("OPENAI_API_KEY", None):
        msg.fail(
            "OPENAI_API_KEY env variable was not found. "
            "Set it by running 'export OPENAI_API_KEY=...' and try again.",
            exits=1,
        )

    paths = {
        "paths.el_kb": str(Path(__file__).parent / "el_kb_data.yml"),
    }
    msg.text(f"Loading config from {config_path}", show=verbose)

    nlp = assemble(
        config_path,
        overrides={**paths}
        if examples_path is None
        else {**paths, "paths.examples": str(examples_path)},
    )

    doc = nlp(text)

    msg.text(f"Text: {doc.text}")
    msg.text(f"Entities: {[(ent.text, ent.label_, ent.kb_id_) for ent in doc.ents]}")

    return doc


if __name__ == "__main__":
    typer.run(run_pipeline)



================================================
FILE: usage_examples/el_openai/zeroshot.cfg
================================================
[paths]
el_kb = null

[nlp]
lang = "en"
pipeline = ["ner", "llm-el"]
batch_size = 128

[components]

[components.ner]
source = "en_core_web_md"

[components.llm-el]
factory = "llm"

[components.llm-el.task]
@llm_tasks = "spacy.EntityLinker.v1"

[components.llm-el.model]
@llm_models = "spacy.GPT-3-5.v1"
config = {"temperature": 0}

[initialize]
vectors = "en_core_web_md"

[initialize.components]
[initialize.components.llm-el]

[initialize.components.llm-el.candidate_selector]
@llm_misc = "spacy.CandidateSelector.v1"

[initialize.components.llm-el.candidate_selector.kb_loader]
@llm_misc = "spacy.KBFileLoader.v1"
path = ${paths.el_kb}



================================================
FILE: usage_examples/multitask_openai/README.md
================================================
# Performing multiple tasks in a single pipeline

This example shows how you can perform multiple LLM-backed tasks within
a single spaCy pipeline.

We could create a new custom task that performs all objectives in a single
LLM query, but in this example we'll only use built-in task templates to
see how easy it is to compose them. Note that breaking down tasks this way
might be a better choice anyway, since it allows you to better control the
performance of your pipeline.

This example shows how you can use a model from OpenAI for categorizing texts
as well as detect entities of interest in zero- or few-shot settings.
Here, we perform binary text classification to determine if a given text
is an `ORDER` or a `INFORMATION` request.

First, create a new API key from
[openai.com](https://platform.openai.com/account/api-keys) or fetch an existing
one. Record the secret key and make sure this is available as an environmental
variable:

```sh
export OPENAI_API_KEY="sk-..."
export OPENAI_API_ORG="org-..."
```

Then, you can run the pipeline on a sample text via:

```sh
python run_pipeline.py [TEXT] [PATH TO CONFIG] [PATH TO FILE WITH EXAMPLES]
```

For example:

```sh
python run_pipeline.py \
    "I'd like to order a small margherita pizza" \
    ./zeroshot.cfg
```
or, for few-shot:
```sh
python run_pipeline.py \
    "I'd like to order a small margherita pizza" \
    ./fewshot.cfg \
    ./examples.yml
```

You can also include examples to perform few-shot annotation. To do so, use the
`fewshot.cfg` file instead. You can find the few-shot examples in
the `examples.yml` file. Feel free to change and update it to your liking.
We also support other file formats, including `.yaml`, `.jsonl` and `.json`.



================================================
FILE: usage_examples/multitask_openai/__init__.py
================================================
from .run_pipeline import run_pipeline

__all__ = ["run_pipeline"]



================================================
FILE: usage_examples/multitask_openai/examples.yml
================================================
- text: i want to buy a large mushroom pizza with extra cheese
  entities:
    SIZE:
      - large
    TYPE:
      - mushroom
    TOPPING:
      - extra cheese
    PRODUCT:
      - large mushroom pizza with extra cheese
  answer: ORDER
- text: do you make chicago-style pizza?
  entities:
    TYPE:
      - chicago-style
    PRODUCT:
      - chicago-style pizza
  answer: INFORMATION
- text: do you deliver in my neighborhood
  answer: INFORMATION
  entities: {}



================================================
FILE: usage_examples/multitask_openai/fewshot.cfg
================================================
[paths]
examples = null

[nlp]
lang = "en"
pipeline = ["llm_ner", "llm_textcat"]

[components]

[components.llm_ner]
factory = "llm"

[components.llm_ner.task]
@llm_tasks = "spacy.NER.v2"
labels = SIZE,TYPE,TOPPING,PRODUCT

[components.llm_ner.task.examples]
@misc = "spacy.FewShotReader.v1"
path = ${paths.examples}

[components.llm_ner.model]
@llm_models = "spacy.GPT-3-5.v2"
name = "gpt-3.5-turbo"
config = {"temperature": 0.0}

[components.llm_textcat]
factory = "llm"

[components.llm_textcat.task]
@llm_tasks = "spacy.TextCat.v2"
labels = INFORMATION,ORDER
exclusive_classes=True

[components.llm_textcat.task.examples]
@misc = "spacy.FewShotReader.v1"
path = ${paths.examples}

[components.llm_textcat.model]
@llm_models = "spacy.GPT-3-5.v2"
name = "gpt-3.5-turbo"
config = {"temperature": 0.0}



================================================
FILE: usage_examples/multitask_openai/run_pipeline.py
================================================
import os
from pathlib import Path
from typing import Optional

import typer
from wasabi import msg

from spacy_llm.util import assemble

Arg = typer.Argument
Opt = typer.Option


def run_pipeline(
    # fmt: off
    text: str = Arg("", help="Text to perform text categorization on."),
    config_path: Path = Arg(..., help="Path to the configuration file to use."),
    examples_path: Optional[Path] = Arg(None, help="Path to the examples file to use (few-shot only)."),
    verbose: bool = Opt(False, "--verbose", "-v", help="Show extra information."),
    # fmt: on
):
    if not os.getenv("OPENAI_API_KEY", None):
        msg.fail(
            "OPENAI_API_KEY env variable was not found. "
            "Set it by running 'export OPENAI_API_KEY=...' and try again.",
            exits=1,
        )

    msg.text(f"Loading config from {config_path}", show=verbose)
    nlp = assemble(
        config_path,
        overrides={}
        if examples_path is None
        else {"paths.examples": str(examples_path)},
    )

    doc = nlp(text)

    msg.text(f"Text: {doc.text}")
    msg.text(f"Intent: {doc.cats}")
    msg.text(f"Entities: {[(ent.text, ent.label_) for ent in doc.ents]}")


if __name__ == "__main__":
    typer.run(run_pipeline)



================================================
FILE: usage_examples/multitask_openai/zeroshot.cfg
================================================
[nlp]
lang = "en"
pipeline = ["llm_ner", "llm_textcat"]

[components]

[components.llm_ner]
factory = "llm"

[components.llm_ner.task]
@llm_tasks = "spacy.NER.v2"
labels = SIZE,TYPE,TOPPING,PRODUCT

[components.llm_ner.model]
@llm_models = "spacy.GPT-3-5.v2"
name = "gpt-3.5-turbo"
config = {"temperature": 0.0}

[components.llm_textcat]
factory = "llm"

[components.llm_textcat.task]
@llm_tasks = "spacy.TextCat.v3"
labels = INFORMATION,ORDER
exclusive_classes=True

[components.llm_textcat.model]
@llm_models = "spacy.GPT-3-5.v2"
name = "gpt-3.5-turbo"



================================================
FILE: usage_examples/ner_dolly/README.md
================================================
# Using open-source Dolly models hosted on Huggingface

This example shows how you can use the [open-source Dolly
models](https://github.com/databrickslabs/dolly) hosted on Huggingface in a
Named Entity Recognition (NER) task. We demonstrate how you can use large
language models in zero- or few-shot annotation settings. 

You can run the pipeline on a sample text via:

```sh
python run_pipeline.py [TEXT] [PATH TO CONFIG] [PATH TO FILE WITH EXAMPLES]
```

For example:

```sh
python run_pipeline.py \
    "Matthew and Maria went to Japan to visit the Nintendo headquarters" \
    ./zeroshot.cfg
```
or, for few-shot:
```sh
python run_pipeline.py \
    "Matthew and Maria went to Japan to visit the Nintendo headquarters" \
    ./fewshot.cfg \
    ./examples.yml
```

By default this uses v3 if the NER recipe, which leverages a chain-of-thought prompt. If you want to run this with v2, 
use `fewshot_v2.cfg`, `zeroshot_v2`.cfg and `examples_v2.yml` instead.

By default, the pipeline assigns `PERSON`, `ORGANIZATION` or `LOCATION` labels
for each entity. You can change these labels by updating the
`zeroshot.cfg` configuration file.

You can also include examples to perform few-shot annotation. To do so, use the
`fewshot.cfg` file instead. You can find the few-shot examples in the
`examples.yml` file. Feel free to change and update it to your liking.
We also support other file formats, including `json` and `jsonl`.

Finally, you can update the Dolly model in the configuration file. We're using
[`dolly-v2-3b`](https://huggingface.co/databricks/dolly-v2-3b) by default, but
you can change it to a larger model size like
[`dolly-v2-7b`](https://huggingface.co/databricks/dolly-v2-7b) or
[`dolly-v2-12b`](https://huggingface.co/databricks/dolly-v2-12b).


================================================
FILE: usage_examples/ner_dolly/__init__.py
================================================
from .run_pipeline import run_pipeline

__all__ = ["run_pipeline"]



================================================
FILE: usage_examples/ner_dolly/examples.yml
================================================
- text: Jack and Jill went up the hill.
  spans:
    - text: Jack
      is_entity: true
      label: PERSON
      reason: is the name of a person
    - text: Jill
      is_entity: true
      label: PERSON
      reason: is the name of a person
    - text: went up
      is_entity: false
      label: ==NONE==
      reason: is a verb
    - text: hill
      is_entity: true
      label: LOCATION
      reason: is a location



================================================
FILE: usage_examples/ner_dolly/examples_v2.yml
================================================
- text: Jack and Jill went up the hill.
  entities:
    PERSON:
      - Jack
      - Jill
    LOCATION:
      - hill
- text: Jack fell down and broke his crown.
  entities:
    PERSON:
      - Jack
- text: Jill came tumbling after.
  entities:
    PERSON:
      - Jill



================================================
FILE: usage_examples/ner_dolly/fewshot.cfg
================================================
[paths]
examples = null

[nlp]
lang = "en"
pipeline = ["llm"]
batch_size = 128

[components]

[components.llm]
factory = "llm"

[components.llm.model]
@llm_models = "spacy.Dolly.v1"
name = "dolly-v2-3b"

[components.llm.task]
@llm_tasks = "spacy.NER.v3"
labels = PERSON,ORGANISATION,LOCATION

[components.llm.task.examples]
@misc = "spacy.FewShotReader.v1"
path = ${paths.examples}

[components.llm.task.normalizer]
@misc = "spacy.LowercaseNormalizer.v1"



================================================
FILE: usage_examples/ner_dolly/fewshot_v2.cfg
================================================
[paths]
examples = null

[nlp]
lang = "en"
pipeline = ["llm"]
batch_size = 128

[components]

[components.llm]
factory = "llm"

[components.llm.model]
@llm_models = "spacy.Dolly.v1"
name = "dolly-v2-3b"

[components.llm.task]
@llm_tasks = "spacy.NER.v2"
labels = PERSON,ORGANISATION,LOCATION

[components.llm.task.examples]
@misc = "spacy.FewShotReader.v1"
path = ${paths.examples}

[components.llm.task.normalizer]
@misc = "spacy.LowercaseNormalizer.v1"



================================================
FILE: usage_examples/ner_dolly/run_pipeline.py
================================================
from pathlib import Path
from typing import Optional

import typer
from wasabi import msg

from spacy_llm.util import assemble

Arg = typer.Argument
Opt = typer.Option


def run_pipeline(
    # fmt: off
    text: str = Arg("", help="Text to perform Named Entity Recognition on."),
    config_path: Path = Arg(..., help="Path to the configuration file to use."),
    examples_path: Optional[Path] = Arg(None, help="Path to the examples file to use (few-shot only)."),
    verbose: bool = Opt(False, "--verbose", "-v", help="Show extra information."),
    # fmt: on
):
    msg.text(f"Loading config from {config_path}", show=verbose)
    nlp = assemble(
        config_path,
        overrides={}
        if examples_path is None
        else {"paths.examples": str(examples_path)},
    )
    doc = nlp(text)

    msg.text(f"Text: {doc.text}")
    msg.text(f"Entities: {[(ent.text, ent.label_) for ent in doc.ents]}")


if __name__ == "__main__":
    typer.run(run_pipeline)



================================================
FILE: usage_examples/ner_dolly/zeroshot.cfg
================================================
[nlp]
lang = "en"
pipeline = ["llm"]
batch_size = 128

[components]

[components.llm]
factory = "llm"

[components.llm.model]
@llm_models = "spacy.Dolly.v1"
name = "dolly-v2-3b"

[components.llm.task]
@llm_tasks = "spacy.NER.v3"
labels = PERSON,ORGANISATION,LOCATION
examples = null

[components.llm.task.normalizer]
@misc = "spacy.LowercaseNormalizer.v1"



================================================
FILE: usage_examples/ner_dolly/zeroshot_v2.cfg
================================================
[nlp]
lang = "en"
pipeline = ["llm"]
batch_size = 128

[components]

[components.llm]
factory = "llm"

[components.llm.model]
@llm_models = "spacy.Dolly.v1"
name = "dolly-v2-3b"

[components.llm.task]
@llm_tasks = "spacy.NER.v2"
labels = PERSON,ORGANISATION,LOCATION
examples = null

[components.llm.task.normalizer]
@misc = "spacy.LowercaseNormalizer.v1"



================================================
FILE: usage_examples/ner_langchain_openai/README.md
================================================
# Using GPT models from OpenAI

This example shows how you can use a model from OpenAI to recognize named entities using the LangChain backend.

First, create a new API key from
[openai.com](https://platform.openai.com/account/api-keys) or fetch an existing
one. Record the secret key and make sure this is available as an environmental
variable:

```sh
export OPENAI_API_KEY="sk-..."
export OPENAI_API_ORG="org-..."
```

Then, you can run the pipeline on a sample text via:

```sh
python run_pipeline.py [TEXT] [PATH TO CONFIG]
```

For example:

```sh
python run_pipeline.py "Jack and Jill went up the hill." ./ner.cfg
```



================================================
FILE: usage_examples/ner_langchain_openai/__init__.py
================================================
from .run_pipeline import run_pipeline

__all__ = ["run_pipeline"]



================================================
FILE: usage_examples/ner_langchain_openai/ner.cfg
================================================
[nlp]
lang = "en"
pipeline = ["llm"]

[components]

[components.llm]
factory = "llm"

[components.llm.task]
@llm_tasks = "spacy.NER.v2"
labels = PERSON,LOCATION
examples = null

[components.llm.model]
@llm_models = "langchain.OpenAIChat.v1"
name = "gpt-3.5-turbo"
config = {}



================================================
FILE: usage_examples/ner_langchain_openai/run_pipeline.py
================================================
import os
from pathlib import Path

import typer
from wasabi import msg

from spacy_llm.util import assemble

Arg = typer.Argument
Opt = typer.Option


def run_pipeline(
    # fmt: off
    text: str = Arg("", help="Text to perform NER on."),
    config_path: Path = Arg(..., help="Path to the configuration file to use."),
    verbose: bool = Opt(False, "--verbose", "-v", help="Show extra information."),
    # fmt: on
):
    if not os.getenv("OPENAI_API_KEY", None):
        msg.fail(
            "OPENAI_API_KEY env variable was not found. "
            "Set it by running 'export OPENAI_API_KEY=...' and try again.",
            exits=1,
        )

    msg.text(f"Loading config from {config_path}", show=verbose)
    nlp = assemble(config_path)
    doc = nlp(text)

    msg.text(f"Text: {doc.text}")
    msg.text(f"Entities: {doc.ents}")


if __name__ == "__main__":
    typer.run(run_pipeline)



================================================
FILE: usage_examples/ner_v3_openai/README.md
================================================
# Using GPT Models from OpenAI for Named Entity Recognition (NER)


This example shows how you can use a model from OpenAI for Named Entity Recognition (NER).
The NER prompt is based on the [PromptNER](https://arxiv.org/abs/2305.15444) paper and
utilizes Chain-of-Thought reasoning to extract named entities.

First, create a new API key from
[openai.com](https://platform.openai.com/account/api-keys) or fetch an existing
one. Record the secret key and make sure this is available as an environmental
variable:

```sh
export OPENAI_API_KEY="sk-..."
export OPENAI_API_ORG="org-..."
```

Then, you can run the pipeline on a sample text via:


```sh
python run_pipeline.py [TEXT] [PATH TO CONFIG] [PATH TO FILE WITH EXAMPLES]
```

For example:

```sh
python run_pipeline.py \
    ""Sriracha sauce goes really well with hoisin stir fry, but you should add it after you use the wok." \
    ./fewshot.cfg
    ./examples.json
```

This example assings labels for DISH, INGREDIENT, and EQUIPMENT.

You can change around the labels and examples for your use case.
You can find the few-shot examples in the
`examples.json` file. Feel free to change and update it to your liking.
We also support other file formats, including `yml` and `jsonl` for these examples.


### Negative examples

While not required, The Chain-of-Thought reasoning for the `spacy.NER.v3` task
works best in our experience when both positive and negative examples are provided.

This prompts the Language model with concrete examples of what **is not** an entity
for your use case.

Here's an example that helps define the INGREDIENT label for the LLM.

```json
[
    {
        "text": "You can't get a great chocolate flavor with carob.",
        "spans": [
            {
                "text": "chocolate",
                "is_entity": false,
                "label": "==NONE==",
                "reason": "is a flavor in this context, not an ingredient"
            },
            {
                "text": "carob",
                "is_entity": true,
                "label": "INGREDIENT",
                "reason": "is an ingredient to add chocolate flavor"
            }
        ]
    }
    ...
]
```

In this example, "chocolate" is not an ingredient even though it could be in other contexts.
We explain that via the "reason" property of this example.



================================================
FILE: usage_examples/ner_v3_openai/__init__.py
================================================
from .run_pipeline import run_pipeline

__all__ = ["run_pipeline"]



================================================
FILE: usage_examples/ner_v3_openai/examples.json
================================================
[
  {
    "text": "You can't get a great chocolate flavor with carob.",
    "spans": [
      {
        "text": "chocolate",
        "is_entity": false,
        "label": "==NONE==",
        "reason": "is a flavor in this context, not an ingredient"
      },
      {
        "text": "carob",
        "is_entity": true,
        "label": "INGREDIENT",
        "reason": "is an ingredient to add chocolate flavor"
      }
    ]
  },
  {
    "text": "You can probably sand-blast it if it's an anodized aluminum pan",
    "spans": [
      {
        "text": "sand-blast",
        "is_entity": false,
        "label": "==NONE==",
        "reason": "is a cleaning technique, not some kind of equipment"
      },
      {
        "text": "anodized aluminum pan",
        "is_entity": true,
        "label": "EQUIPMENT",
        "reason": "is a piece of cooking equipment, anodized is included since it describes the type of pan"
      }
    ]
  }
]



================================================
FILE: usage_examples/ner_v3_openai/fewshot.cfg
================================================
[paths]
examples = null

[nlp]
lang = "en"
pipeline = ["llm"]

[components]

[components.llm]
factory = "llm"

[components.llm.task]
@llm_tasks = "spacy.NER.v3"
labels = ["DISH", "INGREDIENT", "EQUIPMENT"]
description = Entities are the names food dishes,
    ingredients, and any kind of cooking equipment.
    Adjectives, verbs, adverbs are not entities.
    Pronouns are not entities.

[components.llm.task.label_definitions]
DISH = "Known food dishes, e.g. Lobster Ravioli, garlic bread"
INGREDIENT = "Individual parts of a food dish, including herbs and spices."
EQUIPMENT = "Any kind of cooking equipment. e.g. oven, cooking pot, grill"

[components.llm.task.examples]
@misc = "spacy.FewShotReader.v1"
path = "${paths.examples}"

[components.llm.model]
@llm_models = "spacy.GPT-3-5.v1"



================================================
FILE: usage_examples/ner_v3_openai/run_pipeline.py
================================================
from pathlib import Path

import typer
from wasabi import msg

from spacy_llm.util import assemble

Arg = typer.Argument
Opt = typer.Option


def run_pipeline(
    # fmt: off
    text: str = Arg("", help="Text to perform Named Entity Recognition on."),
    config_path: Path = Arg(..., help="Path to the configuration file to use."),
    examples_path: Path = Arg(..., help="Path to the examples file to use."),
    verbose: bool = Opt(False, "--verbose", "-v", help="Show extra information."),
    # fmt: on
):
    msg.text(f"Loading config from {config_path}", show=verbose)
    nlp = assemble(config_path, overrides={"paths.examples": str(examples_path)})
    doc = nlp(text)

    msg.text(f"Text: {doc.text}")
    msg.text(f"Entities: {[(ent.text, ent.label_) for ent in doc.ents]}")


if __name__ == "__main__":
    typer.run(run_pipeline)



================================================
FILE: usage_examples/rel_openai/README.md
================================================
# Relation extraction using LLMs

This example shows how you can use a model from OpenAI for relation extraction in
zero- and few-shot settings.

Here, we use the pretrained [`en_core_web_md` model](https://spacy.io/models/en#en_core_web_sm)
to perform Named Entity Recognition (NER) using a fast and properly evaluated pipeline.
Then, we leverage the OpenAI API to detect the relations between the extracted entities.
In this example, we focus on two simple relations: `LivesIn` and `Visits`.

First, create a new API key from
[openai.com](https://platform.openai.com/account/api-keys) or fetch an existing
one. Record the secret key and make sure this is available as an environmental
variable:

```sh
export OPENAI_API_KEY="sk-..."
export OPENAI_API_ORG="org-..."
```

Then, you can run the pipeline on a sample text via:

```sh
python run_pipeline.py [TEXT] [PATH TO CONFIG]
```

For example:

```sh
python run_pipeline.py \
    "Laura just bought an apartment in Boston." \
    ./zeroshot.cfg
```

or, with few-shot:

```sh
python run_pipeline.py \
    "Laura just bought an apartment in Boston." \
    ./fewshot.cfg
    ./examples.jsonl
```

You can also include examples to perform few-shot annotation. To do so, use the
`openai_rel_fewshot.cfg` file instead. You can find the few-shot examples in
the `examples.jsonl` file. Feel free to change and update it to your liking.
We also support other file formats, including `.json`, `.yml` and `.yaml`.



================================================
FILE: usage_examples/rel_openai/__init__.py
================================================
from .run_pipeline import run_pipeline

__all__ = ["run_pipeline"]



================================================
FILE: usage_examples/rel_openai/examples.jsonl
================================================
{"text": "Laura bought a house in Boston with her husband Mark.", "ents": [{"start_char": 0, "end_char": 5, "label": "PERSON"}, {"start_char": 24, "end_char": 30, "label": "GPE"}, {"start_char": 48, "end_char": 52, "label": "PERSON"}], "relations": [{"dep": 0, "dest": 1, "relation": "LivesIn"}, {"dep": 2, "dest": 1, "relation": "LivesIn"}]}
{"text": "Michael travelled through South America by bike.", "ents": [{"start_char": 0, "end_char": 7, "label": "PERSON"}, {"start_char": 26, "end_char": 39, "label": "LOC"}], "relations": [{"dep": 0, "dest": 1, "relation": "Visits"}]}



================================================
FILE: usage_examples/rel_openai/fewshot.cfg
================================================
[paths]
examples = null

[nlp]
lang = "en"
pipeline = ["ner", "llm_rel"]

[components]

[components.ner]
source = "en_core_web_md"

[components.llm_rel]
factory = "llm"

[components.llm_rel.task]
@llm_tasks = "spacy.REL.v1"
labels = LivesIn,Visits

[components.llm_rel.task.examples]
@misc = "spacy.FewShotReader.v1"
path = ${paths.examples}

[components.llm_rel.model]
@llm_models = "spacy.GPT-3-5.v2"

[initialize]
vectors = "en_core_web_md"



================================================
FILE: usage_examples/rel_openai/run_pipeline.py
================================================
import os
from pathlib import Path
from typing import Optional

import typer
from wasabi import msg

from spacy_llm.util import assemble

Arg = typer.Argument
Opt = typer.Option


def run_pipeline(
    # fmt: off
    text: str = Arg("", help="Text to perform text categorization on."),
    config_path: Path = Arg(..., help="Path to the configuration file to use."),
    examples_path: Optional[Path] = Arg(None, help="Path to the examples file to use (few-shot only)."),
    verbose: bool = Opt(False, "--verbose", "-v", help="Show extra information."),
    # fmt: on
):
    if not os.getenv("OPENAI_API_KEY", None):
        msg.fail(
            "OPENAI_API_KEY env variable was not found. "
            "Set it by running 'export OPENAI_API_KEY=...' and try again.",
            exits=1,
        )

    msg.text(f"Loading config from {config_path}", show=verbose)
    nlp = assemble(
        config_path,
        overrides={}
        if examples_path is None
        else {"paths.examples": str(examples_path)},
    )

    doc = nlp(text)

    msg.text(f"Text: {doc.text}")
    msg.text(f"Entities: {[(ent.text, ent.label_) for ent in doc.ents]}")

    msg.text("Relations:")
    for r in doc._.rel:
        msg.text(f"  - {doc.ents[r.dep]} [{r.relation}] {doc.ents[r.dest]}")


if __name__ == "__main__":
    typer.run(run_pipeline)



================================================
FILE: usage_examples/rel_openai/zeroshot.cfg
================================================
[paths]
examples = null

[nlp]
lang = "en"
pipeline = ["ner", "llm_rel"]

[components]

[components.ner]
source = "en_core_web_md"

[components.llm_rel]
factory = "llm"

[components.llm_rel.task]
@llm_tasks = "spacy.REL.v1"
labels = LivesIn,Visits

[components.llm_rel.model]
@llm_models = "spacy.GPT-3-5.v2"

[initialize]
vectors = "en_core_web_md"



================================================
FILE: usage_examples/streamlit/streamlit_app.py
================================================
import os
from typing import cast

import streamlit as st
from spacy.util import load_config_from_str
from spacy_streamlit import visualize_ner, visualize_textcat

from spacy_llm.pipeline import LLMWrapper
from spacy_llm.util import assemble_from_config

NER_CONFIG = """
[nlp]
lang = "en"
pipeline = ["llm"]
batch_size = 128

[components]

[components.llm]
factory = "llm"

[components.llm.model]
@llm_models = "spacy.GPT-3-5.v2"
name = "gpt-3.5-turbo"
config = {"temperature": 0.0}

[components.llm.task]
@llm_tasks = "spacy.NER.v2"
labels = PERSON,ORGANISATION,LOCATION
examples = null

[components.llm.task.normalizer]
@misc = "spacy.LowercaseNormalizer.v1"
"""

TEXTCAT_CONFIG = """
[nlp]
lang = "en"
pipeline = ["llm"]
batch_size = 128

[components]

[components.llm]
factory = "llm"

[components.llm.model]
@llm_models = "spacy.GPT-3-5.v2"
name = "gpt-3.5-turbo"
config = {"temperature": 0.0}

[components.llm.task]
@llm_tasks = "spacy.TextCat.v2"
labels = COMPLIMENT,INSULT
examples = null
exclusive_classes = true

[components.llm.task.normalizer]
@misc = "spacy.LowercaseNormalizer.v1"
"""

DEFAULT_TEXT = "Ernest Hemingway, born in Illinois, is generally considered one of the best authors of his time."

st.title("spacy-llm Streamlit Demo")
st.markdown(
    """
    The [spacy-llm](https://github.com/explosion/spacy-llm) package integrates
    Large Language Models (LLMs) into spaCy, featuring a modular system
    for fast prototyping and prompting, and turning unstructured responses
    into robust outputs for various NLP tasks, no training data required.

    This demo uses the OpenAI backend to demonstrate the NER and textcat
    tasks.
    """
)

os.environ["OPENAI_API_KEY"] = st.text_input(
    "Your OpenAI API key", type="password", value=os.environ.get("OPENAI_API_KEY", "")
)
text = st.text_area("Text to analyze", DEFAULT_TEXT, height=70)

if os.environ["OPENAI_API_KEY"]:
    textcat_config = load_config_from_str(TEXTCAT_CONFIG)
    textcat_model = assemble_from_config(textcat_config)
    ner_config = load_config_from_str(NER_CONFIG)
    ner_model = assemble_from_config(ner_config)

    models = {"textcat": textcat_model, "ner": ner_model}
    model_names = models.keys()

    selected_model = st.sidebar.selectbox("Model", model_names)
    assert selected_model is not None

    nlp = models[selected_model]
    doc = nlp(text)
    llm_pipe = cast(LLMWrapper, nlp.get_pipe("llm"))
    prompt = "\n".join(
        [str(prompt) for prompt in llm_pipe._task.generate_prompts([doc])]
    )

    if selected_model == "textcat":
        visualize_textcat(doc)
    if selected_model == "ner":
        visualize_ner(doc)

    st.markdown("### Prompt:")
    st.text(prompt)
else:
    st.error("Input your OpenAI API key")



================================================
FILE: usage_examples/tests/__init__.py
================================================
[Empty file]


================================================
FILE: usage_examples/tests/conftest.py
================================================
from spacy_llm.tests.conftest import (
    pytest_addoption,  # noqa: F401
    pytest_runtest_setup,  # noqa: F401
    pytest_collection_modifyitems,  # noqa: F401
)



================================================
FILE: usage_examples/tests/test_readme_examples.py
================================================
from pathlib import Path
from typing import Callable, Iterable

import pytest
import spacy
from spacy import util
from thinc.compat import has_torch_cuda_gpu

from spacy_llm.registry import registry
from spacy_llm.util import assemble


@pytest.mark.external
def test_example_1_classifier():
    with util.make_tempdir() as tmpdir:
        cfg_str = """
        [nlp]
        lang = "en"
        pipeline = ["llm"]

        [components]

        [components.llm]
        factory = "llm"

        [components.llm.task]
        @llm_tasks = "spacy.TextCat.v2"
        labels = ["COMPLIMENT", "INSULT"]

        [components.llm.model]
        @llm_models = "spacy.GPT-3-5.v2"
        """

        with open(tmpdir / "cfg", "w") as text_file:
            text_file.write(cfg_str)

        nlp = assemble(tmpdir / "cfg")
        doc = nlp("You look gorgeous!")
        print(doc.cats)  # noqa: T201


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
def test_example_2_classifier_hf():
    with util.make_tempdir() as tmpdir:
        cfg_str = """
        [nlp]
        lang = "en"
        pipeline = ["llm"]

        [components]

        [components.llm]
        factory = "llm"

        [components.llm.task]
        @llm_tasks = "spacy.TextCat.v2"
        labels = ["COMPLIMENT", "INSULT"]

        [components.llm.model]
        @llm_models = "spacy.Dolly.v1"
        # For better performance, use databricks/dolly-v2-12b instead
        name = "dolly-v2-3b"
        """

        with open(tmpdir / "cfg", "w") as text_file:
            text_file.write(cfg_str)

        nlp = assemble(tmpdir / "cfg")
        doc = nlp("You look gorgeous!")
        print(doc.cats)  # noqa: T201


@pytest.mark.external
def test_example_3_ner():
    examples_path = Path(__file__).parent.parent / "ner_v3_openai" / "examples.json"

    with util.make_tempdir() as tmpdir:

        cfg_str = f"""
        [nlp]
        lang = "en"
        pipeline = ["llm"]

        [components]

        [components.llm]
        factory = "llm"

        [components.llm.task]
        @llm_tasks = "spacy.NER.v3"
        labels = ["DISH", "INGREDIENT", "EQUIPMENT"]
        description = Entities are the names food dishes,
            ingredients, and any kind of cooking equipment.
            Adjectives, verbs, adverbs are not entities.
            Pronouns are not entities.

        [components.llm.task.label_definitions]
        DISH = "Known food dishes, e.g. Lobster Ravioli, garlic bread"
        INGREDIENT = "Individual parts of a food dish, including herbs and spices."
        EQUIPMENT = "Any kind of cooking equipment. e.g. oven, cooking pot, grill"

        [components.llm.task.examples]
        @misc = "spacy.FewShotReader.v1"
        path = {str(examples_path)}

        [components.llm.model]
        @llm_models = "spacy.GPT-3-5.v1"
        """

        with open(tmpdir / "cfg", "w") as text_file:
            text_file.write(cfg_str)

        nlp = assemble(tmpdir / "cfg")
        doc = nlp(
            "Sriracha sauce goes really well with hoisin stir fry, "
            "but you should add it after you use the wok."
        )
        print([(ent.text, ent.label_) for ent in doc.ents])  # noqa: T201


@pytest.mark.external
def test_example_4_python():
    nlp = spacy.blank("en")
    nlp.add_pipe(
        "llm",
        config={
            "task": {
                "@llm_tasks": "spacy.NER.v3",
                "labels": ["DISH", "INGREDIENT", "EQUIPMENT"],
                "examples": [
                    {
                        "text": "You can't get a great chocolate flavor with carob.",
                        "spans": [
                            {
                                "text": "chocolate",
                                "is_entity": False,
                                "label": "==NONE==",
                                "reason": "is a flavor in this context, not an ingredient",
                            },
                            {
                                "text": "carob",
                                "is_entity": True,
                                "label": "INGREDIENT",
                                "reason": "is an ingredient to add chocolate flavor",
                            },
                        ],
                    },
                ],
            },
            "model": {
                "@llm_models": "spacy.GPT-3-5.v2",
            },
        },
    )
    nlp.initialize()
    doc = nlp(
        "Sriracha sauce goes really well with hoisin stir fry, "
        "but you should add it after you use the wok."
    )
    print([(ent.text, ent.label_) for ent in doc.ents])  # noqa: T201


def test_example_5_custom_model():
    import random

    @registry.llm_models("RandomClassification.v1")
    def random_textcat(
        labels: str,
    ) -> Callable[[Iterable[Iterable[str]]], Iterable[Iterable[str]]]:
        labels = labels.split(",")

        def _classify(prompts: Iterable[Iterable[str]]) -> Iterable[Iterable[str]]:
            for prompts_for_doc in prompts:
                yield [random.choice(labels) for _ in prompts_for_doc]

        return _classify

    with util.make_tempdir() as tmpdir:
        cfg_str = """
        [nlp]
        lang = "en"
        pipeline = ["llm"]

        [components]

        [components.llm]
        factory = "llm"

        [components.llm.task]
        @llm_tasks = "spacy.TextCat.v2"
        labels = ORDER,INFORMATION

        [components.llm.model]
        @llm_models = "RandomClassification.v1"
        labels = ${components.llm.task.labels}
        """

        with open(tmpdir / "cfg", "w") as text_file:
            text_file.write(cfg_str)

        with pytest.warns(UserWarning, match="Task supports sharding"):
            nlp = assemble(tmpdir / "cfg")
        nlp("i'd like a large margherita pizza please")



================================================
FILE: usage_examples/tests/test_usage_examples.py
================================================
from pathlib import Path

import pytest
from thinc.compat import has_torch_cuda_gpu

from spacy_llm import cache  # noqa: F401
from spacy_llm.tests.compat import has_openai_key

from .. import el_openai, multitask_openai, ner_dolly, ner_langchain_openai
from .. import ner_v3_openai, rel_openai, textcat_openai

_USAGE_EXAMPLE_PATH = Path(__file__).parent.parent


@pytest.mark.external
@pytest.mark.skipif(has_openai_key is False, reason="OpenAI API key not available")
@pytest.mark.parametrize("config_name", ("zeroshot.cfg", "fewshot.cfg"))
def test_el_openai(config_name: str):
    """Test OpenAI EL usage example.
    config_name (str): Name of config file to use.
    """
    path = _USAGE_EXAMPLE_PATH / "el_openai"
    ents = list(
        el_openai.run_pipeline(
            text="There are some nice restaurants in New York.",
            config_path=path / config_name,
            examples_path=None
            if config_name == "zeroshot.cfg"
            else path / "examples.yml",
            verbose=False,
        ).ents
    )
    assert len(ents) == 1
    assert ents[0].text == "New York"
    assert ents[0].kb_id_ == "Q60"


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
@pytest.mark.parametrize("config_name", ("fewshot_v2.cfg", "zeroshot_v2.cfg"))
def test_ner_dolly(config_name: str):
    """Test NER Dolly usage example.
    config_name (str): Name of config file to use.
    """
    path = _USAGE_EXAMPLE_PATH / "ner_dolly"
    ner_dolly.run_pipeline(
        text="text",
        config_path=path / config_name,
        examples_path=None if "zeroshot" in config_name else path / "examples_v2.yml",
        verbose=False,
    )


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
@pytest.mark.parametrize("config_name", ("fewshot.cfg", "zeroshot.cfg"))
def test_ner_v3_dolly(config_name: str):
    """Test NER Dolly usage example.
    config_name (str): Name of config file to use.
    """
    path = _USAGE_EXAMPLE_PATH / "ner_dolly"
    ner_dolly.run_pipeline(
        text="text",
        config_path=path / config_name,
        examples_path=None if "zeroshot" in config_name else path / "examples.yml",
        verbose=False,
    )


@pytest.mark.gpu
@pytest.mark.skipif(not has_torch_cuda_gpu, reason="needs GPU & CUDA")
@pytest.mark.parametrize("config_name", ("fewshot.cfg", "zeroshot.cfg"))
def test_textcat_dolly(config_name: str):
    """Test Textcat Dolly usage example.
    config_name (str): Name of config file to use.
    """
    path = _USAGE_EXAMPLE_PATH / "textcat_dolly"
    textcat_openai.run_pipeline(
        text="text",
        config_path=path / config_name,
        examples_path=None
        if config_name == "zeroshot.cfg"
        else path / "examples.jsonl",
        verbose=False,
    )


@pytest.mark.external
@pytest.mark.parametrize("config_name", ("fewshot.cfg", "zeroshot.cfg"))
def test_textcat_openai(config_name: str):
    """Test NER Dolly usage example.
    config_name (str): Name of config file to use.
    """
    path = _USAGE_EXAMPLE_PATH / "textcat_openai"
    textcat_openai.run_pipeline(
        text="text",
        config_path=path / config_name,
        examples_path=None
        if config_name == "zeroshot.cfg"
        else path / "examples.jsonl",
        verbose=False,
    )


@pytest.mark.external
def test_ner_v3_openai():
    """Test NER v3 OpenAI usage example."""
    path = _USAGE_EXAMPLE_PATH / "ner_v3_openai"
    ner_v3_openai.run_pipeline(
        text="text",
        config_path=path / "fewshot.cfg",
        examples_path=path / "examples.json",
        verbose=False,
    )


@pytest.mark.external
@pytest.mark.filterwarnings("ignore::DeprecationWarning")
def test_ner_langchain_openai():
    """Test NER LangChain OpenAI usage example."""
    with pytest.warns(UserWarning, match="Task supports sharding"):
        ner_langchain_openai.run_pipeline(
            "text", _USAGE_EXAMPLE_PATH / "ner_langchain_openai" / "ner.cfg", False
        )


@pytest.mark.external
@pytest.mark.filterwarnings("ignore::DeprecationWarning")
@pytest.mark.parametrize("config_name", ("fewshot.cfg", "zeroshot.cfg"))
def test_multitask_openai(config_name: str):
    """Test multitask OpenAI example.
    config_name (str): Name of config file to use.
    """
    path = _USAGE_EXAMPLE_PATH / "multitask_openai"
    multitask_openai.run_pipeline(
        text="text",
        config_path=path / config_name,
        examples_path=None if config_name == "zeroshot.cfg" else path / "examples.yml",
        verbose=False,
    )


@pytest.mark.external
@pytest.mark.parametrize("config_name", ("zeroshot.cfg", "fewshot.cfg"))
def test_rel_openai(config_name: str):
    """Test REL OpenAI usage example.
    config_name (str): Name of config file to use.
    """
    path = _USAGE_EXAMPLE_PATH / "rel_openai"
    rel_openai.run_pipeline(
        text="Sara lives in Lisbon.",
        config_path=path / config_name,
        examples_path=None
        if config_name == "zeroshot.cfg"
        else path / "examples.jsonl",
        verbose=False,
    )



================================================
FILE: usage_examples/textcat_dolly/README.md
================================================
# Using open-source Dolly models hosted on Huggingface

This example shows how you can use the [open-source Dolly
models](https://github.com/databrickslabs/dolly) hosted on Huggingface for categorizing texts in
zero- or few-shot settings. Here, we perform binary text classification to
determine if a given text is an `INSULT` or a `COMPLIMENT`.

You can run the pipeline on a sample text via:

```sh
python run_pipeline.py [TEXT] [PATH TO CONFIG] [PATH TO FILE WITH EXAMPLES]
```

For example:

```sh
python run_pipeline.py "You look great today! Nice shirt!" ./zeroshot.cfg
```
or, for few-shot:
```sh
python run_pipeline.py "You look great today! Nice shirt!" ./fewshot.cfg ./examples.jsonl
```

You can also include examples to perform few-shot annotation. To do so, use the
`fewshot.cfg` file instead. You can find the few-shot examples in
the `examples.jsonl` file. Feel free to change and update it to your liking.
We also support other file formats, including `.yml`, `.yaml` and `.json`.

Finally, you can update the Dolly model in the configuration file. We're using
[`dolly-v2-3b`](https://huggingface.co/databricks/dolly-v2-3b) by default, but
you can change it to a larger model size like
[`dolly-v2-7b`](https://huggingface.co/databricks/dolly-v2-7b) or
[`dolly-v2-12b`](https://huggingface.co/databricks/dolly-v2-12b).



================================================
FILE: usage_examples/textcat_dolly/__init__.py
================================================
from .run_pipeline import run_pipeline

__all__ = ["run_pipeline"]



================================================
FILE: usage_examples/textcat_dolly/examples.jsonl
================================================
{"text":"Shall I compare thee to a summer's day? Thou art more lovely and more temperate","answer":"COMPLIMENT"}
{"text":"That you have such a February face, so full of frost, of storm and cloudiness","answer":"INSULT"}
{"text":"Thou art wise as thou art beautiful","answer":"COMPLIMENT"}



================================================
FILE: usage_examples/textcat_dolly/fewshot.cfg
================================================
[paths]
examples = null

[nlp]
lang = "en"
pipeline = ["llm"]
batch_size = 128

[components]

[components.llm]
factory = "llm"

[components.llm.model]
@llm_models = "spacy.Dolly.v1"
name = "dolly-v2-3b"

[components.llm.task]
@llm_tasks = "spacy.TextCat.v3"
labels = COMPLIMENT,INSULT
exclusive_classes = true

[components.llm.task.examples]
@misc = "spacy.FewShotReader.v1"
path = ${paths.examples}

[components.llm.task.normalizer]
@misc = "spacy.LowercaseNormalizer.v1"



================================================
FILE: usage_examples/textcat_dolly/run_pipeline.py
================================================
import os
from pathlib import Path
from typing import Optional

import typer
from wasabi import msg

from spacy_llm.util import assemble

Arg = typer.Argument
Opt = typer.Option


def run_pipeline(
    # fmt: off
    text: str = Arg("", help="Text to perform text categorization on."),
    config_path: Path = Arg(..., help="Path to the configuration file to use."),
    examples_path: Optional[Path] = Arg(None, help="Path to the examples file to use (few-shot only)."),
    verbose: bool = Opt(False, "--verbose", "-v", help="Show extra information."),
    # fmt: on
):
    if not os.getenv("OPENAI_API_KEY", None):
        msg.fail(
            "OPENAI_API_KEY env variable was not found. "
            "Set it by running 'export OPENAI_API_KEY=...' and try again.",
            exits=1,
        )

    msg.text(f"Loading config from {config_path}", show=verbose)
    nlp = assemble(
        config_path,
        overrides={}
        if examples_path is None
        else {"paths.examples": str(examples_path)},
    )
    doc = nlp(text)

    msg.text(f"Text: {doc.text}")
    msg.text(f"Categories: {doc.cats}")


if __name__ == "__main__":
    typer.run(run_pipeline)



================================================
FILE: usage_examples/textcat_dolly/zeroshot.cfg
================================================
[nlp]
lang = "en"
pipeline = ["llm"]
batch_size = 128

[components]

[components.llm]
factory = "llm"

[components.llm.model]
@llm_models = "spacy.Dolly.v1"
name = "dolly-v2-3b"

[components.llm.task]
@llm_tasks = "spacy.TextCat.v3"
labels = COMPLIMENT,INSULT
examples = null
exclusive_classes = true

[components.llm.task.normalizer]
@misc = "spacy.LowercaseNormalizer.v1"



================================================
FILE: usage_examples/textcat_openai/README.md
================================================
# Using GPT models from OpenAI

This example shows how you can use a model from OpenAI for categorizing texts in
zero- or few-shot settings. Here, we perform binary text classification to
determine if a given text is an `INSULT` or a `COMPLIMENT`.

First, create a new API key from
[openai.com](https://platform.openai.com/account/api-keys) or fetch an existing
one. Record the secret key and make sure this is available as an environmental
variable:

```sh
export OPENAI_API_KEY="sk-..."
export OPENAI_API_ORG="org-..."
```

Then, you can run the pipeline on a sample text via:

```sh
python run_pipeline.py [TEXT] [PATH TO CONFIG] [PATH TO FILE WITH EXAMPLES]
```

For example:

```sh
python run_pipeline.py "You look great today! Nice shirt!" ./zeroshot.cfg
```
or, for few-shot:
```sh
python run_pipeline.py "You look great today! Nice shirt!" ./fewshot.cfg ./examples.jsonl
```

You can also include examples to perform few-shot annotation. To do so, use the 
`fewshot.cfg` file instead. You can find the few-shot examples in
the `examples.jsonl` file. Feel free to change and update it to your liking.
We also support other file formats, including `.yml`, `.yaml` and `.json`.


================================================
FILE: usage_examples/textcat_openai/__init__.py
================================================
from .run_pipeline import run_pipeline

__all__ = ["run_pipeline"]



================================================
FILE: usage_examples/textcat_openai/examples.jsonl
================================================
{"text":"Shall I compare thee to a summer's day? Thou art more lovely and more temperate","answer":"COMPLIMENT"}
{"text":"That you have such a February face, so full of frost, of storm and cloudiness","answer":"INSULT"}
{"text":"Thou art wise as thou art beautiful","answer":"COMPLIMENT"}



================================================
FILE: usage_examples/textcat_openai/fewshot.cfg
================================================
[paths]
examples = null

[nlp]
lang = "en"
pipeline = ["llm"]
batch_size = 128

[components]

[components.llm]
factory = "llm"

[components.llm.model]
@llm_models = "spacy.GPT-3-5.v2"
config = {"temperature": 0.0}

[components.llm.task]
@llm_tasks = "spacy.TextCat.v2"
labels = COMPLIMENT,INSULT
exclusive_classes = true

[components.llm.task.examples]
@misc = "spacy.FewShotReader.v1"
path = ${paths.examples}

[components.llm.task.normalizer]
@misc = "spacy.LowercaseNormalizer.v1"



================================================
FILE: usage_examples/textcat_openai/run_pipeline.py
================================================
import os
from pathlib import Path
from typing import Optional

import typer
from wasabi import msg

from spacy_llm.util import assemble

Arg = typer.Argument
Opt = typer.Option


def run_pipeline(
    # fmt: off
    text: str = Arg("", help="Text to perform text categorization on."),
    config_path: Path = Arg(..., help="Path to the configuration file to use."),
    examples_path: Optional[Path] = Arg(None, help="Path to the examples file to use (few-shot only)."),
    verbose: bool = Opt(False, "--verbose", "-v", help="Show extra information."),
    # fmt: on
):
    if not os.getenv("OPENAI_API_KEY", None):
        msg.fail(
            "OPENAI_API_KEY env variable was not found. "
            "Set it by running 'export OPENAI_API_KEY=...' and try again.",
            exits=1,
        )

    msg.text(f"Loading config from {config_path}", show=verbose)
    nlp = assemble(
        config_path,
        overrides={}
        if examples_path is None
        else {"paths.examples": str(examples_path)},
    )
    doc = nlp(text)

    msg.text(f"Text: {doc.text}")
    msg.text(f"Categories: {doc.cats}")


if __name__ == "__main__":
    typer.run(run_pipeline)



================================================
FILE: usage_examples/textcat_openai/zeroshot.cfg
================================================
[nlp]
lang = "en"
pipeline = ["llm"]
batch_size = 128

[components]

[components.llm]
factory = "llm"

[components.llm.model]
@llm_models = "spacy.GPT-3-5.v2"
config = {"temperature": 0.0}

[components.llm.task]
@llm_tasks = "spacy.TextCat.v2"
labels = COMPLIMENT,INSULT
examples = null
exclusive_classes = true

[components.llm.task.normalizer]
@misc = "spacy.LowercaseNormalizer.v1"



================================================
FILE: .github/FUNDING.yml
================================================
custom: https://explosion.ai/merch



================================================
FILE: .github/PULL_REQUEST_TEMPLATE.md
================================================
<!--- Provide a general summary of your changes in the title. -->

## Description
<!--- Use this section to describe your changes. If your changes required
testing, include information about the testing environment and the tests you
ran. If your test fixes a bug reported in an issue, don't forget to include the
issue number. If your PR is still a work in progress, that's totally fine â€“ just
include a note to let us know. -->

### Corresponding documentation PR
<!--- Add the link to the corresponding documentation PR here, if applicable. -->

### Types of change
<!-- What type of change does your PR cover? Is it a bug fix, an enhancement
or new feature, or a change to the documentation? -->

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [ ] I confirm that I have the right to submit this contribution under the project's MIT license.
- [ ] I ran all tests in `tests` and `usage_examples/tests`, and all new and existing tests passed. This includes
  - all external tests (i. e. `pytest` ran with `--external`)
  - all tests requiring a GPU (i. e. `pytest` ran with `--gpu`)
- [ ] My changes don't require a change to the documentation, or if they do, I've added all required information.



================================================
FILE: .github/workflows/cibuildwheel.yml
================================================
name: Build

on:
  push:
    tags:
      # ytf did they invent their own syntax that's almost regex?
      # ** matches 'zero or more of any character'
      - 'release-v[0-9]+.[0-9]+.[0-9]+**'
      - 'prerelease-v[0-9]+.[0-9]+.[0-9]+**'
jobs:
  build_wheels:
    name: Build universal wheel
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
      - name: Configure Python version
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
          architecture: x64
      - name: Build wheels
        run: |
          python -m pip install wheel
          python -m pip wheel . -w ./wheelhouse
      - uses: actions/upload-artifact@v4
        with:
          name: cibw-wheel-pure
          path: wheelhouse/spacy_llm-*.whl

  build_sdist:
    name: Build source distribution
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Build sdist
        run: pipx run build --sdist
      - uses: actions/upload-artifact@v4
        with:
          name: cibw-sdist
          path: dist/*.tar.gz
  create_release:
    needs: [build_wheels, build_sdist]
    runs-on: ubuntu-latest
    permissions:
      contents: write
      checks: write
      actions: read
      issues: read
      packages: write
      pull-requests: read
      repository-projects: read
      statuses: read
    steps:
      - name: Get the tag name and determine if it's a prerelease
        id: get_tag_info
        run: |
          FULL_TAG=${GITHUB_REF#refs/tags/}
          if [[ $FULL_TAG == release-* ]]; then
            TAG_NAME=${FULL_TAG#release-}
            IS_PRERELEASE=false
          elif [[ $FULL_TAG == prerelease-* ]]; then
            TAG_NAME=${FULL_TAG#prerelease-}
            IS_PRERELEASE=true
          else
            echo "Tag does not match expected patterns" >&2
            exit 1
          fi
          echo "FULL_TAG=$TAG_NAME" >> $GITHUB_ENV
          echo "TAG_NAME=$TAG_NAME" >> $GITHUB_ENV
          echo "IS_PRERELEASE=$IS_PRERELEASE" >> $GITHUB_ENV
      - uses: actions/download-artifact@v4
        with:
          # unpacks all CIBW artifacts into dist/
          pattern: cibw-*
          path: dist
          merge-multiple: true
      - name: Create Draft Release
        id: create_release
        uses: softprops/action-gh-release@v2
        if: startsWith(github.ref, 'refs/tags/')
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          name: ${{ env.TAG_NAME }}
          draft: true
          prerelease: ${{ env.IS_PRERELEASE }}
          files: "./dist/*" 



================================================
FILE: .github/workflows/explosionbot.yml
================================================
name: Explosion Bot

on:
  issue_comment:
    types:
      - created
      - edited

jobs:
  explosion-bot:
    if: github.repository_owner == 'explosion'
    runs-on: ubuntu-latest
    steps:
      - name: Dump GitHub context
        env:
          GITHUB_CONTEXT: ${{ toJson(github) }}
        run: echo "$GITHUB_CONTEXT"
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
      - name: Install and run explosion-bot
        run: |
          pip install git+https://${{ secrets.EXPLOSIONBOT_TOKEN }}@github.com/explosion/explosion-bot
          python -m explosionbot
        env:
          INPUT_TOKEN: ${{ secrets.EXPLOSIONBOT_TOKEN }}
          INPUT_BK_TOKEN: ${{ secrets.BUILDKITE_SECRET }}
          ENABLED_COMMANDS: "test_gpu,test_llm_external"
          ALLOWED_TEAMS: "spaCy"



================================================
FILE: .github/workflows/publish_pypi.yml
================================================
# The cibuildwheel action triggers on creation of a release, this
# triggers on publication.
# The expected workflow is to create a draft release and let the wheels
# upload, and then hit 'publish', which uploads to PyPi.

on:
  release:
    types:
      - published

jobs:
  upload_pypi:
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/spacy-llm
    permissions:
      id-token: write
      contents: read
    if: github.event_name == 'release' && github.event.action == 'published'
    # or, alternatively, upload to PyPI on every tag starting with 'v' (remove on: release above to use this)
    # if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags/v')
    steps:
      - uses: robinraju/release-downloader@v1
        with:
          tag: ${{ github.event.release.tag_name }}
          fileName: '*'
          out-file-path: 'dist'
      - uses: pypa/gh-action-pypi-publish@release/v1



================================================
FILE: .github/workflows/test.yml
================================================
name: Core tests

on:
  push:
    branches:
    - main
    paths-ignore:
      - "*.md"
  pull_request:
    types: [opened, synchronize, reopened, edited]
    paths-ignore:
      - "*.md"
  workflow_dispatch:

env:
  MODULE_NAME: 'spacy_llm'
  RUN_MYPY: 'false'

jobs:
  run:
    strategy:
      fail-fast: true
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python_version: ["3.12"]
        include:
          - os: windows-latest
            python_version: "3.9"
          - os: macos-latest
            python_version: "3.9"
          - os: ubuntu-latest
            python_version: "3.9"
          - os: windows-latest
            python_version: "3.10"
          - os: macos-latest
            python_version: "3.11"

    runs-on: ${{ matrix.os }}

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python_version }}

      - name: Build sdist
        run: |
          python -m pip install --upgrade pip
          python -m pip install -U build pip setuptools
          python -m pip install -U -r requirements.txt
          python -m build --sdist

      - name: Run mypy
        shell: bash
        if: ${{ env.RUN_MYPY == 'true' }}
        run: |
          python -m mypy $MODULE_NAME

      - name: Delete source directory
        shell: bash
        run: |
          rm -rf $MODULE_NAME

      - name: Uninstall all packages
        run: |
          python -m pip freeze > installed.txt
          python -m pip uninstall -y -r installed.txt

      - name: Install from sdist
        shell: bash
        run: |
          SDIST=$(python -c "import os;print(os.listdir('./dist')[-1])" 2>&1)
          python -m pip install dist/$SDIST

      - name: Test import
        shell: bash
        run: |
          python -c "import $MODULE_NAME" -Werror

      - name: Install test requirements
        run: |
          python -m pip install -U -r requirements.txt
          python -m pip install -U -r requirements-dev.txt
          python -m spacy download en_core_web_md

      - name: Run spacy_llm tests
        shell: bash
        run: |
          python -m pytest --pyargs $MODULE_NAME

      - name: Run usage tests
        shell: bash
        run: |
          python -m pytest --pyargs usage_examples



================================================
FILE: .github/workflows/test_external.yml
================================================
name: External tests

on:
  schedule:
    - cron: "0 0 * * *"
  workflow_dispatch:
  pull_request:
    types: [ labeled, opened, synchronize, reopened ]

env:
  MODULE_NAME: "spacy_llm"
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
  ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
  CO_API_KEY: ${{ secrets.CO_API_KEY }}
  TEST_EXTERNAL: 1

jobs:
  run:
    runs-on: ubuntu-latest
    if: "github.repository_owner == 'explosion' && (contains(github.event.pull_request.labels.*.name, 'Test external') || github.event_name == 'schedule' || github.event_name == 'workflow_dispatch')"

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.9
        uses: actions/setup-python@v5
        with:
          python-version: 3.9
          cache: "pip"

      - name: Build sdist
        run: |
          python -m pip install -U build pip setuptools
          python -m build --sdist

      - name: Delete source directory
        shell: bash
        run: |
          rm -rf $MODULE_NAME

      - name: Install from sdist
        shell: bash
        run: |
          SDIST=$(python -c "import os;print(os.listdir('./dist')[-1])" 2>&1)
          python -m pip install dist/$SDIST

      - name: Test import
        shell: bash
        run: |
          python -c "import $MODULE_NAME" -Werror

      - name: Install test requirements
        run: |
          python -m spacy download en_core_web_md
          python -m pip install -U -r requirements.txt -r requirements-dev.txt

      - name: Run tests
        shell: bash
        run: python -m pytest --pyargs $MODULE_NAME -x

      - name: Run usage tests
        shell: bash
        run: python -m pytest --pyargs usage_examples -x

#      - name: Report Status
#        if: always()
#        uses: ravsamhq/notify-slack-action@v1
#        with:
#          status: ${{ job.status }}
#          notification_title: "{workflow} have {status_message}"
#          message_format: "{emoji} external workflow has {status_message}"
#          notify_when: "failure"
#        env:
#          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_MONITORING_WEBHOOK_URL }}



================================================
FILE: .github/workflows/test_gpu.yml
================================================
name: GPU tests

on:
#  schedule:
#    - cron: "0 0 * * *"
  issue_comment:
    types: [created]
  workflow_dispatch:
  pull_request:
    types: [ labeled, opened, synchronize, reopened ]

jobs:
  run:
    if: "github.repository_owner == 'explosion' && (contains(github.event.pull_request.labels.*.name, 'Test GPU') || github.event_name == 'schedule' || github.event_name == 'workflow_dispatch')"
    runs-on: ubuntu-latest

    steps:

      - name: Trigger buildkite build
        uses: buildkite/trigger-pipeline-action@v1.2.0
        env:
          PIPELINE: explosion-ai/spacy-llm-gpu-tests
          BRANCH: main
          MESSAGE: ":github: spacy-llm GPU tests - triggered from a GitHub Action"
          BUILDKITE_API_ACCESS_TOKEN: ${{ secrets.BUILDKITE_SECRET }}



================================================
FILE: .github/workflows/validate.yml
================================================
name: Pre-commit checks

on: pull_request

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python 3.9
        uses: actions/setup-python@v5
        with:
          python-version: 3.9
          cache: "pip" # caching pip dependencies

      - name: Install pre-commit
        run: |
          pip install 'pre-commit==3.3.2'
          pre-commit install

      - name: Set env variables
        run: |
          echo "PY=$(python -VV | sha256sum | cut -d' ' -f1)" >> $GITHUB_ENV
          echo "PRECOMMIT=$(pip list | grep pre-commit)" >> $GITHUB_ENV

      - uses: actions/cache@v3
        with:
          path: ~/.cache/pre-commit
          key: pre-commit|${{ env.PY }}|${{ env.PRECOMMIT }}|${{ hashFiles('.pre-commit-config.yaml') }}

      - name: Run all pre-commit checks
        # TODO: remove skip once files are clean
        run: SKIP=isort pre-commit run --all-files --hook-stage manual -c .pre-commit-config.yaml


